[
  {
    "title": "Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion",
    "abstract": "Can we generate a control policy for an agent using just one demonstration of\ndesired behaviors as a prompt, as effortlessly as creating an image from a\ntextual description? In this paper, we present Make-An-Agent, a novel policy\nparameter generator that leverages the power of conditional diffusion models\nfor behavior-to-policy generation. Guided by behavior embeddings that encode\ntrajectory information, our policy generator synthesizes latent parameter\nrepresentations, which can then be decoded into policy networks. Trained on\npolicy network checkpoints and their corresponding trajectories, our generation\nmodel demonstrates remarkable versatility and scalability on multiple tasks and\nhas a strong generalization ability on unseen tasks to output well-performed\npolicies with only few-shot demonstrations as inputs. We showcase its efficacy\nand efficiency on various domains and tasks, including varying objectives,\nbehaviors, and even across different robot manipulators. Beyond simulation, we\ndirectly deploy policies generated by Make-An-Agent onto real-world robots on\nlocomotion tasks.",
    "authors": [
      "Yongyuan Liang",
      "Tingqiang Xu",
      "Kaizhe Hu",
      "Guangqi Jiang",
      "Furong Huang",
      "Huazhe Xu"
    ]
  },
  {
    "title": "VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation",
    "abstract": "In the realm of vision models, the primary mode of representation is using\npixels to rasterize the visual world. Yet this is not always the best or unique\nway to represent visual content, especially for designers and artists who\ndepict the world using geometry primitives such as polygons. Vector graphics\n(VG), on the other hand, offer a textual representation of visual content,\nwhich can be more concise and powerful for content like cartoons or sketches.\nRecent studies have shown promising results on processing vector graphics with\ncapable Large Language Models (LLMs). However, such works focus solely on\nqualitative results, understanding, or a specific type of vector graphics. We\npropose VGBench, a comprehensive benchmark for LLMs on handling vector graphics\nthrough diverse aspects, including (a) both visual understanding and\ngeneration, (b) evaluation of various vector graphics formats, (c) diverse\nquestion types, (d) wide range of prompting techniques, (e) under multiple\nLLMs. Evaluating on our collected 4279 understanding and 5845 generation\nsamples, we find that LLMs show strong capability on both aspects while\nexhibiting less desirable performance on low-level formats (SVG). Both data and\nevaluation pipeline will be open-sourced at https://vgbench.github.io.",
    "authors": [
      "Bocheng Zou",
      "Mu Cai",
      "Jianrui Zhang",
      "Yong Jae Lee"
    ]
  },
  {
    "title": "Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes",
    "abstract": "Traditional reference segmentation tasks have predominantly focused on silent\nvisual scenes, neglecting the integral role of multimodal perception and\ninteraction in human experiences. In this work, we introduce a novel task\ncalled Reference Audio-Visual Segmentation (Ref-AVS), which seeks to segment\nobjects within the visual domain based on expressions containing multimodal\ncues. Such expressions are articulated in natural language forms but are\nenriched with multimodal cues, including audio and visual descriptions. To\nfacilitate this research, we construct the first Ref-AVS benchmark, which\nprovides pixel-level annotations for objects described in corresponding\nmultimodal-cue expressions. To tackle the Ref-AVS task, we propose a new method\nthat adequately utilizes multimodal cues to offer precise segmentation\nguidance. Finally, we conduct quantitative and qualitative experiments on three\ntest subsets to compare our approach with existing methods from related tasks.\nThe results demonstrate the effectiveness of our method, highlighting its\ncapability to precisely segment objects using multimodal-cue expressions.\nDataset is available at\n\\href{https://gewu-lab.github.io/Ref-AVS}{https://gewu-lab.github.io/Ref-AVS}.",
    "authors": [
      "Yaoting Wang",
      "Peiwen Sun",
      "Dongzhan Zhou",
      "Guangyao Li",
      "Honggang Zhang",
      "Di Hu"
    ]
  },
  {
    "title": "Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?",
    "abstract": "Data science and engineering workflows often span multiple stages, from\nwarehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As\nvision language models (VLMs) advance in multimodal understanding and code\ngeneration, VLM-based agents could potentially automate these workflows by\ngenerating SQL queries, Python code, and GUI operations. This automation can\nimprove the productivity of experts while democratizing access to large-scale\ndata analysis. In this paper, we introduce Spider2-V, the first multimodal\nagent benchmark focusing on professional data science and engineering\nworkflows, featuring 494 real-world tasks in authentic computer environments\nand incorporating 20 enterprise-level professional applications. These tasks,\nderived from real-world use cases, evaluate the ability of a multimodal agent\nto perform data-related tasks by writing code and managing the GUI in\nenterprise data software systems. To balance realistic simulation with\nevaluation simplicity, we devote significant effort to developing automatic\nconfigurations for task setup and carefully crafting evaluation metrics for\neach task. Furthermore, we supplement multimodal agents with comprehensive\ndocuments of these enterprise data software systems. Our empirical evaluation\nreveals that existing state-of-the-art LLM/VLM-based agents do not reliably\nautomate full data workflows (14.0% success). Even with step-by-step guidance,\nthese agents still underperform in tasks that require fine-grained,\nknowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted\nworkspaces (10.6%). We hope that Spider2-V paves the way for autonomous\nmultimodal agents to transform the automation of data science and engineering\nworkflow. Our code and data are available at https://spider2-v.github.io.",
    "authors": [
      "Ruisheng Cao",
      "Fangyu Lei",
      "Haoyuan Wu",
      "Jixuan Chen",
      "Yeqiao Fu",
      "Hongcheng Gao",
      "Xinzhuang Xiong",
      "Hanchong Zhang",
      "Yuchen Mao",
      "Wenjing Hu",
      "Tianbao Xie",
      "Hongshen Xu",
      "Danyang Zhang",
      "Sida Wang",
      "Ruoxi Sun",
      "Pengcheng Yin",
      "Caiming Xiong",
      "Ansong Ni",
      "Qian Liu",
      "Victor Zhong",
      "Lu Chen",
      "Kai Yu",
      "Tao Yu"
    ]
  },
  {
    "title": "Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together",
    "abstract": "Natural Language Processing (NLP) systems are increasingly taking the form of\nmulti-stage pipelines involving multiple distinct language models (LMs) and\nprompting strategies. Here we address the question of how to fine-tune such\nsystems to improve their performance. We cast this as a problem of optimizing\nthe underlying LM weights and the prompting strategies together, and consider a\nchallenging but highly realistic scenario in which we have no gold labels for\nany intermediate stages in the pipeline. To address this challenge, we evaluate\napproximate optimization strategies in which we bootstrap training labels for\nall pipeline stages and use these to optimize the pipeline's prompts and\nfine-tune its weights alternatingly. In experiments with multi-hop QA,\nmathematical reasoning, and feature-based classification, we find that simple\napproaches for optimizing the prompts and weights together outperform directly\noptimizing weights alone and prompts alone by up to 65% and 5%, respectively,\non average across LMs and tasks. We will release our new optimizers in DSPy at\nhttp://dspy.ai",
    "authors": [
      "Dilara Soylu",
      "Christopher Potts",
      "Omar Khattab"
    ]
  },
  {
    "title": "Benchmarking Vision Language Models for Cultural Understanding",
    "abstract": "Foundation models and vision-language pre-training have notably advanced\nVision Language Models (VLMs), enabling multimodal processing of visual and\nlinguistic data. However, their performance has been typically assessed on\ngeneral scene understanding - recognizing objects, attributes, and actions -\nrather than cultural comprehension. This study introduces CulturalVQA, a visual\nquestion-answering benchmark aimed at assessing VLM's geo-diverse cultural\nunderstanding. We curate a collection of 2,378 image-question pairs with 1-5\nanswers per question representing cultures from 11 countries across 5\ncontinents. The questions probe understanding of various facets of culture such\nas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on\nCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of\ncultural understanding across regions, with strong cultural understanding\ncapabilities for North America while significantly lower performance for\nAfrica. We observe disparity in their performance across cultural facets too,\nwith clothing, rituals, and traditions seeing higher performances than food and\ndrink. These disparities help us identify areas where VLMs lack cultural\nunderstanding and demonstrate the potential of CulturalVQA as a comprehensive\nevaluation set for gauging VLM progress in understanding diverse cultures.",
    "authors": [
      "Shravan Nayak",
      "Kanishk Jain",
      "Rabiul Awal",
      "Siva Reddy",
      "Sjoerd van Steenkiste",
      "Lisa Anne Hendricks",
      "Karolina Sta\u0144czak",
      "Aishwarya Agrawal"
    ]
  },
  {
    "title": "Leveraging LLM-Respondents for Item Evaluation: a Psychometric Analysis",
    "abstract": "Effective educational measurement relies heavily on the curation of\nwell-designed item pools (i.e., possessing the right psychometric properties).\nHowever, item calibration is time-consuming and costly, requiring a sufficient\nnumber of respondents for the response process. We explore using six different\nLLMs (GPT-3.5, GPT-4, Llama 2, Llama 3, Gemini-Pro, and Cohere Command R Plus)\nand various combinations of them using sampling methods to produce responses\nwith psychometric properties similar to human answers. Results show that some\nLLMs have comparable or higher proficiency in College Algebra than college\nstudents. No single LLM mimics human respondents due to narrow proficiency\ndistributions, but an ensemble of LLMs can better resemble college students'\nability distribution. The item parameters calibrated by LLM-Respondents have\nhigh correlations (e.g. > 0.8 for GPT-3.5) compared to their human calibrated\ncounterparts, and closely resemble the parameters of the human subset (e.g.\n0.02 Spearman correlation difference). Several augmentation strategies are\nevaluated for their relative performance, with resampling methods proving most\neffective, enhancing the Spearman correlation from 0.89 (human only) to 0.93\n(augmented human).",
    "authors": [
      "Yunting Liu",
      "Shreya Bhandari",
      "Zachary A. Pardos"
    ]
  },
  {
    "title": "Leveraging Multimodal CycleGAN for the Generation of Anatomically Accurate Synthetic CT Scans from MRIs",
    "abstract": "In many clinical settings, the use of both Computed Tomography (CT) and\nMagnetic Resonance (MRI) is necessary to pursue a thorough understanding of the\npatient's anatomy and to plan a suitable therapeutical strategy; this is often\nthe case in MRI-based radiotherapy, where CT is always necessary to prepare the\ndose delivery, as it provides the essential information about the radiation\nabsorption properties of the tissues. Sometimes, MRI is preferred to contour\nthe target volumes. However, this approach is often not the most efficient, as\nit is more expensive, time-consuming and, most importantly, stressful for the\npatients. To overcome this issue, in this work, we analyse the capabilities of\ndifferent configurations of Deep Learning models to generate synthetic CT scans\nfrom MRI, leveraging the power of Generative Adversarial Networks (GANs) and,\nin particular, the CycleGAN architecture, capable of working in an unsupervised\nmanner and without paired images, which were not available. Several CycleGAN\nmodels were trained unsupervised to generate CT scans from different MRI\nmodalities with and without contrast agents. To overcome the problem of not\nhaving a ground truth, distribution-based metrics were used to assess the\nmodel's performance quantitatively, together with a qualitative evaluation\nwhere physicians were asked to differentiate between real and synthetic images\nto understand how realistic the generated images were. The results show how,\ndepending on the input modalities, the models can have very different\nperformances; however, models with the best quantitative results, according to\nthe distribution-based metrics used, can generate very difficult images to\ndistinguish from the real ones, even for physicians, demonstrating the\napproach's potential.",
    "authors": [
      "Leonardo Crespi",
      "Samuele Camnasio",
      "Damiano Dei",
      "Nicola Lambri",
      "Pietro Mancosu",
      "Marta Scorsetti",
      "Daniele Loiacono"
    ]
  },
  {
    "title": "Hey, That's My Model! Introducing Chain & Hash, An LLM Fingerprinting Technique",
    "abstract": "Amid growing concerns over the ease of theft and misuse of Large Language\nModels (LLMs), the need for fingerprinting models has increased.\nFingerprinting, in this context, means that the model owner can link a given\nmodel to their original version, thereby identifying if their model is being\nmisused or has been completely stolen. In this paper, we first define a set\nfive properties a successful fingerprint should satisfy; namely, the\nfingerprint should be Transparent, Efficient, Persistent, Robust, and\nUnforgeable. Next, we propose Chain & Hash, a new, simple fingerprinting\napproach that implements a fingerprint with a cryptographic flavor, achieving\nall these properties. Chain & Hash involves generating a set of questions (the\nfingerprints) along with a set of potential answers. These elements are hashed\ntogether using a secure hashing technique to select the value for each\nquestion, hence providing an unforgeability property-preventing adversaries\nfrom claiming false ownership. We evaluate the Chain & Hash technique on\nmultiple models and demonstrate its robustness against benign transformations,\nsuch as fine-tuning on different datasets, and adversarial attempts to erase\nthe fingerprint. Finally, our experiments demonstrate the efficiency of\nimplementing Chain & Hash and its utility, where fingerprinted models achieve\nalmost the same performance as non-fingerprinted ones across different\nbenchmarks.",
    "authors": [
      "Mark Russinovich",
      "Ahmed Salem"
    ]
  },
  {
    "title": "Deep Causal Learning to Explain and Quantify The Geo-Tension's Impact on Natural Gas Market",
    "abstract": "Natural gas demand is a crucial factor for predicting natural gas prices and\nthus has a direct influence on the power system. However, existing methods face\nchallenges in assessing the impact of shocks, such as the outbreak of the\nRussian-Ukrainian war. In this context, we apply deep neural network-based\nGranger causality to identify important drivers of natural gas demand.\nFurthermore, the resulting dependencies are used to construct a counterfactual\ncase without the outbreak of the war, providing a quantifiable estimate of the\noverall effect of the shock on various German energy sectors. The code and\ndataset are available at https://github.com/bonaldli/CausalEnergy.",
    "authors": [
      "Philipp Kai Peter",
      "Yulin Li",
      "Ziyue Li",
      "Wolfgang Ketter"
    ]
  },
  {
    "title": "Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models",
    "abstract": "Automated heuristic design (AHD) has gained considerable attention for its\npotential to automate the development of effective heuristics. The recent\nadvent of large language models (LLMs) has paved a new avenue for AHD, with\ninitial efforts focusing on framing AHD as an evolutionary program search (EPS)\nproblem. However, inconsistent benchmark settings, inadequate baselines, and a\nlack of detailed component analysis have left the necessity of integrating LLMs\nwith search strategies and the true progress achieved by existing LLM-based EPS\nmethods to be inadequately justified. This work seeks to fulfill these research\nqueries by conducting a large-scale benchmark comprising four LLM-based EPS\nmethods and four AHD problems across nine LLMs and five independent runs. Our\nextensive experiments yield meaningful insights, providing empirical grounding\nfor the importance of evolutionary search in LLM-based AHD approaches, while\nalso contributing to the advancement of future EPS algorithmic development. To\nfoster accessibility and reproducibility, we have fully open-sourced our\nbenchmark and corresponding results.",
    "authors": [
      "Rui Zhang",
      "Fei Liu",
      "Xi Lin",
      "Zhenkun Wang",
      "Zhichao Lu",
      "Qingfu Zhang"
    ]
  },
  {
    "title": "GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via VLM",
    "abstract": "Large vision-language models (LVLMs), such as the Generative Pre-trained\nTransformer 4-omni (GPT-4o), are emerging multi-modal foundation models which\nhave great potential as powerful artificial-intelligence (AI) assistance tools\nfor a myriad of applications, including healthcare, industrial, and academic\nsectors. Although such foundation models perform well in a wide range of\ngeneral tasks, their capability without fine-tuning is often limited in\nspecialized tasks. However, full fine-tuning of large foundation models is\nchallenging due to enormous computation/memory/dataset requirements. We show\nthat GPT-4o can decode hand gestures from forearm ultrasound data even with no\nfine-tuning, and improves with few-shot, in-context learning.",
    "authors": [
      "Keshav Bimbraw",
      "Ye Wang",
      "Jing Liu",
      "Toshiaki Koike-Akino"
    ]
  },
  {
    "title": "Weighted Grouped Query Attention in Transformers",
    "abstract": "The attention mechanism forms the foundational blocks for transformer\nlanguage models. Recent approaches show that scaling the model achieves\nhuman-level performance. However, with increasing demands for scaling and\nconstraints on hardware memory, the inference costs of these models remain\nhigh. To reduce the inference time, Multi-Query Attention (MQA) and\nGrouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet\nal., 2023) respectively. In this paper, we propose a variation of Grouped-Query\nAttention, termed Weighted Grouped-Query Attention (WGQA). We introduced new\nlearnable parameters for each key and value head in the T5 decoder attention\nblocks, enabling the model to take a weighted average during finetuning. Our\nmodel achieves an average of 0.53% improvement over GQA, and the performance\nconverges to traditional Multi-head attention (MHA) with no additional overhead\nduring inference. We evaluated the introduction of these parameters and\nsubsequent finetuning informs the model about the grouping mechanism during\ntraining, thereby enhancing performance. Additionally, we demonstrate the\nscaling laws in our analysis by comparing the results between T5-small and\nT5-base architecture.",
    "authors": [
      "Sai Sena Chinnakonduru",
      "Astarag Mohapatra"
    ]
  },
  {
    "title": "An Actionable Framework for Assessing Bias and Fairness in Large Language Model Use Cases",
    "abstract": "Large language models (LLMs) can exhibit bias in a variety of ways. Such\nbiases can create or exacerbate unfair outcomes for certain groups within a\nprotected attribute, including, but not limited to sex, race, sexual\norientation, or age. This paper aims to provide a technical guide for\npractitioners to assess bias and fairness risks in LLM use cases. The main\ncontribution of this work is a decision framework that allows practitioners to\ndetermine which metrics to use for a specific LLM use case. To achieve this,\nthis study categorizes LLM bias and fairness risks, maps those risks to a\ntaxonomy of LLM use cases, and then formally defines various metrics to assess\neach type of risk. As part of this work, several new bias and fairness metrics\nare introduced, including innovative counterfactual metrics as well as metrics\nbased on stereotype classifiers. Instead of focusing solely on the model\nitself, the sensitivity of both prompt-risk and model-risk are taken into\naccount by defining evaluations at the level of an LLM use case, characterized\nby a model and a population of prompts. Furthermore, because all of the\nevaluation metrics are calculated solely using the LLM output, the proposed\nframework is highly practical and easily actionable for practitioners.",
    "authors": [
      "Dylan Bouchard"
    ]
  },
  {
    "title": "Offline Reinforcement Learning with Imputed Rewards",
    "abstract": "Offline Reinforcement Learning (ORL) offers a robust solution to training\nagents in applications where interactions with the environment must be strictly\nlimited due to cost, safety, or lack of accurate simulation environments.\nDespite its potential to facilitate deployment of artificial agents in the real\nworld, Offline Reinforcement Learning typically requires very many\ndemonstrations annotated with ground-truth rewards. Consequently,\nstate-of-the-art ORL algorithms can be difficult or impossible to apply in\ndata-scarce scenarios. In this paper we propose a simple but effective Reward\nModel that can estimate the reward signal from a very limited sample of\nenvironment transitions annotated with rewards. Once the reward signal is\nmodeled, we use the Reward Model to impute rewards for a large sample of\nreward-free transitions, thus enabling the application of ORL techniques. We\ndemonstrate the potential of our approach on several D4RL continuous locomotion\ntasks. Our results show that, using only 1\\% of reward-labeled transitions from\nthe original datasets, our learned reward model is able to impute rewards for\nthe remaining 99\\% of the transitions, from which performant agents can be\nlearned using Offline Reinforcement Learning.",
    "authors": [
      "Carlo Romeo",
      "Andrew D. Bagdanov"
    ]
  },
  {
    "title": "MetaLLM: A High-performant and Cost-efficient Dynamic Framework for Wrapping LLMs",
    "abstract": "The rapid progress in machine learning (ML) has brought forth many large\nlanguage models (LLMs) that excel in various tasks and areas. These LLMs come\nwith different abilities and costs in terms of computation or pricing. Since\nthe demand for each query can vary, e.g., because of the queried domain or its\ncomplexity, defaulting to one LLM in an application is not usually the best\nchoice, whether it is the biggest, priciest, or even the one with the best\naverage test performance. Consequently, picking the right LLM that is both\naccurate and cost-effective for an application remains a challenge. In this\npaper, we introduce MetaLLM, a framework that dynamically and intelligently\nroutes each query to the optimal LLM (among several available LLMs) for\nclassification tasks, achieving significantly improved accuracy and\ncost-effectiveness. By framing the selection problem as a multi-armed bandit,\nMetaLLM balances prediction accuracy and cost efficiency under uncertainty. Our\nexperiments, conducted on popular LLM platforms such as OpenAI's GPT models,\nAmazon's Titan, Anthropic's Claude, and Meta's LLaMa, showcase MetaLLM's\nefficacy in real-world scenarios, laying the groundwork for future extensions\nbeyond classification tasks.",
    "authors": [
      "Quang H. Nguyen",
      "Duy C. Hoang",
      "Juliette Decugis",
      "Saurav Manchanda",
      "Nitesh V. Chawla",
      "Khoa D. Doan"
    ]
  },
  {
    "title": "BiasScanner: Automatic Detection and Classification of News Bias to Strengthen Democracy",
    "abstract": "The increasing consumption of news online in the 21st century coincided with\nincreased publication of disinformation, biased reporting, hate speech and\nother unwanted Web content. We describe BiasScanner, an application that aims\nto strengthen democracy by supporting news consumers with scrutinizing news\narticles they are reading online. BiasScanner contains a server-side\npre-trained large language model to identify biased sentences of news articles\nand a front-end Web browser plug-in. At the time of writing, BiasScanner can\nidentify and classify more than two dozen types of media bias at the sentence\nlevel, making it the most fine-grained model and only deployed application\n(automatic system in use) of its kind. It was implemented in a light-weight and\nprivacy-respecting manner, and in addition to highlighting likely biased\nsentence it also provides explanations for each classification decision as well\nas a summary analysis for each news article. While prior research has addressed\nnews bias detection, we are not aware of any work that resulted in a deployed\nbrowser plug-in (c.f. also biasscanner.org for a Web demo).",
    "authors": [
      "Tim Menzner",
      "Jochen L. Leidner"
    ]
  },
  {
    "title": "Towards Enhanced Classification of Abnormal Lung sound in Multi-breath: A Light Weight Multi-label and Multi-head Attention Classification Method",
    "abstract": "This study aims to develop an auxiliary diagnostic system for classifying\nabnormal lung respiratory sounds, enhancing the accuracy of automatic abnormal\nbreath sound classification through an innovative multi-label learning approach\nand multi-head attention mechanism. Addressing the issue of class imbalance and\nlack of diversity in existing respiratory sound datasets, our study employs a\nlightweight and highly accurate model, using a two-dimensional label set to\nrepresent multiple respiratory sound characteristics. Our method achieved a\n59.2% ICBHI score in the four-category task on the ICBHI2017 dataset,\ndemonstrating its advantages in terms of lightweight and high accuracy. This\nstudy not only improves the accuracy of automatic diagnosis of lung respiratory\nsound abnormalities but also opens new possibilities for clinical applications.",
    "authors": [
      "Yi-Wei Chua",
      "Yun-Chien Cheng"
    ]
  },
  {
    "title": "Enabling MCTS Explainability for Sequential Planning Through Computation Tree Logic",
    "abstract": "Monte Carlo tree search (MCTS) is one of the most capable online search\nalgorithms for sequential planning tasks, with significant applications in\nareas such as resource allocation and transit planning. Despite its strong\nperformance in real-world deployment, the inherent complexity of MCTS makes it\nchallenging to understand for users without technical background. This paper\nconsiders the use of MCTS in transportation routing services, where the\nalgorithm is integrated to develop optimized route plans. These plans are\nrequired to meet a range of constraints and requirements simultaneously,\nfurther complicating the task of explaining the algorithm's operation in\nreal-world contexts. To address this critical research gap, we introduce a\nnovel computation tree logic-based explainer for MCTS. Our framework begins by\ntaking user-defined requirements and translating them into rigorous logic\nspecifications through the use of language templates. Then, our explainer\nincorporates a logic verification and quantitative evaluation module that\nvalidates the states and actions traversed by the MCTS algorithm. The outcomes\nof this analysis are then rendered into human-readable descriptive text using a\nsecond set of language templates. The user satisfaction of our approach was\nassessed through a survey with 82 participants. The results indicated that our\nexplanatory approach significantly outperforms other baselines in user\npreference.",
    "authors": [
      "Ziyan An",
      "Hendrik Baier",
      "Abhishek Dubey",
      "Ayan Mukhopadhyay",
      "Meiyi Ma"
    ]
  },
  {
    "title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
    "abstract": "As large language models (LLMs) advance, it becomes more challenging to\nreliably evaluate their output due to the high costs of human evaluation. To\nmake progress towards better LLM autoraters, we introduce FLAMe, a family of\nFoundational Large Autorater Models. FLAMe is trained on our large and diverse\ncollection of 100+ quality assessment tasks comprising 5M+ human judgments,\ncurated and standardized using publicly released human evaluations from\nprevious research. FLAMe significantly improves generalization to a wide\nvariety of held-out tasks, outperforming LLMs trained on proprietary data like\nGPT-4 and Claude-3 on many tasks. We show that FLAMe can also serve as a\npowerful starting point for further downstream fine-tuning, using reward\nmodeling evaluation as a case study (FLAMe-RM). Notably, on RewardBench, our\nFLAMe-RM-24B model (with an accuracy of 87.8%) is the top-performing generative\nmodel trained exclusively on permissively licensed data, outperforming both\nGPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we explore a more\ncomputationally efficient approach using a novel tail-patch fine-tuning\nstrategy to optimize our FLAMe multitask mixture for reward modeling evaluation\n(FLAMe-Opt-RM), offering competitive RewardBench performance while requiring\napproximately 25x less training datapoints. Overall, our FLAMe variants\noutperform all popular proprietary LLM-as-a-Judge models we consider across 8\nout of 12 autorater evaluation benchmarks, encompassing 53 quality assessment\ntasks, including RewardBench and LLM-AggreFact. Finally, our analysis reveals\nthat FLAMe is significantly less biased than these LLM-as-a-Judge models on the\nCoBBLEr autorater bias benchmark, while effectively identifying high-quality\nresponses for code generation.",
    "authors": [
      "Tu Vu",
      "Kalpesh Krishna",
      "Salaheddin Alzubi",
      "Chris Tar",
      "Manaal Faruqui",
      "Yun-Hsuan Sung"
    ]
  },
  {
    "title": "GuideLight: \"Industrial Solution\" Guidance for More Practical Traffic Signal Control Agents",
    "abstract": "Currently, traffic signal control (TSC) methods based on reinforcement\nlearning (RL) have proven superior to traditional methods. However, most RL\nmethods face difficulties when applied in the real world due to three factors:\ninput, output, and the cycle-flow relation. The industry's observable input is\nmuch more limited than simulation-based RL methods. For real-world solutions,\nonly flow can be reliably collected, whereas common RL methods need more. For\nthe output action, most RL methods focus on acyclic control, which real-world\nsignal controllers do not support. Most importantly, industry standards require\na consistent cycle-flow relationship: non-decreasing and different response\nstrategies for low, medium, and high-level flows, which is ignored by the RL\nmethods. To narrow the gap between RL methods and industry standards, we\ninnovatively propose to use industry solutions to guide the RL agent.\nSpecifically, we design behavior cloning and curriculum learning to guide the\nagent to mimic and meet industry requirements and, at the same time, leverage\nthe power of exploration and exploitation in RL for better performance. We\ntheoretically prove that such guidance can largely decrease the sample\ncomplexity to polynomials in the horizon when searching for an optimal policy.\nOur rigid experiments show that our method has good cycle-flow relation and\nsuperior performance.",
    "authors": [
      "Haoyuan Jiang",
      "Xuantang Xiong",
      "Ziyue Li",
      "Hangyu Mao",
      "Guanghu Sui",
      "Jingqing Ruan",
      "Yuheng Cheng",
      "Hua Wei",
      "Wolfgang Ketter",
      "Rui Zhao"
    ]
  },
  {
    "title": "FabGPT: An Efficient Large Multimodal Model for Complex Wafer Defect Knowledge Queries",
    "abstract": "Intelligence is key to advancing integrated circuit (IC) fabrication. Recent\nbreakthroughs in Large Multimodal Models (LMMs) have unlocked unparalleled\nabilities in understanding images and text, fostering intelligent fabrication.\nLeveraging the power of LMMs, we introduce FabGPT, a customized IC fabrication\nlarge multimodal model for wafer defect knowledge query. FabGPT manifests\nexpertise in conducting defect detection in Scanning Electron Microscope (SEM)\nimages, performing root cause analysis, and providing expert question-answering\n(Q&A) on fabrication processes. FabGPT matches enhanced multimodal features to\nautomatically detect minute defects under complex wafer backgrounds and reduce\nthe subjectivity of manual threshold settings. Besides, the proposed modulation\nmodule and interactive corpus training strategy embed wafer defect knowledge\ninto the pre-trained model, effectively balancing Q&A queries related to defect\nknowledge and original knowledge and mitigating the modality bias issues.\nExperiments on in-house fab data (SEM-WaD) show that our FabGPT achieves\nsignificant performance improvement in wafer defect detection and knowledge\nquerying.",
    "authors": [
      "Yuqi Jiang",
      "Xudong Lu",
      "Qian Jin",
      "Qi Sun",
      "Hanming Wu",
      "Cheng Zhuo"
    ]
  },
  {
    "title": "Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval",
    "abstract": "Retrieval-augmented generation (RAG) has significantly advanced large\nlanguage models (LLMs) by enabling dynamic information retrieval to mitigate\nknowledge gaps and hallucinations in generated content. However, these systems\noften falter with complex reasoning and consistency across diverse queries. In\nthis work, we present Think-on-Graph 2.0, an enhanced RAG framework that aligns\nquestions with the knowledge graph and uses it as a navigational tool, which\ndeepens and refines the RAG paradigm for information collection and\nintegration. The KG-guided navigation fosters deep and long-range associations\nto uphold logical consistency and optimize the scope of retrieval for precision\nand interoperability. In conjunction, factual consistency can be better ensured\nthrough semantic similarity guided by precise directives. ToG${2.0}$ not only\nimproves the accuracy and reliability of LLMs' responses but also demonstrates\nthe potential of hybrid structured knowledge systems to significantly advance\nLLM reasoning, aligning it closer to human-like performance. We conducted\nextensive experiments on four public datasets to demonstrate the advantages of\nour method compared to the baseline.",
    "authors": [
      "Shengjie Ma",
      "Chengjin Xu",
      "Xuhui Jiang",
      "Muzhi Li",
      "Huaren Qu",
      "Jian Guo"
    ]
  },
  {
    "title": "Mammographic Breast Positioning Assessment via Deep Learning",
    "abstract": "Breast cancer remains a leading cause of cancer-related deaths among women\nworldwide, with mammography screening as the most effective method for the\nearly detection. Ensuring proper positioning in mammography is critical, as\npoor positioning can lead to diagnostic errors, increased patient stress, and\nhigher costs due to recalls. Despite advancements in deep learning (DL) for\nbreast cancer diagnostics, limited focus has been given to evaluating\nmammography positioning. This paper introduces a novel DL methodology to\nquantitatively assess mammogram positioning quality, specifically in\nmediolateral oblique (MLO) views using attention and coordinate convolution\nmodules. Our method identifies key anatomical landmarks, such as the nipple and\npectoralis muscle, and automatically draws a posterior nipple line (PNL),\noffering robust and inherently explainable alternative to well-known\nclassification and regression-based approaches. We compare the performance of\nproposed methodology with various regression and classification-based models.\nThe CoordAtt UNet model achieved the highest accuracy of 88.63% $\\pm$ 2.84 and\nspecificity of 90.25% $\\pm$ 4.04, along with a noteworthy sensitivity of 86.04%\n$\\pm$ 3.41. In landmark detection, the same model also recorded the lowest mean\nerrors in key anatomical points and the smallest angular error of 2.42 degrees.\nOur results indicate that models incorporating attention mechanisms and\nCoordConv module increase the accuracy in classifying breast positioning\nquality and detecting anatomical landmarks. Furthermore, we make the labels and\nsource codes available to the community to initiate an open research area for\nmammography, accessible at https://github.com/tanyelai/deep-breast-positioning.",
    "authors": [
      "Toygar Tanyel",
      "Nurper Denizoglu",
      "Mustafa Ege Seker",
      "Deniz Alis",
      "Esma Cerekci",
      "Ercan Karaarslan",
      "Erkin Aribal",
      "Ilkay Oksuz"
    ]
  },
  {
    "title": "Graphusion: Leveraging Large Language Models for Scientific Knowledge Graph Fusion and Construction in NLP Education",
    "abstract": "Knowledge graphs (KGs) are crucial in the field of artificial intelligence\nand are widely applied in downstream tasks, such as enhancing Question\nAnswering (QA) systems. The construction of KGs typically requires significant\neffort from domain experts. Recently, Large Language Models (LLMs) have been\nused for knowledge graph construction (KGC), however, most existing approaches\nfocus on a local perspective, extracting knowledge triplets from individual\nsentences or documents. In this work, we introduce Graphusion, a zero-shot KGC\nframework from free text. The core fusion module provides a global view of\ntriplets, incorporating entity merging, conflict resolution, and novel triplet\ndiscovery. We showcase how Graphusion could be applied to the natural language\nprocessing (NLP) domain and validate it in the educational scenario.\nSpecifically, we introduce TutorQA, a new expert-verified benchmark for graph\nreasoning and QA, comprising six tasks and a total of 1,200 QA pairs. Our\nevaluation demonstrates that Graphusion surpasses supervised baselines by up to\n10% in accuracy on link prediction. Additionally, it achieves average scores of\n2.92 and 2.37 out of 3 in human evaluations for concept entity extraction and\nrelation recognition, respectively.",
    "authors": [
      "Rui Yang",
      "Boming Yang",
      "Sixun Ouyang",
      "Tianwei She",
      "Aosong Feng",
      "Yuang Jiang",
      "Freddy Lecue",
      "Jinghui Lu",
      "Irene Li"
    ]
  },
  {
    "title": "GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework",
    "abstract": "Methods to evaluate Large Language Model (LLM) responses and detect\ninconsistencies, also known as hallucinations, with respect to the provided\nknowledge, are becoming increasingly important for LLM applications. Current\nmetrics fall short in their ability to provide explainable decisions,\nsystematically check all pieces of information in the response, and are often\ntoo computationally expensive to be used in practice. We present GraphEval: a\nhallucination evaluation framework based on representing information in\nKnowledge Graph (KG) structures. Our method identifies the specific triples in\nthe KG that are prone to hallucinations and hence provides more insight into\nwhere in the response a hallucination has occurred, if at all, than previous\nmethods. Furthermore, using our approach in conjunction with state-of-the-art\nnatural language inference (NLI) models leads to an improvement in balanced\naccuracy on various hallucination benchmarks, compared to using the raw NLI\nmodels. Lastly, we explore the use of GraphEval for hallucination correction by\nleveraging the structure of the KG, a method we name GraphCorrect, and\ndemonstrate that the majority of hallucinations can indeed be rectified.",
    "authors": [
      "Hannah Sansford",
      "Nicholas Richardson",
      "Hermina Petric Maretic",
      "Juba Nait Saada"
    ]
  },
  {
    "title": "AdapTable: Test-Time Adaptation for Tabular Data via Shift-Aware Uncertainty Calibrator and Label Distribution Handler",
    "abstract": "In real-world applications, tabular data often suffer from distribution\nshifts due to their widespread and abundant nature, leading to erroneous\npredictions of pre-trained machine learning models. However, addressing such\ndistribution shifts in the tabular domain has been relatively underexplored due\nto unique challenges such as varying attributes and dataset sizes, as well as\nthe limited representation learning capabilities of deep learning models for\ntabular data. Particularly, with the recent promising paradigm of test-time\nadaptation (TTA), where we adapt the off-the-shelf model to the unlabeled\ntarget domain during the inference phase without accessing the source domain,\nwe observe that directly adopting commonly used TTA methods from other domains\noften leads to model collapse. We systematically explore challenges in tabular\ndata test-time adaptation, including skewed entropy, complex latent space\ndecision boundaries, confidence calibration issues with both overconfident and\nunder-confident, and model bias towards source label distributions along with\nclass imbalances. Based on these insights, we introduce AdapTable, a novel\ntabular test-time adaptation method that directly modifies output probabilities\nby estimating target label distributions and adjusting initial probabilities\nbased on calibrated uncertainty. Extensive experiments on both natural\ndistribution shifts and synthetic corruptions demonstrate the adaptation\nefficacy of the proposed method.",
    "authors": [
      "Changhun Kim",
      "Taewon Kim",
      "Seungyeon Woo",
      "June Yong Yang",
      "Eunho Yang"
    ]
  },
  {
    "title": "MSegRNN:Enhanced SegRNN Model with Mamba for Long-Term Time Series Forecasting",
    "abstract": "The field of long-term time series forecasting demands handling extensive\nlook-back windows and long-range prediction steps, posing significant\nchallenges for RNN-based methodologies. Among these, SegRNN, a robust\nRNN-driven model, has gained considerable attention in LTSF analysis for\nachieving state-of-the-art results while maintaining a remarkably streamlined\narchitecture. Concurrently, the Mamba structure has demonstrated its advantages\nin small to medium-sized models due to its capability for information\nselection. This study introduces a variant of SegRNN that preprocesses\ninformation using a fine-tuned single-layer Mamba structure. Additionally, it\nincorporates implicit segmentation and residual structures into the model's\nencoding section to further reduce the inherent data iterative cycles of RNN\narchitectures and implicitly integrate inter-channel correlations. This\nvariant, named MSegRNN, utilizes the Mamba structure to select useful\ninformation, resulting in a transformed sequence. The linear-strategy-adapted\nderivative retains the superior memory efficiency of the original SegRNN while\ndemonstrating enhanced performance. Empirical evaluations on real-world LTSF\ndatasets demonstrate the superior performance of our model, thereby\ncontributing to the advancement of LTSF methodologies.",
    "authors": [
      "GaoXiang Zhao",
      "XiaoQiang Wang"
    ]
  },
  {
    "title": "Continual Deep Learning on the Edge via Stochastic Local Competition among Subnetworks",
    "abstract": "Continual learning on edge devices poses unique challenges due to stringent\nresource constraints. This paper introduces a novel method that leverages\nstochastic competition principles to promote sparsity, significantly reducing\ndeep network memory footprint and computational demand. Specifically, we\npropose deep networks that comprise blocks of units that compete locally to win\nthe representation of each arising new task; competition takes place in a\nstochastic manner. This type of network organization results in sparse\ntask-specific representations from each network layer; the sparsity pattern is\nobtained during training and is different among tasks. Crucially, our method\nsparsifies both the weights and the weight gradients, thus facilitating\ntraining on edge devices. This is performed on the grounds of winning\nprobability for each unit in a block. During inference, the network retains\nonly the winning unit and zeroes-out all weights pertaining to non-winning\nunits for the task at hand. Thus, our approach is specifically tailored for\ndeployment on edge devices, providing an efficient and scalable solution for\ncontinual learning in resource-limited environments.",
    "authors": [
      "Theodoros Christophides",
      "Kyriakos Tolias",
      "Sotirios Chatzis"
    ]
  },
  {
    "title": "Scaling 3D Reasoning with LMMs to Large Robot Mission Environments Using Datagraphs",
    "abstract": "This paper addresses the challenge of scaling Large Multimodal Models (LMMs)\nto expansive 3D environments. Solving this open problem is especially relevant\nfor robot deployment in many first-responder scenarios, such as\nsearch-and-rescue missions that cover vast spaces. The use of LMMs in these\nsettings is currently hampered by the strict context windows that limit the\nLMM's input size. We therefore introduce a novel approach that utilizes a\ndatagraph structure, which allows the LMM to iteratively query smaller sections\nof a large environment. Using the datagraph in conjunction with graph traversal\nalgorithms, we can prioritize the most relevant locations to the query, thereby\nimproving the scalability of 3D scene language tasks. We illustrate the\ndatagraph using 3D scenes, but these can be easily substituted by other dense\nmodalities that represent the environment, such as pointclouds or Gaussian\nsplats. We demonstrate the potential to use the datagraph for two 3D scene\nlanguage task use cases, in a search-and-rescue mission example.",
    "authors": [
      "W. J. Meijer",
      "A. C. Kemmeren",
      "E. H. J. Riemens",
      "J. E. Fransman",
      "M. van Bekkum",
      "G. J. Burghouts",
      "J. D. van Mil"
    ]
  },
  {
    "title": "Aligning Neuronal Coding of Dynamic Visual Scenes with Foundation Vision Models",
    "abstract": "Our brains represent the ever-changing environment with neurons in a highly\ndynamic fashion. The temporal features of visual pixels in dynamic natural\nscenes are entrapped in the neuronal responses of the retina. It is crucial to\nestablish the intrinsic temporal relationship between visual pixels and\nneuronal responses. Recent foundation vision models have paved an advanced way\nof understanding image pixels. Yet, neuronal coding in the brain largely lacks\na deep understanding of its alignment with pixels. Most previous studies employ\nstatic images or artificial videos derived from static images for emulating\nmore real and complicated stimuli. Despite these simple scenarios effectively\nhelp to separate key factors influencing visual coding, complex temporal\nrelationships receive no consideration. To decompose the temporal features of\nvisual coding in natural scenes, here we propose Vi-ST, a spatiotemporal\nconvolutional neural network fed with a self-supervised Vision Transformer\n(ViT) prior, aimed at unraveling the temporal-based encoding patterns of\nretinal neuronal populations. The model demonstrates robust predictive\nperformance in generalization tests. Furthermore, through detailed ablation\nexperiments, we demonstrate the significance of each temporal module.\nFurthermore, we introduce a visual coding evaluation metric designed to\nintegrate temporal considerations and compare the impact of different numbers\nof neuronal populations on complementary coding. In conclusion, our proposed\nVi-ST demonstrates a novel modeling framework for neuronal coding of dynamic\nvisual scenes in the brain, effectively aligning our brain representation of\nvideo with neuronal activity. The code is available at\nhttps://github.com/wurining/Vi-ST.",
    "authors": [
      "Rining Wu",
      "Feixiang Zhou",
      "Ziwei Yin",
      "Jian K. Liu"
    ]
  },
  {
    "title": "Transforming Agency. On the mode of existence of Large Language Models",
    "abstract": "This paper investigates the ontological characterization of Large Language\nModels (LLMs) like ChatGPT. Between inflationary and deflationary accounts, we\npay special attention to their status as agents. This requires explaining in\ndetail the architecture, processing, and training procedures that enable LLMs\nto display their capacities, and the extensions used to turn LLMs into\nagent-like systems. After a systematic analysis we conclude that a LLM fails to\nmeet necessary and sufficient conditions for autonomous agency in the light of\nembodied theories of mind: the individuality condition (it is not the product\nof its own activity, it is not even directly affected by it), the normativity\ncondition (it does not generate its own norms or goals), and, partially the\ninteractional asymmetry condition (it is not the origin and sustained source of\nits interaction with the environment). If not agents, then ... what are LLMs?\nWe argue that ChatGPT should be characterized as an interlocutor or linguistic\nautomaton, a library-that-talks, devoid of (autonomous) agency, but capable to\nengage performatively on non-purposeful yet purpose-structured and\npurpose-bounded tasks. When interacting with humans, a \"ghostly\" component of\nthe human-machine interaction makes it possible to enact genuine conversational\nexperiences with LLMs. Despite their lack of sensorimotor and biological\nembodiment, LLMs textual embodiment (the training corpus) and resource-hungry\ncomputational embodiment, significantly transform existing forms of human\nagency. Beyond assisted and extended agency, the LLM-human coupling can produce\nmidtended forms of agency, closer to the production of intentional agency than\nto the extended instrumentality of any previous technologies.",
    "authors": [
      "Xabier E. Barandiaran",
      "Lola S. Almendros"
    ]
  },
  {
    "title": "When Synthetic Traces Hide Real Content: Analysis of Stable Diffusion Image Laundering",
    "abstract": "In recent years, methods for producing highly realistic synthetic images have\nsignificantly advanced, allowing the creation of high-quality images from text\nprompts that describe the desired content. Even more impressively, Stable\nDiffusion (SD) models now provide users with the option of creating synthetic\nimages in an image-to-image translation fashion, modifying images in the latent\nspace of advanced autoencoders. This striking evolution, however, brings an\nalarming consequence: it is possible to pass an image through SD autoencoders\nto reproduce a synthetic copy of the image with high realism and almost no\nvisual artifacts. This process, known as SD image laundering, can transform\nreal images into lookalike synthetic ones and risks complicating forensic\nanalysis for content authenticity verification. Our paper investigates the\nforensic implications of image laundering, revealing a serious potential to\nobscure traces of real content, including sensitive and harmful materials that\ncould be mistakenly classified as synthetic, thereby undermining the protection\nof individuals depicted. To address this issue, we propose a two-stage\ndetection pipeline that effectively differentiates between pristine, laundered,\nand fully synthetic images (those generated from text prompts), showing\nrobustness across various conditions. Finally, we highlight another alarming\nproperty of image laundering, which appears to mask the unique artifacts\nexploited by forensic detectors to solve the camera model identification task,\nstrongly undermining their performance. Our experimental code is available at\nhttps://github.com/polimi-ispl/synthetic-image-detection.",
    "authors": [
      "Sara Mandelli",
      "Paolo Bestagini",
      "Stefano Tubaro"
    ]
  },
  {
    "title": "On-Device Training of Fully Quantized Deep Neural Networks on Cortex-M Microcontrollers",
    "abstract": "On-device training of DNNs allows models to adapt and fine-tune to newly\ncollected data or changing domains while deployed on microcontroller units\n(MCUs). However, DNN training is a resource-intensive task, making the\nimplementation and execution of DNN training algorithms on MCUs challenging due\nto low processor speeds, constrained throughput, limited floating-point\nsupport, and memory constraints. In this work, we explore on-device training of\nDNNs for Cortex-M MCUs. We present a method that enables efficient training of\nDNNs completely in place on the MCU using fully quantized training (FQT) and\ndynamic partial gradient updates. We demonstrate the feasibility of our\napproach on multiple vision and time-series datasets and provide insights into\nthe tradeoff between training accuracy, memory overhead, energy, and latency on\nreal hardware.",
    "authors": [
      "Mark Deutel",
      "Frank Hannig",
      "Christopher Mutschler",
      "J\u00fcrgen Teich"
    ]
  },
  {
    "title": "CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated Responses",
    "abstract": "The rapid progress in Large Language Models (LLMs) poses potential risks such\nas generating unethical content. Assessing LLMs' values can help expose their\nmisalignment, but relies on reference-free evaluators, e.g., fine-tuned LLMs or\nclose-source ones like GPT-4, to identify values reflected in generated\nresponses. Nevertheless, these evaluators face two challenges in open-ended\nvalue evaluation: they should align with changing human value definitions with\nminimal annotation, against their own bias (adaptability), and detect varying\nvalue expressions and scenarios robustly (generalizability). To handle these\nchallenges, we introduce CLAVE, a novel framework which integrates two\ncomplementary LLMs, a large one to extract high-level value concepts from a few\nhuman labels, leveraging its extensive knowledge and generalizability, and a\nsmaller one fine-tuned on such concepts to better align with human value\nunderstanding. This dual-model approach enables calibration with any value\nsystems using <100 human-labeled samples per value type. Then we present\nValEval, a comprehensive dataset comprising 13k+ (text,value,label) tuples\nacross diverse domains, covering three major value systems. We benchmark the\ncapabilities of 12+ popular LLM evaluators and analyze their strengths and\nweaknesses. Our findings reveal that combining fine-tuned small models and\nprompt-based large ones serves as a superior balance in value evaluation.",
    "authors": [
      "Jing Yao",
      "Xiaoyuan Yi",
      "Xing Xie"
    ]
  },
  {
    "title": "Sibyl: Simple yet Effective Agent Framework for Complex Real-world Reasoning",
    "abstract": "Existing agents based on large language models (LLMs) demonstrate robust\nproblem-solving capabilities by integrating LLMs' inherent knowledge, strong\nin-context learning and zero-shot capabilities, and the use of tools combined\nwith intricately designed LLM invocation workflows by humans. However, these\nagents still exhibit shortcomings in long-term reasoning and under-use the\npotential of existing tools, leading to noticeable deficiencies in complex\nreal-world reasoning scenarios. To address these limitations, we introduce\nSibyl, a simple yet powerful LLM-based agent framework designed to tackle\ncomplex reasoning tasks by efficiently leveraging a minimal set of tools.\nDrawing inspiration from Global Workspace Theory, Sibyl incorporates a global\nworkspace to enhance the management and sharing of knowledge and conversation\nhistory throughout the system. Furthermore, guided by Society of Mind Theory,\nSibyl implements a multi-agent debate-based jury to self-refine the final\nanswers, ensuring a comprehensive and balanced approach. This approach aims to\nreduce system complexity while expanding the scope of problems solvable-from\nmatters typically resolved by humans in minutes to those requiring hours or\neven days, thus facilitating a shift from System-1 to System-2 thinking. Sibyl\nhas been designed with a focus on scalability and ease of debugging by\nincorporating the concept of reentrancy from functional programming from its\ninception, with the aim of seamless and low effort integration in other LLM\napplications to improve capabilities. Our experimental results on the GAIA\nbenchmark test set reveal that the Sibyl agent instantiated with GPT-4 achieves\nstate-of-the-art performance with an average score of 34.55%, compared to other\nagents based on GPT-4. We hope that Sibyl can inspire more reliable and\nreusable LLM-based agent solutions to address complex real-world reasoning\ntasks.",
    "authors": [
      "Yulong Wang",
      "Tianhao Shen",
      "Lifeng Liu",
      "Jian Xie"
    ]
  },
  {
    "title": "SEMINAR: Search Enhanced Multi-modal Interest Network and Approximate Retrieval for Lifelong Sequential Recommendation",
    "abstract": "The modeling of users' behaviors is crucial in modern recommendation systems.\nA lot of research focuses on modeling users' lifelong sequences, which can be\nextremely long and sometimes exceed thousands of items. These models use the\ntarget item to search for the most relevant items from the historical sequence.\nHowever, training lifelong sequences in click through rate (CTR) prediction or\npersonalized search ranking (PSR) is extremely difficult due to the\ninsufficient learning problem of ID embedding, especially when the IDs in the\nlifelong sequence features do not exist in the samples of training dataset.\nAdditionally, existing target attention mechanisms struggle to learn the\nmulti-modal representations of items in the sequence well. The distribution of\nmulti-modal embedding (text, image and attributes) output of user's interacted\nitems are not properly aligned and there exist divergence across modalities. We\nalso observe that users' search query sequences and item browsing sequences can\nfully depict users' intents and benefit from each other. To address these\nchallenges, we propose a unified lifelong multi-modal sequence model called\nSEMINAR-Search Enhanced Multi-Modal Interest Network and Approximate Retrieval.\nSpecifically, a network called Pretraining Search Unit (PSU) learns the\nlifelong sequences of multi-modal query-item pairs in a pretraining-finetuning\nmanner with multiple objectives: multi-modal alignment, next query-item pair\nprediction, query-item relevance prediction, etc. After pretraining, the\ndownstream model restores the pretrained embedding as initialization and\nfinetunes the network. To accelerate the online retrieval speed of multi-modal\nembedding, we propose a multi-modal codebook-based product quantization\nstrategy to approximate the exact attention calculati",
    "authors": [
      "Kaiming Shen",
      "Xichen Ding",
      "Zixiang Zheng",
      "Yuqi Gong",
      "Qianqian Li",
      "Zhongyi Liu",
      "Guannan Zhang"
    ]
  },
  {
    "title": "Classification of Heart Sounds Using Multi-Branch Deep Convolutional Network and LSTM-CNN",
    "abstract": "This paper presents a fast and cost-effective method for diagnosing cardiac\nabnormalities with high accuracy and reliability using low-cost systems in\nclinics. The primary limitation of automatic diagnosing of cardiac diseases is\nthe rarity of correct and acceptable labeled samples, which can be expensive to\nprepare. To address this issue, two methods are proposed in this work. The\nfirst method is a unique Multi-Branch Deep Convolutional Neural Network (MBDCN)\narchitecture inspired by human auditory processing, specifically designed to\noptimize feature extraction by employing various sizes of convolutional filters\nand audio signal power spectrum as input. In the second method, called as Long\nshort-term memory-Convolutional Neural (LSCN) model, Additionally, the network\narchitecture includes Long Short-Term Memory (LSTM) network blocks to improve\nfeature extraction in the time domain. The innovative approach of combining\nmultiple parallel branches consisting of the one-dimensional convolutional\nlayers along with LSTM blocks helps in achieving superior results in audio\nsignal processing tasks. The experimental results demonstrate superiority of\nthe proposed methods over the state-of-the-art techniques. The overall\nclassification accuracy of heart sounds with the LSCN network is more than 96%.\nThe efficiency of this network is significant compared to common feature\nextraction methods such as Mel Frequency Cepstral Coefficients (MFCC) and\nwavelet transform. Therefore, the proposed method shows promising results in\nthe automatic analysis of heart sounds and has potential applications in the\ndiagnosis and early detection of cardiovascular diseases.",
    "authors": [
      "Seyed Amir Latifi",
      "Hassan Ghassemian",
      "Maryam Imani"
    ]
  },
  {
    "title": "Addressing Image Hallucination in Text-to-Image Generation through Factual Image Retrieval",
    "abstract": "Text-to-image generation has shown remarkable progress with the emergence of\ndiffusion models. However, these models often generate factually inconsistent\nimages, failing to accurately reflect the factual information and common sense\nconveyed by the input text prompts. We refer to this issue as Image\nhallucination. Drawing from studies on hallucinations in language models, we\nclassify this problem into three types and propose a methodology that uses\nfactual images retrieved from external sources to generate realistic images.\nDepending on the nature of the hallucination, we employ off-the-shelf image\nediting tools, either InstructPix2Pix or IP-Adapter, to leverage factual\ninformation from the retrieved image. This approach enables the generation of\nimages that accurately reflect the facts and common sense.",
    "authors": [
      "Youngsun Lim",
      "Hyunjung Shim"
    ]
  },
  {
    "title": "Qwen2 Technical Report",
    "abstract": "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face1 and ModelScope2, and the\nsupplementary materials including example code on GitHub3. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.",
    "authors": [
      "An Yang",
      "Baosong Yang",
      "Binyuan Hui",
      "Bo Zheng",
      "Bowen Yu",
      "Chang Zhou",
      "Chengpeng Li",
      "Chengyuan Li",
      "Dayiheng Liu",
      "Fei Huang",
      "Guanting Dong",
      "Haoran Wei",
      "Huan Lin",
      "Jialong Tang",
      "Jialin Wang",
      "Jian Yang",
      "Jianhong Tu",
      "Jianwei Zhang",
      "Jianxin Ma",
      "Jin Xu",
      "Jingren Zhou",
      "Jinze Bai",
      "Jinzheng He",
      "Junyang Lin",
      "Kai Dang",
      "Keming Lu",
      "Keqin Chen",
      "Kexin Yang",
      "Mei Li",
      "Mingfeng Xue",
      "Na Ni",
      "Pei Zhang",
      "Peng Wang",
      "Ru Peng",
      "Rui Men",
      "Ruize Gao",
      "Runji Lin",
      "Shijie Wang",
      "Shuai Bai",
      "Sinan Tan",
      "Tianhang Zhu",
      "Tianhao Li",
      "Tianyu Liu",
      "Wenbin Ge",
      "Xiaodong Deng",
      "Xiaohuan Zhou",
      "Xingzhang Ren",
      "Xinyu Zhang",
      "Xipin Wei",
      "Xuancheng Ren",
      "Yang Fan",
      "Yang Yao",
      "Yichang Zhang",
      "Yu Wan",
      "Yunfei Chu",
      "Zeyu Cui",
      "Zhenru Zhang",
      "Zhihao Fan"
    ]
  },
  {
    "title": "Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems",
    "abstract": "Retrieval-augmented generation (RAG) techniques leverage the in-context\nlearning capabilities of large language models (LLMs) to produce more accurate\nand relevant responses. Originating from the simple 'retrieve-then-read'\napproach, the RAG framework has evolved into a highly flexible and modular\nparadigm. A critical component, the Query Rewriter module, enhances knowledge\nretrieval by generating a search-friendly query. This method aligns input\nquestions more closely with the knowledge base. Our research identifies\nopportunities to enhance the Query Rewriter module to Query Rewriter+ by\ngenerating multiple queries to overcome the Information Plateaus associated\nwith a single query and by rewriting questions to eliminate Ambiguity, thereby\nclarifying the underlying intent. We also find that current RAG systems exhibit\nissues with Irrelevant Knowledge; to overcome this, we propose the Knowledge\nFilter. These two modules are both based on the instruction-tuned Gemma-2B\nmodel, which together enhance response quality. The final identified issue is\nRedundant Retrieval; we introduce the Memory Knowledge Reservoir and the\nRetriever Trigger to solve this. The former supports the dynamic expansion of\nthe RAG system's knowledge base in a parameter-free manner, while the latter\noptimizes the cost for accessing external knowledge, thereby improving resource\nutilization and response efficiency. These four RAG modules synergistically\nimprove the response quality and efficiency of the RAG system. The\neffectiveness of these modules has been validated through experiments and\nablation studies across six common QA datasets. The source code can be accessed\nat https://github.com/Ancientshi/ERM4.",
    "authors": [
      "Yunxiao Shi",
      "Xing Zi",
      "Zijing Shi",
      "Haimin Zhang",
      "Qiang Wu",
      "Min Xu"
    ]
  },
  {
    "title": "Spatio-temporal neural distance fields for conditional generative modeling of the heart",
    "abstract": "The rhythmic pumping motion of the heart stands as a cornerstone in life, as\nit circulates blood to the entire human body through a series of carefully\ntimed contractions of the individual chambers. Changes in the size, shape and\nmovement of the chambers can be important markers for cardiac disease and\nmodeling this in relation to clinical demography or disease is therefore of\ninterest. Existing methods for spatio-temporal modeling of the human heart\nrequire shape correspondence over time or suffer from large memory\nrequirements, making it difficult to use for complex anatomies. We introduce a\nnovel conditional generative model, where the shape and movement is modeled\nimplicitly in the form of a spatio-temporal neural distance field and\nconditioned on clinical demography. The model is based on an auto-decoder\narchitecture and aims to disentangle the individual variations from that\nrelated to the clinical demography. It is tested on the left atrium (including\nthe left atrial appendage), where it outperforms current state-of-the-art\nmethods for anatomical sequence completion and generates synthetic sequences\nthat realistically mimics the shape and motion of the real left atrium. In\npractice, this means we can infer functional measurements from a static image,\ngenerate synthetic populations with specified demography or disease and\ninvestigate how non-imaging clinical data effect the shape and motion of\ncardiac anatomies.",
    "authors": [
      "Kristine S\u00f8rensen",
      "Paula Diez",
      "Jan Margeta",
      "Yasmin El Youssef",
      "Michael Pham",
      "Jonas Jalili Pedersen",
      "Tobias K\u00fchl",
      "Ole de Backer",
      "Klaus Kofoed",
      "Oscar Camara",
      "Rasmus Paulsen"
    ]
  },
  {
    "title": "XEQ Scale for Evaluating XAI Experience Quality Grounded in Psychometric Theory",
    "abstract": "Explainable Artificial Intelligence (XAI) aims to improve the transparency of\nautonomous decision-making through explanations. Recent literature has\nemphasised users' need for holistic \"multi-shot\" explanations and the ability\nto personalise their engagement with XAI systems. We refer to this user-centred\ninteraction as an XAI Experience. Despite advances in creating XAI experiences,\nevaluating them in a user-centred manner has remained challenging. To address\nthis, we introduce the XAI Experience Quality (XEQ) Scale (pronounced \"Seek\"\nScale), for evaluating the user-centred quality of XAI experiences.\nFurthermore, XEQ quantifies the quality of experiences across four evaluation\ndimensions: learning, utility, fulfilment and engagement. These contributions\nextend the state-of-the-art of XAI evaluation, moving beyond the\none-dimensional metrics frequently developed to assess single-shot\nexplanations. In this paper, we present the XEQ scale development and\nvalidation process, including content validation with XAI experts as well as\ndiscriminant and construct validation through a large-scale pilot study. Out\npilot study results offer strong evidence that establishes the XEQ Scale as a\ncomprehensive framework for evaluating user-centred XAI experiences.",
    "authors": [
      "Anjana Wijekoon",
      "Nirmalie Wiratunga",
      "David Corsar",
      "Kyle Martin",
      "Ikechukwu Nkisi-Orji",
      "Belen D\u00edaz-Agudo",
      "Derek Bridge"
    ]
  },
  {
    "title": "An Empirical Study of Validating Synthetic Data for Formula Generation",
    "abstract": "Large language models (LLMs) can be leveraged to help with writing formulas\nin spreadsheets, but resources on these formulas are scarce, impacting both the\nbase performance of pre-trained models and limiting the ability to fine-tune\nthem. Given a corpus of formulas, we can use a(nother) model to generate\nsynthetic natural language utterances for fine-tuning. However, it is important\nto validate whether the NL generated by the LLM is indeed accurate to be\nbeneficial for fine-tuning. In this paper, we provide empirical results on the\nimpact of validating these synthetic training examples with surrogate\nobjectives that evaluate the accuracy of the synthetic annotations. We\ndemonstrate that validation improves performance over raw data across four\nmodels (2 open and 2 closed weight). Interestingly, we show that although\nvalidation tends to prune more challenging examples, it increases the\ncomplexity of problems that models can solve after being fine-tuned on\nvalidated data.",
    "authors": [
      "Usneek Singh",
      "Jos\u00e9 Cambronero",
      "Sumit Gulwani",
      "Aditya Kanade",
      "Anirudh Khatry",
      "Vu Le",
      "Mukul Singh",
      "Gust Verbruggen"
    ]
  },
  {
    "title": "Prompt Selection Matters: Enhancing Text Annotations for Social Sciences with Large Language Models",
    "abstract": "Large Language Models have recently been applied to text annotation tasks\nfrom social sciences, equalling or surpassing the performance of human workers\nat a fraction of the cost. However, no inquiry has yet been made on the impact\nof prompt selection on labelling accuracy. In this study, we show that\nperformance greatly varies between prompts, and we apply the method of\nautomatic prompt optimization to systematically craft high quality prompts. We\nalso provide the community with a simple, browser-based implementation of the\nmethod at https://prompt-ultra.github.io/ .",
    "authors": [
      "Louis Abraham",
      "Charles Arnal",
      "Antoine Marie"
    ]
  },
  {
    "title": "Risk-aware Trajectory Prediction by Incorporating Spatio-temporal Traffic Interaction Analysis",
    "abstract": "To operate in open-ended environments where humans interact in complex,\ndiverse ways, autonomous robots must learn to predict their behaviour,\nespecially when that behavior is potentially dangerous to other agents or to\nthe robot. However, reducing the risk of accidents requires prior knowledge of\nwhere potential collisions may occur and how. Therefore, we propose to gain\nthis information by analyzing locations and speeds that commonly correspond to\nhigh-risk interactions within the dataset, and use it within training to\ngenerate better predictions in high risk situations. Through these\nlocation-based and speed-based re-weighting techniques, we achieve improved\noverall performance, as measured by most-likely FDE and KDE, as well as\nimproved performance on high-speed vehicles, and vehicles within high-risk\nlocations.\n  2023 IEEE International Conference on Robotics and Automation (ICRA)",
    "authors": [
      "Divya Thuremella",
      "Lewis Ince",
      "Lars Kunze"
    ]
  },
  {
    "title": "Bidirectional Stereo Image Compression with Cross-Dimensional Entropy Model",
    "abstract": "With the rapid advancement of stereo vision technologies, stereo image\ncompression has emerged as a crucial field that continues to draw significant\nattention. Previous approaches have primarily employed a unidirectional\nparadigm, where the compression of one view is dependent on the other,\nresulting in imbalanced compression. To address this issue, we introduce a\nsymmetric bidirectional stereo image compression architecture, named BiSIC.\nSpecifically, we propose a 3D convolution based codec backbone to capture local\nfeatures and incorporate bidirectional attention blocks to exploit global\nfeatures. Moreover, we design a novel cross-dimensional entropy model that\nintegrates various conditioning factors, including the spatial context, channel\ncontext, and stereo dependency, to effectively estimate the distribution of\nlatent representations for entropy coding. Extensive experiments demonstrate\nthat our proposed BiSIC outperforms conventional image/video compression\nstandards, as well as state-of-the-art learning-based methods, in terms of both\nPSNR and MS-SSIM.",
    "authors": [
      "Zhening Liu",
      "Xinjie Zhang",
      "Jiawei Shao",
      "Zehong Lin",
      "Jun Zhang"
    ]
  },
  {
    "title": "Arena Learning: Build Data Flywheel for LLMs Post-training via Simulated Chatbot Arena",
    "abstract": "Assessing the effectiveness of large language models (LLMs) presents\nsubstantial challenges. The method of conducting human-annotated battles in an\nonline Chatbot Arena is a highly effective evaluative technique. However, this\napproach is limited by the costs and time required for human annotation. In\nthis paper, we introduce Arena Learning, an innovative offline strategy\ndesigned to simulate these arena battles using AI-driven annotations to\nevaluate battle outcomes, thus facilitating the continuous improvement of the\ntarget model through both supervised fine-tuning and reinforcement learning.\nArena Learning comprises two key elements. First, it ensures precise\nevaluations and maintains consistency between offline simulations and online\ncompetitions via WizardArena, a pipeline developed to accurately predict the\nElo rankings of various models using a meticulously designed offline test set.\nOur results demonstrate that WizardArena's predictions closely align with those\nfrom the online Arena. Second, it involves the continuous improvement of\ntraining data based on the battle results and the refined model. We establish a\ndata flywheel to iteratively update the training data by highlighting the\nweaknesses of the target model based on its battle results, enabling it to\nlearn from the strengths of multiple different models. We apply Arena Learning\nto train our target model, WizardLM-$\\beta$, and demonstrate significant\nperformance enhancements across various metrics. This fully automated training\nand evaluation pipeline sets the stage for continuous advancements in various\nLLMs via post-training. Notably, Arena Learning plays a pivotal role in the\nsuccess of WizardLM-2, and this paper serves both as an exploration of its\nefficacy and a foundational study for future discussions related to WizardLM-2\nand its derivatives.",
    "authors": [
      "Haipeng Luo",
      "Qingfeng Sun",
      "Can Xu",
      "Pu Zhao",
      "Qingwei Lin",
      "Jianguang Lou",
      "Shifeng Chen",
      "Yansong Tang",
      "Weizhu Chen"
    ]
  },
  {
    "title": "An evaluation of CNN models and data augmentation techniques in hierarchical localization of mobile robots",
    "abstract": "This work presents an evaluation of CNN models and data augmentation to carry\nout the hierarchical localization of a mobile robot by using omnidireccional\nimages. In this sense, an ablation study of different state-of-the-art CNN\nmodels used as backbone is presented and a variety of data augmentation visual\neffects are proposed for addressing the visual localization of the robot. The\nproposed method is based on the adaption and re-training of a CNN with a dual\npurpose: (1) to perform a rough localization step in which the model is used to\npredict the room from which an image was captured, and (2) to address the fine\nlocalization step, which consists in retrieving the most similar image of the\nvisual map among those contained in the previously predicted room by means of a\npairwise comparison between descriptors obtained from an intermediate layer of\nthe CNN. In this sense, we evaluate the impact of different state-of-the-art\nCNN models such as ConvNeXt for addressing the proposed localization. Finally,\na variety of data augmentation visual effects are separately employed for\ntraining the model and their impact is assessed. The performance of the\nresulting CNNs is evaluated under real operation conditions, including changes\nin the lighting conditions. Our code is publicly available on the project\nwebsite https://github.com/juanjo-cabrera/IndoorLocalizationSingleCNN.git",
    "authors": [
      "J. J. Cabrera",
      "O. J. C\u00e9spedes",
      "S. Cebollada",
      "O. Reinoso",
      "L. Pay\u00e1"
    ]
  },
  {
    "title": "Three Dogmas of Reinforcement Learning",
    "abstract": "Modern reinforcement learning has been conditioned by at least three dogmas.\nThe first is the environment spotlight, which refers to our tendency to focus\non modeling environments rather than agents. The second is our treatment of\nlearning as finding the solution to a task, rather than adaptation. The third\nis the reward hypothesis, which states that all goals and purposes can be well\nthought of as maximization of a reward signal. These three dogmas shape much of\nwhat we think of as the science of reinforcement learning. While each of the\ndogmas have played an important role in developing the field, it is time we\nbring them to the surface and reflect on whether they belong as basic\ningredients of our scientific paradigm. In order to realize the potential of\nreinforcement learning as a canonical frame for researching intelligent agents,\nwe suggest that it is time we shed dogmas one and two entirely, and embrace a\nnuanced approach to the third.",
    "authors": [
      "David Abel",
      "Mark K. Ho",
      "Anna Harutyunyan"
    ]
  },
  {
    "title": "Boosting Zero-Shot Crosslingual Performance using LLM-Based Augmentations with Effective Data Selection",
    "abstract": "Large language models (LLMs) are very proficient text generators. We leverage\nthis capability of LLMs to generate task-specific data via zero-shot prompting\nand promote cross-lingual transfer for low-resource target languages. Given\ntask-specific data in a source language and a teacher model trained on this\ndata, we propose using this teacher to label LLM generations and employ a set\nof simple data selection strategies that use the teacher's label probabilities.\nOur data selection strategies help us identify a representative subset of\ndiverse generations that help boost zero-shot accuracies while being efficient,\nin comparison to using all the LLM generations (without any subset selection).\nWe also highlight other important design choices that affect cross-lingual\nperformance such as the use of translations of source data and what labels are\nbest to use for the LLM generations. We observe significant performance gains\nacross sentiment analysis and natural language inference tasks (of up to a\nmaximum of 7.13 absolute points and 1.5 absolute points on average) across a\nnumber of target languages (Hindi, Marathi, Urdu, Swahili) and domains.",
    "authors": [
      "Barah Fazili",
      "Ashish Sunil Agrawal",
      "Preethi Jyothi"
    ]
  },
  {
    "title": "Leveraging Hybrid Intelligence Towards Sustainable and Energy-Efficient Machine Learning",
    "abstract": "Hybrid intelligence aims to enhance decision-making, problem-solving, and\noverall system performance by combining the strengths of both, human cognitive\nabilities and artificial intelligence. With the rise of Large Language Models\n(LLM), progressively participating as smart agents to accelerate machine\nlearning development, Hybrid Intelligence is becoming an increasingly important\ntopic for effective interaction between humans and machines. This paper\npresents an approach to leverage Hybrid Intelligence towards sustainable and\nenergy-aware machine learning. When developing machine learning models, final\nmodel performance commonly rules the optimization process while the efficiency\nof the process itself is often neglected. Moreover, in recent times, energy\nefficiency has become equally crucial due to the significant environmental\nimpact of complex and large-scale computational processes. The contribution of\nthis work covers the interactive inclusion of secondary knowledge sources\nthrough Human-in-the-loop (HITL) and LLM agents to stress out and further\nresolve inefficiencies in the machine learning development process.",
    "authors": [
      "Daniel Geissler",
      "Paul Lukowicz"
    ]
  },
  {
    "title": "Learning Social Cost Functions for Human-Aware Path Planning",
    "abstract": "Achieving social acceptance is one of the main goals of Social Robotic\nNavigation. Despite this topic has received increasing interest in recent\nyears, most of the research has focused on driving the robotic agent along\nobstacle-free trajectories, planning around estimates of future human motion to\nrespect personal distances and optimize navigation. However, social\ninteractions in everyday life are also dictated by norms that do not strictly\ndepend on movement, such as when standing at the end of a queue rather than\ncutting it. In this paper, we propose a novel method to recognize common social\nscenarios and modify a traditional planner's cost function to adapt to them.\nThis solution enables the robot to carry out different social navigation\nbehaviors that would not arise otherwise, maintaining the robustness of\ntraditional navigation. Our approach allows the robot to learn different social\nnorms with a single learned model, rather than having different modules for\neach task. As a proof of concept, we consider the tasks of queuing and respect\ninteraction spaces of groups of people talking to one another, but the method\ncan be extended to other human activities that do not involve motion.",
    "authors": [
      "Andrea Eirale",
      "Matteo Leonetti",
      "Marcello Chiaberge"
    ]
  },
  {
    "title": "Efficient Continual Learning with Low Memory Footprint For Edge Device",
    "abstract": "Continual learning(CL) is a useful technique to acquire dynamic knowledge\ncontinually. Although powerful cloud platforms can fully exert the ability of\nCL,e.g., customized recommendation systems, similar personalized requirements\nfor edge devices are almost disregarded. This phenomenon stems from the huge\nresource overhead involved in training neural networks and overcoming the\nforgetting problem of CL. This paper focuses on these scenarios and proposes a\ncompact algorithm called LightCL. Different from other CL methods bringing huge\nresource consumption to acquire generalizability among all tasks for delaying\nforgetting, LightCL compress the resource consumption of already generalized\ncomponents in neural networks and uses a few extra resources to improve memory\nin other parts. We first propose two new metrics of learning plasticity and\nmemory stability to seek generalizability during CL. Based on the discovery\nthat lower and middle layers have more generalizability and deeper layers are\nopposite, we $\\textit{Maintain Generalizability}$ by freezing the lower and\nmiddle layers. Then, we $\\textit{Memorize Feature Patterns}$ to stabilize the\nfeature extracting patterns of previous tasks to improve generalizability in\ndeeper layers. In the experimental comparison, LightCL outperforms other SOTA\nmethods in delaying forgetting and reduces at most $\\textbf{6.16$\\times$}$\nmemory footprint, proving the excellent performance of LightCL in efficiency.\nWe also evaluate the efficiency of our method on an edge device, the Jetson\nNano, which further proves our method's practical effectiveness.",
    "authors": [
      "Zeqing Wang",
      "Fei Cheng",
      "Kangye Ji",
      "Bohu Huang"
    ]
  },
  {
    "title": "Understanding the Dependence of Perception Model Competency on Regions in an Image",
    "abstract": "While deep neural network (DNN)-based perception models are useful for many\napplications, these models are black boxes and their outputs are not yet well\nunderstood. To confidently enable a real-world, decision-making system to\nutilize such a perception model without human intervention, we must enable the\nsystem to reason about the perception model's level of competency and respond\nappropriately when the model is incompetent. In order for the system to make an\nintelligent decision about the appropriate action when the model is\nincompetent, it would be useful for the system to understand why the model is\nincompetent. We explore five novel methods for identifying regions in the input\nimage contributing to low model competency, which we refer to as image\ncropping, segment masking, pixel perturbation, competency gradients, and\nreconstruction loss. We assess the ability of these five methods to identify\nunfamiliar objects, recognize regions associated with unseen classes, and\nidentify unexplored areas in an environment. We find that the competency\ngradients and reconstruction loss methods show great promise in identifying\nregions associated with low model competency, particularly when aspects of the\nimage that are unfamiliar to the perception model are causing this reduction in\ncompetency. Both of these methods boast low computation times and high levels\nof accuracy in detecting image regions that are unfamiliar to the model,\nallowing them to provide potential utility in decision-making pipelines. The\ncode for reproducing our methods and results is available on GitHub:\nhttps://github.com/sarapohland/explainable-competency.",
    "authors": [
      "Sara Pohland",
      "Claire Tomlin"
    ]
  },
  {
    "title": "3D Geometric Shape Assembly via Efficient Point Cloud Matching",
    "abstract": "Learning to assemble geometric shapes into a larger target structure is a\npivotal task in various practical applications. In this work, we tackle this\nproblem by establishing local correspondences between point clouds of part\nshapes in both coarse- and fine-levels. To this end, we introduce Proxy Match\nTransform (PMT), an approximate high-order feature transform layer that enables\nreliable matching between mating surfaces of parts while incurring low costs in\nmemory and computation. Building upon PMT, we introduce a new framework, dubbed\nProxy Match TransformeR (PMTR), for the geometric assembly task. We evaluate\nthe proposed PMTR on the large-scale 3D geometric shape assembly benchmark\ndataset of Breaking Bad and demonstrate its superior performance and efficiency\ncompared to state-of-the-art methods. Project page:\nhttps://nahyuklee.github.io/pmtr.",
    "authors": [
      "Nahyuk Lee",
      "Juhong Min",
      "Junha Lee",
      "Seungwook Kim",
      "Kanghee Lee",
      "Jaesik Park",
      "Minsu Cho"
    ]
  },
  {
    "title": "An experimental evaluation of Siamese Neural Networks for robot localization using omnidirectional imaging in indoor environments",
    "abstract": "The objective of this paper is to address the localization problem using\nomnidirectional images captured by a catadioptric vision system mounted on the\nrobot. For this purpose, we explore the potential of Siamese Neural Networks\nfor modeling indoor environments using panoramic images as the unique source of\ninformation. Siamese Neural Networks are characterized by their ability to\ngenerate a similarity function between two input data, in this case, between\ntwo panoramic images. In this study, Siamese Neural Networks composed of two\nConvolutional Neural Networks (CNNs) are used. The output of each CNN is a\ndescriptor which is used to characterize each image. The dissimilarity of the\nimages is computed by measuring the distance between these descriptors. This\nfact makes Siamese Neural Networks particularly suitable to perform image\nretrieval tasks. First, we evaluate an initial task strongly related to\nlocalization that consists in detecting whether two images have been captured\nin the same or in different rooms. Next, we assess Siamese Neural Networks in\nthe context of a global localization problem. The results outperform previous\ntechniques for solving the localization task using the COLD-Freiburg dataset,\nin a variety of lighting conditions, specially when using images captured in\ncloudy and night conditions.",
    "authors": [
      "J. J. Cabrera",
      "V. Rom\u00e1n",
      "A. Gil",
      "O. Reinoso",
      "L. Pay\u00e1"
    ]
  },
  {
    "title": "TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription Prediction",
    "abstract": "Traditional Chinese medicine (TCM) relies on specific combinations of herbs\nin prescriptions to treat symptoms and signs, a practice that spans thousands\nof years. Predicting TCM prescriptions presents a fascinating technical\nchallenge with practical implications. However, this task faces limitations due\nto the scarcity of high-quality clinical datasets and the intricate\nrelationship between symptoms and herbs. To address these issues, we introduce\nDigestDS, a new dataset containing practical medical records from experienced\nexperts in digestive system diseases. We also propose a method, TCM-FTP (TCM\nFine-Tuning Pre-trained), to leverage pre-trained large language models (LLMs)\nthrough supervised fine-tuning on DigestDS. Additionally, we enhance\ncomputational efficiency using a low-rank adaptation technique. TCM-FTP also\nincorporates data augmentation by permuting herbs within prescriptions,\ncapitalizing on their order-agnostic properties. Impressively, TCM-FTP achieves\nan F1-score of 0.8031, surpassing previous methods significantly. Furthermore,\nit demonstrates remarkable accuracy in dosage prediction, achieving a\nnormalized mean square error of 0.0604. In contrast, LLMs without fine-tuning\nperform poorly. Although LLMs have shown capabilities on a wide range of tasks,\nthis work illustrates the importance of fine-tuning for TCM prescription\nprediction, and we have proposed an effective way to do that.",
    "authors": [
      "Xingzhi Zhou",
      "Xin Dong",
      "Chunhao Li",
      "Yuning Bai",
      "Yulong Xu",
      "Ka Chun Cheung",
      "Simon See",
      "Xinpeng Song",
      "Runshun Zhang",
      "Xuezhong Zhou",
      "Nevin L. Zhang"
    ]
  },
  {
    "title": "Learning Dynamics of LLM Finetuning",
    "abstract": "Learning dynamics, which describes how the learning of specific training\nexamples influences the model's prediction of other examples, give us a\npowerful tool for understanding the behavior of deep learning systems. We study\nthe learning dynamics of large language models during finetuning, by analyzing\nthe step-wise decomposition and accumulated influence among different\nresponses. Our framework allows a uniform interpretation of many interesting\nobservations about the training of popular algorithms for both instruction\ntuning and preference tuning. The analysis not only explains where the benefits\nof these methods come from but also inspires a simple, effective method to\nfurther improve the alignment performance. Code for experiments is available at\nhttps://github.com/Joshua-Ren/Learning_dynamics_LLM.",
    "authors": [
      "Yi Ren",
      "Danica J. Sutherland"
    ]
  },
  {
    "title": "How and where does CLIP process negation?",
    "abstract": "Various benchmarks have been proposed to test linguistic understanding in\npre-trained vision \\& language (VL) models. Here we build on the existence task\nfrom the VALSE benchmark (Parcalabescu et al, 2022) which we use to test\nmodels' understanding of negation, a particularly interesting issue for\nmultimodal models. However, while such VL benchmarks are useful for measuring\nmodel performance, they do not reveal anything about the internal processes\nthrough which these models arrive at their outputs in such visio-linguistic\ntasks. We take inspiration from the growing literature on model\ninterpretability to explain the behaviour of VL models on the understanding of\nnegation. Specifically, we approach these questions through an in-depth\nanalysis of the text encoder in CLIP (Radford et al, 2021), a highly\ninfluential VL model. We localise parts of the encoder that process negation\nand analyse the role of attention heads in this task. Our contributions are\nthreefold. We demonstrate how methods from the language model interpretability\nliterature (such as causal tracing) can be translated to multimodal models and\ntasks; we provide concrete insights into how CLIP processes negation on the\nVALSE existence task; and we highlight inherent limitations in the VALSE\ndataset as a benchmark for linguistic understanding.",
    "authors": [
      "Vincent Quantmeyer",
      "Pablo Mosteiro",
      "Albert Gatt"
    ]
  },
  {
    "title": "IDEAL: Leveraging Infinite and Dynamic Characterizations of Large Language Models for Query-focused Summarization",
    "abstract": "Query-focused summarization (QFS) aims to produce summaries that answer\nparticular questions of interest, enabling greater user control and\npersonalization. With the advent of large language models (LLMs), shows their\nimpressive capability of textual understanding through large-scale pretraining,\nwhich implies the great potential of extractive snippet generation. In this\npaper, we systematically investigated two indispensable characteristics that\nthe LLMs-based QFS models should be harnessed, Lengthy Document Summarization\nand Efficiently Fine-grained Query-LLM Alignment, respectively.\nCorrespondingly, we propose two modules called Query-aware HyperExpert and\nQuery-focused Infini-attention to access the aforementioned characteristics.\nThese innovations pave the way for broader application and accessibility in the\nfield of QFS technology. Extensive experiments conducted on existing QFS\nbenchmarks indicate the effectiveness and generalizability of the proposed\napproach. Our code is publicly available at\nhttps://github.com/DCDmllm/IDEAL_Summary.",
    "authors": [
      "Jie Cao",
      "Dian Jiao",
      "Qiang Yan",
      "Wenqiao Zhang",
      "Siliang Tang",
      "Yueting Zhuang"
    ]
  },
  {
    "title": "SuperPADL: Scaling Language-Directed Physics-Based Control with Progressive Supervised Distillation",
    "abstract": "Physically-simulated models for human motion can generate high-quality\nresponsive character animations, often in real-time. Natural language serves as\na flexible interface for controlling these models, allowing expert and\nnon-expert users to quickly create and edit their animations. Many recent\nphysics-based animation methods, including those that use text interfaces,\ntrain control policies using reinforcement learning (RL). However, scaling\nthese methods beyond several hundred motions has remained challenging.\nMeanwhile, kinematic animation models are able to successfully learn from\nthousands of diverse motions by leveraging supervised learning methods.\nInspired by these successes, in this work we introduce SuperPADL, a scalable\nframework for physics-based text-to-motion that leverages both RL and\nsupervised learning to train controllers on thousands of diverse motion clips.\nSuperPADL is trained in stages using progressive distillation, starting with a\nlarge number of specialized experts using RL. These experts are then\niteratively distilled into larger, more robust policies using a combination of\nreinforcement learning and supervised learning. Our final SuperPADL controller\nis trained on a dataset containing over 5000 skills and runs in real time on a\nconsumer GPU. Moreover, our policy can naturally transition between skills,\nallowing for users to interactively craft multi-stage animations. We\nexperimentally demonstrate that SuperPADL significantly outperforms RL-based\nbaselines at this large data scale.",
    "authors": [
      "Jordan Juravsky",
      "Yunrong Guo",
      "Sanja Fidler",
      "Xue Bin Peng"
    ]
  },
  {
    "title": "Kinetic Typography Diffusion Model",
    "abstract": "This paper introduces a method for realistic kinetic typography that\ngenerates user-preferred animatable 'text content'. We draw on recent advances\nin guided video diffusion models to achieve visually-pleasing text appearances.\nTo do this, we first construct a kinetic typography dataset, comprising about\n600K videos. Our dataset is made from a variety of combinations in 584\ntemplates designed by professional motion graphics designers and involves\nchanging each letter's position, glyph, and size (i.e., flying, glitches,\nchromatic aberration, reflecting effects, etc.). Next, we propose a video\ndiffusion model for kinetic typography. For this, there are three requirements:\naesthetic appearances, motion effects, and readable letters. This paper\nidentifies the requirements. For this, we present static and dynamic captions\nused as spatial and temporal guidance of a video diffusion model, respectively.\nThe static caption describes the overall appearance of the video, such as\ncolors, texture and glyph which represent a shape of each letter. The dynamic\ncaption accounts for the movements of letters and backgrounds. We add one more\nguidance with zero convolution to determine which text content should be\nvisible in the video. We apply the zero convolution to the text content, and\nimpose it on the diffusion model. Lastly, our glyph loss, only minimizing a\ndifference between the predicted word and its ground-truth, is proposed to make\nthe prediction letters readable. Experiments show that our model generates\nkinetic typography videos with legible and artistic letter motions based on\ntext prompts.",
    "authors": [
      "Seonmi Park",
      "Inhwan Bae",
      "Seunghyun Shin",
      "Hae-Gon Jeon"
    ]
  },
  {
    "title": "GROOT: Generating Robust Watermark for Diffusion-Model-Based Audio Synthesis",
    "abstract": "Amid the burgeoning development of generative models like diffusion models,\nthe task of differentiating synthesized audio from its natural counterpart\ngrows more daunting. Deepfake detection offers a viable solution to combat this\nchallenge. Yet, this defensive measure unintentionally fuels the continued\nrefinement of generative models. Watermarking emerges as a proactive and\nsustainable tactic, preemptively regulating the creation and dissemination of\nsynthesized content. Thus, this paper, as a pioneer, proposes the generative\nrobust audio watermarking method (Groot), presenting a paradigm for proactively\nsupervising the synthesized audio and its source diffusion models. In this\nparadigm, the processes of watermark generation and audio synthesis occur\nsimultaneously, facilitated by parameter-fixed diffusion models equipped with a\ndedicated encoder. The watermark embedded within the audio can subsequently be\nretrieved by a lightweight decoder. The experimental results highlight Groot's\noutstanding performance, particularly in terms of robustness, surpassing that\nof the leading state-of-the-art methods. Beyond its impressive resilience\nagainst individual post-processing attacks, Groot exhibits exceptional\nrobustness when facing compound attacks, maintaining an average watermark\nextraction accuracy of around 95%.",
    "authors": [
      "Weizhi Liu",
      "Yue Li",
      "Dongdong Lin",
      "Hui Tian",
      "Haizhou Li"
    ]
  },
  {
    "title": "LiteFocus: Accelerated Diffusion Inference for Long Audio Synthesis",
    "abstract": "Latent diffusion models have shown promising results in audio generation,\nmaking notable advancements over traditional methods. However, their\nperformance, while impressive with short audio clips, faces challenges when\nextended to longer audio sequences. These challenges are due to model's\nself-attention mechanism and training predominantly on 10-second clips, which\ncomplicates the extension to longer audio without adaptation. In response to\nthese issues, we introduce a novel approach, LiteFocus that enhances the\ninference of existing audio latent diffusion models in long audio synthesis.\nObserved the attention pattern in self-attention, we employ a dual sparse form\nfor attention calculation, designated as same-frequency focus and\ncross-frequency compensation, which curtails the attention computation under\nsame-frequency constraints, while enhancing audio quality through\ncross-frequency refillment. LiteFocus demonstrates substantial reduction on\ninference time with diffusion-based TTA model by 1.99x in synthesizing\n80-second audio clips while also obtaining improved audio quality.",
    "authors": [
      "Zhenxiong Tan",
      "Xinyin Ma",
      "Gongfan Fang",
      "Xinchao Wang"
    ]
  },
  {
    "title": "BandControlNet: Parallel Transformers-based Steerable Popular Music Generation with Fine-Grained Spatiotemporal Features",
    "abstract": "Controllable music generation promotes the interaction between humans and\ncomposition systems by projecting the users' intent on their desired music. The\nchallenge of introducing controllability is an increasingly important issue in\nthe symbolic music generation field. When building controllable generative\npopular multi-instrument music systems, two main challenges typically present\nthemselves, namely weak controllability and poor music quality. To address\nthese issues, we first propose spatiotemporal features as powerful and\nfine-grained controls to enhance the controllability of the generative model.\nIn addition, an efficient music representation called REMI_Track is designed to\nconvert multitrack music into multiple parallel music sequences and shorten the\nsequence length of each track with Byte Pair Encoding (BPE) techniques.\nSubsequently, we release BandControlNet, a conditional model based on parallel\nTransformers, to tackle the multiple music sequences and generate high-quality\nmusic samples that are conditioned to the given spatiotemporal control\nfeatures. More concretely, the two specially designed modules of\nBandControlNet, namely structure-enhanced self-attention (SE-SA) and\nCross-Track Transformer (CTT), are utilized to strengthen the resulting musical\nstructure and inter-track harmony modeling respectively. Experimental results\ntested on two popular music datasets of different lengths demonstrate that the\nproposed BandControlNet outperforms other conditional music generation models\non most objective metrics in terms of fidelity and inference speed and shows\ngreat robustness in generating long music samples. The subjective evaluations\nshow BandControlNet trained on short datasets can generate music with\ncomparable quality to state-of-the-art models, while outperforming them\nsignificantly using longer datasets.",
    "authors": [
      "Jing Luo",
      "Xinyu Yang",
      "Dorien Herremans"
    ]
  },
  {
    "title": "The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism",
    "abstract": "Current evaluations of large language models (LLMs) often overlook\nnon-determinism, typically focusing on a single output per example. This limits\nour understanding of LLM performance variability in real-world applications.\nOur study addresses this issue by exploring key questions about the performance\ndifferences between greedy decoding and sampling, identifying benchmarks'\nconsistency regarding non-determinism, and examining unique model behaviors.\nThrough extensive experiments, we observe that greedy decoding generally\noutperforms sampling methods for most evaluated tasks. We also observe\nconsistent performance across different LLM sizes and alignment methods, noting\nthat alignment can reduce sampling variance. Moreover, our best-of-N sampling\napproach demonstrates that smaller LLMs can match or surpass larger models such\nas GPT-4-Turbo, highlighting the untapped potential of smaller LLMs. This\nresearch shows the importance of considering non-determinism in LLM evaluations\nand provides insights for future LLM development and evaluation.",
    "authors": [
      "Yifan Song",
      "Guoyin Wang",
      "Sujian Li",
      "Bill Yuchen Lin"
    ]
  },
  {
    "title": "GraphPrint: Extracting Features from 3D Protein Structure for Drug Target Affinity Prediction",
    "abstract": "Accurate drug target affinity prediction can improve drug candidate\nselection, accelerate the drug discovery process, and reduce drug production\ncosts. Previous work focused on traditional fingerprints or used features\nextracted based on the amino acid sequence in the protein, ignoring its 3D\nstructure which affects its binding affinity. In this work, we propose\nGraphPrint: a framework for incorporating 3D protein structure features for\ndrug target affinity prediction. We generate graph representations for protein\n3D structures using amino acid residue location coordinates and combine them\nwith drug graph representation and traditional features to jointly learn drug\ntarget affinity. Our model achieves a mean square error of 0.1378 and a\nconcordance index of 0.8929 on the KIBA dataset and improves over using\ntraditional protein features alone. Our ablation study shows that the 3D\nprotein structure-based features provide information complementary to\ntraditional features.",
    "authors": [
      "Amritpal Singh"
    ]
  },
  {
    "title": "DDFAD: Dataset Distillation Framework for Audio Data",
    "abstract": "Deep neural networks (DNNs) have achieved significant success in numerous\napplications. The remarkable performance of DNNs is largely attributed to the\navailability of massive, high-quality training datasets. However, processing\nsuch massive training data requires huge computational and storage resources.\nDataset distillation is a promising solution to this problem, offering the\ncapability to compress a large dataset into a smaller distilled dataset. The\nmodel trained on the distilled dataset can achieve comparable performance to\nthe model trained on the whole dataset.\n  While dataset distillation has been demonstrated in image data, none have\nexplored dataset distillation for audio data. In this work, for the first time,\nwe propose a Dataset Distillation Framework for Audio Data (DDFAD).\nSpecifically, we first propose the Fused Differential MFCC (FD-MFCC) as\nextracted features for audio data. After that, the FD-MFCC is distilled through\nthe matching training trajectory distillation method. Finally, we propose an\naudio signal reconstruction algorithm based on the Griffin-Lim Algorithm to\nreconstruct the audio signal from the distilled FD-MFCC. Extensive experiments\ndemonstrate the effectiveness of DDFAD on various audio datasets. In addition,\nwe show that DDFAD has promising application prospects in many applications,\nsuch as continual learning and neural architecture search.",
    "authors": [
      "Wenbo Jiang",
      "Rui Zhang",
      "Hongwei Li",
      "Xiaoyuan Liu",
      "Haomiao Yang",
      "Shui Yu"
    ]
  },
  {
    "title": "Backdoor Attacks against Image-to-Image Networks",
    "abstract": "Recently, deep learning-based Image-to-Image (I2I) networks have become the\npredominant choice for I2I tasks such as image super-resolution and denoising.\nDespite their remarkable performance, the backdoor vulnerability of I2I\nnetworks has not been explored. To fill this research gap, we conduct a\ncomprehensive investigation on the susceptibility of I2I networks to backdoor\nattacks. Specifically, we propose a novel backdoor attack technique, where the\ncompromised I2I network behaves normally on clean input images, yet outputs a\npredefined image of the adversary for malicious input images containing the\ntrigger. To achieve this I2I backdoor attack, we propose a targeted universal\nadversarial perturbation (UAP) generation algorithm for I2I networks, where the\ngenerated UAP is used as the backdoor trigger. Additionally, in the backdoor\ntraining process that contains the main task and the backdoor task, multi-task\nlearning (MTL) with dynamic weighting methods is employed to accelerate\nconvergence rates. In addition to attacking I2I tasks, we extend our I2I\nbackdoor to attack downstream tasks, including image classification and object\ndetection. Extensive experiments demonstrate the effectiveness of the I2I\nbackdoor on state-of-the-art I2I network architectures, as well as the\nrobustness against different mainstream backdoor defenses.",
    "authors": [
      "Wenbo Jiang",
      "Hongwei Li",
      "Jiaming He",
      "Rui Zhang",
      "Guowen Xu",
      "Tianwei Zhang",
      "Rongxing Lu"
    ]
  },
  {
    "title": "Enhancing Building Safety Design for Active Shooter Incidents: Exploration of Building Exit Parameters using Reinforcement Learning-Based Simulations",
    "abstract": "With the alarming rise in active shooter incidents (ASIs) in the United\nStates, enhancing public safety through building design has become a pressing\nneed. This study proposes a reinforcement learning-based simulation approach\naddressing gaps in existing research that has neglected the dynamic behaviours\nof shooters. We developed an autonomous agent to simulate an active shooter\nwithin a realistic office environment, aiming to offer insights into the\ninteractions between building design parameters and ASI outcomes. A case study\nis conducted to quantitatively investigate the impact of building exit numbers\n(total count of accessible exits) and configuration (arrangement of which exits\nare available or not) on evacuation and harm rates. Findings demonstrate that\ngreater exit availability significantly improves evacuation outcomes and\nreduces harm. Exits nearer to the shooter's initial position hold greater\nimportance for accessibility than those farther away. By encompassing dynamic\nshooter behaviours, this study offers preliminary insights into effective\nbuilding safety design against evolving threats.",
    "authors": [
      "Ruying Liu",
      "Wanjing Wu",
      "Burcin Becerik-Gerber",
      "Gale M. Lucas"
    ]
  },
  {
    "title": "A Multi-Stage Framework for 3D Individual Tooth Segmentation in Dental CBCT",
    "abstract": "Cone beam computed tomography (CBCT) is a common way of diagnosing dental\nrelated diseases. Accurate segmentation of 3D tooth is of importance for the\ntreatment. Although deep learning based methods have achieved convincing\nresults in medical image processing, they need a large of annotated data for\nnetwork training, making it very time-consuming in data collection and\nannotation. Besides, domain shift widely existing in the distribution of data\nacquired by different devices impacts severely the model generalization. To\nresolve the problem, we propose a multi-stage framework for 3D tooth\nsegmentation in dental CBCT, which achieves the third place in the\n\"Semi-supervised Teeth Segmentation\" 3D (STS-3D) challenge. The experiments on\nvalidation set compared with other semi-supervised segmentation methods further\nindicate the validity of our approach.",
    "authors": [
      "Chunshi Wang",
      "Bin Zhao",
      "Shuxue Ding"
    ]
  },
  {
    "title": "Expanding the Scope: Inductive Knowledge Graph Reasoning with Multi-Starting Progressive Propagation",
    "abstract": "Knowledge graphs (KGs) are widely acknowledged as incomplete, and new\nentities are constantly emerging in the real world. Inductive KG reasoning aims\nto predict missing facts for these new entities. Among existing models, graph\nneural networks (GNNs) based ones have shown promising performance for this\ntask. However, they are still challenged by inefficient message propagation due\nto the distance and scalability issues. In this paper, we propose a new\ninductive KG reasoning model, MStar, by leveraging conditional message passing\nneural networks (C-MPNNs). Our key insight is to select multiple query-specific\nstarting entities to expand the scope of progressive propagation. To propagate\nquery-related messages to a farther area within limited steps, we subsequently\ndesign a highway layer to propagate information toward these selected starting\nentities. Moreover, we introduce a training strategy called LinkVerify to\nmitigate the impact of noisy training samples. Experimental results validate\nthat MStar achieves superior performance compared with state-of-the-art models,\nespecially for distant entities.",
    "authors": [
      "Zhoutian Shao",
      "Yuanning Cui",
      "Wei Hu"
    ]
  },
  {
    "title": "Empowering LLMs for Verilog Generation through Multi-Level Summarization",
    "abstract": "The increasing complexity and high costs associated with modern processor\ndesign have led to a surge in demand for processor design automation.\nInstruction-tuned large language models (LLMs) have demonstrated remarkable\nperformance in automatically generating code for general-purpose programming\nlanguages like Python. However, these methods fail on hardware description\nlanguages (HDLs) like Verilog due to the scarcity of high-quality instruction\ntuning data, as even advanced LLMs like GPT-3.5 exhibit limited performance on\nVerilog generation. Regarding this issue, we observe that (1) Verilog code\ncollected from the real world has higher quality than those generated by LLMs.\n(2) LLMs like GPT-3.5 excel in summarizing Verilog code rather than generating\nit. Based on these observations, this paper introduces CodeV, a series of\nopen-source instruction-tuned Verilog generation LLMs. Instead of generating\ndescriptions first and then getting the corresponding code from advanced LLMs,\nwe prompt the LLM with Verilog code and let the LLM generate the corresponding\nnatural language description by multi-level summarization. Experimental results\nshow that CodeV relatively surpasses the previous open-source SOTA by 14.4%\n(BetterV in VerilogEval) and 11.3% (RTLCoder in RTLLM) respectively, and also\nrelatively outperforms previous commercial SOTA GPT-4 by 22.1% in VerilogEval.",
    "authors": [
      "Yang Zhao",
      "Di Huang",
      "Chongxiao Li",
      "Pengwei Jin",
      "Ziyuan Nan",
      "Tianyun Ma",
      "Lei Qi",
      "Yansong Pan",
      "Zhenxing Zhang",
      "Rui Zhang",
      "Xishan Zhang",
      "Zidong Du",
      "Qi Guo",
      "Xing Hu",
      "Yunji Chen"
    ]
  },
  {
    "title": "Learning Rapid Turning, Aerial Reorientation, and Balancing using Manipulator as a Tail",
    "abstract": "In this research, we investigated the innovative use of a manipulator as a\ntail in quadruped robots to augment their physical capabilities. Previous\nstudies have primarily focused on enhancing various abilities by attaching\nrobotic tails that function solely as tails on quadruped robots. While these\ntails improve the performance of the robots, they come with several\ndisadvantages, such as increased overall weight and higher costs. To mitigate\nthese limitations, we propose the use of a 6-DoF manipulator as a tail,\nallowing it to serve both as a tail and as a manipulator. To control this\nhighly complex robot, we developed a controller based on reinforcement learning\nfor the robot equipped with the manipulator. Our experimental results\ndemonstrate that robots equipped with a manipulator outperform those without a\nmanipulator in tasks such as rapid turning, aerial reorientation, and\nbalancing. These results indicate that the manipulator can improve the agility\nand stability of quadruped robots, similar to a tail, in addition to its\nmanipulation capabilities.",
    "authors": [
      "Insung Yang",
      "Jemin Hwangbo"
    ]
  },
  {
    "title": "Melon Fruit Detection and Quality Assessment Using Generative AI-Based Image Data Augmentation",
    "abstract": "Monitoring and managing the growth and quality of fruits are very important\ntasks. To effectively train deep learning models like YOLO for real-time fruit\ndetection, high-quality image datasets are essential. However, such datasets\nare often lacking in agriculture. Generative AI models can help create\nhigh-quality images. In this study, we used MidJourney and Firefly tools to\ngenerate images of melon greenhouses and post-harvest fruits through\ntext-to-image, pre-harvest image-to-image, and post-harvest image-to-image\nmethods. We evaluated these AIgenerated images using PSNR and SSIM metrics and\ntested the detection performance of the YOLOv9 model. We also assessed the net\nquality of real and generated fruits. Our results showed that generative AI\ncould produce images very similar to real ones, especially for post-harvest\nfruits. The YOLOv9 model detected the generated images well, and the net\nquality was also measurable. This shows that generative AI can create realistic\nimages useful for fruit detection and quality assessment, indicating its great\npotential in agriculture. This study highlights the potential of AI-generated\nimages for data augmentation in melon fruit detection and quality assessment\nand envisions a positive future for generative AI applications in agriculture.",
    "authors": [
      "Seungri Yoon",
      "Yunseong Cho",
      "Tae In Ahn"
    ]
  },
  {
    "title": "Cooperative Reward Shaping for Multi-Agent Pathfinding",
    "abstract": "The primary objective of Multi-Agent Pathfinding (MAPF) is to plan efficient\nand conflict-free paths for all agents. Traditional multi-agent path planning\nalgorithms struggle to achieve efficient distributed path planning for multiple\nagents. In contrast, Multi-Agent Reinforcement Learning (MARL) has been\ndemonstrated as an effective approach to achieve this objective. By modeling\nthe MAPF problem as a MARL problem, agents can achieve efficient path planning\nand collision avoidance through distributed strategies under partial\nobservation. However, MARL strategies often lack cooperation among agents due\nto the absence of global information, which subsequently leads to reduced MAPF\nefficiency. To address this challenge, this letter introduces a unique reward\nshaping technique based on Independent Q-Learning (IQL). The aim of this method\nis to evaluate the influence of one agent on its neighbors and integrate such\nan interaction into the reward function, leading to active cooperation among\nagents. This reward shaping method facilitates cooperation among agents while\noperating in a distributed manner. The proposed approach has been evaluated\nthrough experiments across various scenarios with different scales and agent\ncounts. The results are compared with those from other state-of-the-art (SOTA)\nplanners. The evidence suggests that the approach proposed in this letter\nparallels other planners in numerous aspects, and outperforms them in scenarios\nfeaturing a large number of agents.",
    "authors": [
      "Zhenyu Song",
      "Ronghao Zheng",
      "Senlin Zhang",
      "Meiqin Liu"
    ]
  },
  {
    "title": "Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity",
    "abstract": "Video-to-audio (V2A) generation leverages visual-only video features to\nrender plausible sounds that match the scene. Importantly, the generated sound\nonsets should match the visual actions that are aligned with them, otherwise\nunnatural synchronization artifacts arise. Recent works have explored the\nprogression of conditioning sound generators on still images and then video\nfeatures, focusing on quality and semantic matching while ignoring\nsynchronization, or by sacrificing some amount of quality to focus on improving\nsynchronization only. In this work, we propose a V2A generative model, named\nMaskVAT, that interconnects a full-band high-quality general audio codec with a\nsequence-to-sequence masked generative model. This combination allows modeling\nboth high audio quality, semantic matching, and temporal synchronicity at the\nsame time. Our results show that, by combining a high-quality codec with the\nproper pre-trained audio-visual features and a sequence-to-sequence parallel\nstructure, we are able to yield highly synchronized results on one hand, whilst\nbeing competitive with the state of the art of non-codec generative audio\nmodels. Sample videos and generated audios are available at\nhttps://maskvat.github.io .",
    "authors": [
      "Santiago Pascual",
      "Chunghsin Yeh",
      "Ioannis Tsiamas",
      "Joan Serr\u00e0"
    ]
  },
  {
    "title": "By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting",
    "abstract": "Large language models (LLMs) have demonstrated exceptional abilities across\nvarious domains. However, utilizing LLMs for ubiquitous sensing applications\nremains challenging as existing text-prompt methods show significant\nperformance degradation when handling long sensor data sequences. We propose a\nvisual prompting approach for sensor data using multimodal LLMs (MLLMs). We\ndesign a visual prompt that directs MLLMs to utilize visualized sensor data\nalongside the target sensory task descriptions. Additionally, we introduce a\nvisualization generator that automates the creation of optimal visualizations\ntailored to a given sensory task, eliminating the need for prior task-specific\nknowledge. We evaluated our approach on nine sensory tasks involving four\nsensing modalities, achieving an average of 10% higher accuracy than text-based\nprompts and reducing token costs by 15.8x. Our findings highlight the\neffectiveness and cost-efficiency of visual prompts with MLLMs for various\nsensory tasks.",
    "authors": [
      "Hyungjun Yoon",
      "Biniyam Aschalew Tolera",
      "Taesik Gong",
      "Kimin Lee",
      "Sung-Ju Lee"
    ]
  },
  {
    "title": "Communication- and Computation-Efficient Distributed Decision-Making in Multi-Robot Networks",
    "abstract": "We provide a distributed coordination paradigm that enables scalable and\nnear-optimal joint motion planning among multiple robots. Our coordination\nparadigm contrasts with current paradigms that are either near-optimal but\nimpractical for replanning times or real-time but offer no near-optimality\nguarantees. We are motivated by the future of collaborative mobile autonomy,\nwhere distributed teams of robots will coordinate via vehicle-to-vehicle (v2v)\ncommunication to execute information-heavy tasks like mapping, surveillance,\nand target tracking. To enable rapid distributed coordination, we must curtail\nthe explosion of information-sharing across the network, thus limiting robot\ncoordination. However, this can lead to suboptimal plans, causing overlapping\ntrajectories instead of complementary ones. We make theoretical and algorithmic\ncontributions to balance the trade-off between decision speed and optimality.\nWe introduce tools for distributed submodular optimization, a diminishing\nreturns property in information-gathering tasks. Theoretically, we analyze how\nlocal network topology affects near-optimality at the global level.\nAlgorithmically, we provide a communication- and computation-efficient\ncoordination algorithm for agents to balance the trade-off. Our algorithm is up\nto two orders faster than competitive near-optimal algorithms. In simulations\nof surveillance tasks with up to 45 robots, it enables real-time planning at\nthe order of 1 Hz with superior coverage performance. To enable the\nsimulations, we provide a high-fidelity simulator that extends AirSim by\nintegrating a collaborative autonomy pipeline and simulating v2v communication\ndelays.",
    "authors": [
      "Zirui Xu",
      "Sandilya Sai Garimella",
      "Vasileios Tzoumas"
    ]
  },
  {
    "title": "NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models",
    "abstract": "Cognitive textual and visual reasoning tasks, such as puzzles, series, and\nanalogies, demand the ability to quickly reason, decipher, and evaluate\npatterns both textually and spatially. While LLMs and VLMs, through extensive\ntraining on large amounts of human-curated data, have attained a high level of\npseudo-human intelligence in some common sense reasoning tasks, they still\nstruggle with more complex reasoning tasks that require cognitive\nunderstanding. In this work, we introduce a new dataset, NTSEBench, designed to\nevaluate the cognitive multi-modal reasoning and problem-solving skills of\nlarge models. The dataset comprises 2,728 multiple-choice questions comprising\nof a total of 4,642 images across 26 categories sampled from the NTSE\nexamination conducted nationwide in India, featuring both visual and textual\ngeneral aptitude questions that do not rely on rote learning. We establish\nbaselines on the dataset using state-of-the-art LLMs and VLMs. To facilitate a\ncomparison between open source and propriety models, we propose four distinct\nmodeling strategies to handle different modalities (text and images) in the\ndataset instances.",
    "authors": [
      "Pranshu Pandya",
      "Agney S Talwarr",
      "Vatsal Gupta",
      "Tushar Kataria",
      "Vivek Gupta",
      "Dan Roth"
    ]
  },
  {
    "title": "Enhanced Self-supervised Learning for Multi-modality MRI Segmentation and Classification: A Novel Approach Avoiding Model Collapse",
    "abstract": "Multi-modality magnetic resonance imaging (MRI) can provide complementary\ninformation for computer-aided diagnosis. Traditional deep learning algorithms\nare suitable for identifying specific anatomical structures segmenting lesions\nand classifying diseases with magnetic resonance images. However, manual labels\nare limited due to high expense, which hinders further improvement of model\naccuracy. Self-supervised learning (SSL) can effectively learn feature\nrepresentations from unlabeled data by pre-training and is demonstrated to be\neffective in natural image analysis. Most SSL methods ignore the similarity of\nmulti-modality MRI, leading to model collapse. This limits the efficiency of\npre-training, causing low accuracy in downstream segmentation and\nclassification tasks. To solve this challenge, we establish and validate a\nmulti-modality MRI masked autoencoder consisting of hybrid mask pattern (HMP)\nand pyramid barlow twin (PBT) module for SSL on multi-modality MRI analysis.\nThe HMP concatenates three masking steps forcing the SSL to learn the semantic\nconnections of multi-modality images by reconstructing the masking patches. We\nhave proved that the proposed HMP can avoid model collapse. The PBT module\nexploits the pyramidal hierarchy of the network to construct barlow twin loss\nbetween masked and original views, aligning the semantic representations of\nimage patches at different vision scales in latent space. Experiments on\nBraTS2023, PI-CAI, and lung gas MRI datasets further demonstrate the\nsuperiority of our framework over the state-of-the-art. The performance of the\nsegmentation and classification is substantially enhanced, supporting the\naccurate detection of small lesion areas. The code is available at\nhttps://github.com/LinxuanHan/M2-MAE.",
    "authors": [
      "Linxuan Han",
      "Sa Xiao",
      "Zimeng Li",
      "Haidong Li",
      "Xiuchao Zhao",
      "Fumin Guo",
      "Yeqing Han",
      "Xin Zhou"
    ]
  },
  {
    "title": "An Empirical Study of Mamba-based Pedestrian Attribute Recognition",
    "abstract": "Current strong pedestrian attribute recognition models are developed based on\nTransformer networks, which are computationally heavy. Recently proposed models\nwith linear complexity (e.g., Mamba) have garnered significant attention and\nhave achieved a good balance between accuracy and computational cost across a\nvariety of visual tasks. Relevant review articles also suggest that while these\nmodels can perform well on some pedestrian attribute recognition datasets, they\nare generally weaker than the corresponding Transformer models. To further tap\ninto the potential of the novel Mamba architecture for PAR tasks, this paper\ndesigns and adapts Mamba into two typical PAR frameworks, i.e., the text-image\nfusion approach and pure vision Mamba multi-label recognition framework. It is\nfound that interacting with attribute tags as additional input does not always\nlead to an improvement, specifically, Vim can be enhanced, but VMamba cannot.\nThis paper further designs various hybrid Mamba-Transformer variants and\nconducts thorough experimental validations. These experimental results indicate\nthat simply enhancing Mamba with a Transformer does not always lead to\nperformance improvements but yields better results under certain settings. We\nhope this empirical study can further inspire research in Mamba for PAR, and\neven extend into the domain of multi-label recognition, through the design of\nthese network structures and comprehensive experimentation. The source code of\nthis work will be released at \\url{https://github.com/Event-AHU/OpenPAR}",
    "authors": [
      "Xiao Wang",
      "Weizhe Kong",
      "Jiandong Jin",
      "Shiao Wang",
      "Ruichong Gao",
      "Qingchuan Ma",
      "Chenglong Li",
      "Jin Tang"
    ]
  },
  {
    "title": "Mutual Learning for Acoustic Matching and Dereverberation via Visual Scene-driven Diffusion",
    "abstract": "Visual acoustic matching (VAM) is pivotal for enhancing the immersive\nexperience, and the task of dereverberation is effective in improving audio\nintelligibility. Existing methods treat each task independently, overlooking\nthe inherent reciprocity between them. Moreover, these methods depend on paired\ntraining data, which is challenging to acquire, impeding the utilization of\nextensive unpaired data. In this paper, we introduce MVSD, a mutual learning\nframework based on diffusion models. MVSD considers the two tasks\nsymmetrically, exploiting the reciprocal relationship to facilitate learning\nfrom inverse tasks and overcome data scarcity. Furthermore, we employ the\ndiffusion model as foundational conditional converters to circumvent the\ntraining instability and over-smoothing drawbacks of conventional GAN\narchitectures. Specifically, MVSD employs two converters: one for VAM called\nreverberator and one for dereverberation called dereverberator. The\ndereverberator judges whether the reverberation audio generated by reverberator\nsounds like being in the conditional visual scenario, and vice versa. By\nforming a closed loop, these two converters can generate informative feedback\nsignals to optimize the inverse tasks, even with easily acquired one-way\nunpaired data. Extensive experiments on two standard benchmarks, i.e.,\nSoundSpaces-Speech and Acoustic AVSpeech, exhibit that our framework can\nimprove the performance of the reverberator and dereverberator and better match\nspecified visual scenarios.",
    "authors": [
      "Jian Ma",
      "Wenguan Wang",
      "Yi Yang",
      "Feng Zheng"
    ]
  },
  {
    "title": "Accessing Vision Foundation Models at ImageNet-level Costs",
    "abstract": "Vision foundation models are renowned for their generalization ability due to\nmassive training data. Nevertheless, they demand tremendous training resources,\nand the training data is often inaccessible, e.g., CLIP, DINOv2, posing great\nchallenges to developing derivatives that could advance research in this field.\nIn this work, we offer a very simple and general solution, named Proteus, to\ndistill foundation models into smaller equivalents on ImageNet-1K without\naccess to the original training data. Specifically, we remove the designs from\nconventional knowledge distillation settings that result in dataset bias and\npresent three levels of training objectives, i.e., token, patch, and feature,\nto maximize the efficacy of knowledge transfer. In this manner, Proteus is\ntrained at ImageNet-level costs with surprising ability, facilitating the\naccessibility of training foundation models for the broader research community.\nLeveraging DINOv2-g/14 as the teacher, Proteus-L/14 matches the performance of\nthe Oracle method DINOv2-L/14 (142M training data) across 15 benchmarks and\noutperforms other vision foundation models including CLIP-L/14 (400M),\nOpenCLIP-L/14 (400M/2B) and SynCLR-L/14 (600M).",
    "authors": [
      "Yitian Zhang",
      "Xu Ma",
      "Yue Bai",
      "Huan Wang",
      "Yun Fu"
    ]
  },
  {
    "title": "Leave No Knowledge Behind During Knowledge Distillation: Towards Practical and Effective Knowledge Distillation for Code-Switching ASR Using Realistic Data",
    "abstract": "Recent advances in automatic speech recognition (ASR) often rely on large\nspeech foundation models for generating high-quality transcriptions. However,\nthese models can be impractical due to limited computing resources. The\nsituation is even more severe in terms of more realistic or difficult\nscenarios, such as code-switching ASR (CS-ASR). To address this, we present a\nframework for developing more efficient models for CS-ASR through knowledge\ndistillation using realistic speech-only data. Our proposed method, Leave No\nKnowledge Behind During Knowledge Distillation (K$^2$D), leverages both the\nteacher model's knowledge and additional insights from a small auxiliary model.\nWe evaluate our approach on two in-domain and two out-domain datasets,\ndemonstrating that K$^2$D is effective. By conducting K$^2$D on the unlabeled\nrealistic data, we have successfully obtained a 2-time smaller model with\n5-time faster generation speed while outperforming the baseline methods and the\nteacher model on all the testing sets. We have made our model publicly\navailable on Hugging Face\n(https://huggingface.co/andybi7676/k2d-whisper.zh-en).",
    "authors": [
      "Liang-Hsuan Tseng",
      "Zih-Ching Chen",
      "Wei-Shun Chang",
      "Cheng-Kuang Lee",
      "Tsung-Ren Huang",
      "Hung-yi Lee"
    ]
  },
  {
    "title": "Mutual Learning for Acoustic Matching and Dereverberation via Visual Scene-driven Diffusion",
    "abstract": "Visual acoustic matching (VAM) is pivotal for enhancing the immersive\nexperience, and the task of dereverberation is effective in improving audio\nintelligibility. Existing methods treat each task independently, overlooking\nthe inherent reciprocity between them. Moreover, these methods depend on paired\ntraining data, which is challenging to acquire, impeding the utilization of\nextensive unpaired data. In this paper, we introduce MVSD, a mutual learning\nframework based on diffusion models. MVSD considers the two tasks\nsymmetrically, exploiting the reciprocal relationship to facilitate learning\nfrom inverse tasks and overcome data scarcity. Furthermore, we employ the\ndiffusion model as foundational conditional converters to circumvent the\ntraining instability and over-smoothing drawbacks of conventional GAN\narchitectures. Specifically, MVSD employs two converters: one for VAM called\nreverberator and one for dereverberation called dereverberator. The\ndereverberator judges whether the reverberation audio generated by reverberator\nsounds like being in the conditional visual scenario, and vice versa. By\nforming a closed loop, these two converters can generate informative feedback\nsignals to optimize the inverse tasks, even with easily acquired one-way\nunpaired data. Extensive experiments on two standard benchmarks, i.e.,\nSoundSpaces-Speech and Acoustic AVSpeech, exhibit that our framework can\nimprove the performance of the reverberator and dereverberator and better match\nspecified visual scenarios.",
    "authors": [
      "Jian Ma",
      "Wenguan Wang",
      "Yi Yang",
      "Feng Zheng"
    ]
  }
]