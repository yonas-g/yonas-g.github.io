[
  {
    "title": "Does Refusal Training in LLMs Generalize to the Past Tense?",
    "abstract": "Refusal training is widely used to prevent LLMs from generating harmful,\nundesirable, or illegal outputs. We reveal a curious generalization gap in the\ncurrent refusal training approaches: simply reformulating a harmful request in\nthe past tense (e.g., \"How to make a Molotov cocktail?\" to \"How did people make\na Molotov cocktail?\") is often sufficient to jailbreak many state-of-the-art\nLLMs. We systematically evaluate this method on Llama-3 8B, GPT-3.5 Turbo,\nGemma-2 9B, Phi-3-Mini, GPT-4o, and R2D2 models using GPT-3.5 Turbo as a\nreformulation model. For example, the success rate of this simple attack on\nGPT-4o increases from 1% using direct requests to 88% using 20 past tense\nreformulation attempts on harmful requests from JailbreakBench with GPT-4 as a\njailbreak judge. Interestingly, we also find that reformulations in the future\ntense are less effective, suggesting that refusal guardrails tend to consider\npast historical questions more benign than hypothetical future questions.\nMoreover, our experiments on fine-tuning GPT-3.5 Turbo show that defending\nagainst past reformulations is feasible when past tense examples are explicitly\nincluded in the fine-tuning data. Overall, our findings highlight that the\nwidely used alignment techniques -- such as SFT, RLHF, and adversarial training\n-- employed to align the studied models can be brittle and do not always\ngeneralize as intended. We provide code and jailbreak artifacts at\nhttps://github.com/tml-epfl/llm-past-tense.",
    "authors": [
      "Maksym Andriushchenko",
      "Nicolas Flammarion"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11969v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "Efficient Training with Denoised Neural Weights",
    "abstract": "Good weight initialization serves as an effective measure to reduce the\ntraining cost of a deep neural network (DNN) model. The choice of how to\ninitialize parameters is challenging and may require manual tuning, which can\nbe time-consuming and prone to human error. To overcome such limitations, this\nwork takes a novel step towards building a weight generator to synthesize the\nneural weights for initialization. We use the image-to-image translation task\nwith generative adversarial networks (GANs) as an example due to the ease of\ncollecting model weights spanning a wide range. Specifically, we first collect\na dataset with various image editing concepts and their corresponding trained\nweights, which are later used for the training of the weight generator. To\naddress the different characteristics among layers and the substantial number\nof weights to be predicted, we divide the weights into equal-sized blocks and\nassign each block an index. Subsequently, a diffusion model is trained with\nsuch a dataset using both text conditions of the concept and the block indexes.\nBy initializing the image translation model with the denoised weights predicted\nby our diffusion model, the training requires only 43.3 seconds. Compared to\ntraining from scratch (i.e., Pix2pix), we achieve a 15x training time\nacceleration for a new concept while obtaining even better image generation\nquality.",
    "authors": [
      "Yifan Gong",
      "Zheng Zhan",
      "Yanyu Li",
      "Yerlan Idelbayev",
      "Andrey Zharkov",
      "Kfir Aberman",
      "Sergey Tulyakov",
      "Yanzhi Wang",
      "Jian Ren"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11966v1",
    "category": "Machine Learning"
  },
  {
    "title": "Motion-Oriented Compositional Neural Radiance Fields for Monocular Dynamic Human Modeling",
    "abstract": "This paper introduces Motion-oriented Compositional Neural Radiance Fields\n(MoCo-NeRF), a framework designed to perform free-viewpoint rendering of\nmonocular human videos via novel non-rigid motion modeling approach. In the\ncontext of dynamic clothed humans, complex cloth dynamics generate non-rigid\nmotions that are intrinsically distinct from skeletal articulations and\ncritically important for the rendering quality. The conventional approach\nmodels non-rigid motions as spatial (3D) deviations in addition to skeletal\ntransformations. However, it is either time-consuming or challenging to achieve\noptimal quality due to its high learning complexity without a direct\nsupervision. To target this problem, we propose a novel approach of modeling\nnon-rigid motions as radiance residual fields to benefit from more direct color\nsupervision in the rendering and utilize the rigid radiance fields as a prior\nto reduce the complexity of the learning process. Our approach utilizes a\nsingle multiresolution hash encoding (MHE) to concurrently learn the canonical\nT-pose representation from rigid skeletal motions and the radiance residual\nfield for non-rigid motions. Additionally, to further improve both training\nefficiency and usability, we extend MoCo-NeRF to support simultaneous training\nof multiple subjects within a single framework, thanks to our effective design\nfor modeling non-rigid motions. This scalability is achieved through the\nintegration of a global MHE and learnable identity codes in addition to\nmultiple local MHEs. We present extensive results on ZJU-MoCap and MonoCap,\nclearly demonstrating state-of-the-art performance in both single- and\nmulti-subject settings. The code and model will be made publicly available at\nthe project page: https://stevejaehyeok.github.io/publications/moco-nerf.",
    "authors": [
      "Jaehyeok Kim",
      "Dongyoon Wee",
      "Dan Xu"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11962v1",
    "category": "Robotics"
  },
  {
    "title": "Rethinking Transformer-based Multi-document Summarization: An Empirical Investigation",
    "abstract": "The utilization of Transformer-based models prospers the growth of\nmulti-document summarization (MDS). Given the huge impact and widespread\nadoption of Transformer-based models in various natural language processing\ntasks, investigating their performance and behaviors in the context of MDS\nbecomes crucial for advancing the field and enhancing the quality of summary.\nTo thoroughly examine the behaviours of Transformer-based MDS models, this\npaper presents five empirical studies on (1) measuring the impact of document\nboundary separators quantitatively; (2) exploring the effectiveness of\ndifferent mainstream Transformer structures; (3) examining the sensitivity of\nthe encoder and decoder; (4) discussing different training strategies; and (5)\ndiscovering the repetition in a summary generation. The experimental results on\nprevalent MDS datasets and eleven evaluation metrics show the influence of\ndocument boundary separators, the granularity of different level features and\ndifferent model training strategies. The results also reveal that the decoder\nexhibits greater sensitivity to noises compared to the encoder. This\nunderscores the important role played by the decoder, suggesting a potential\ndirection for future research in MDS. Furthermore, the experimental results\nindicate that the repetition problem in the generated summaries has\ncorrelations with the high uncertainty scores.",
    "authors": [
      "Congbo Ma",
      "Wei Emma Zhang",
      "Dileepa Pitawela",
      "Haojie Zhuang",
      "Yanfeng Shu"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11948v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "Tackling Oversmoothing in GNN via Graph Sparsification: A Truss-based Approach",
    "abstract": "Graph Neural Network (GNN) achieves great success for node-level and\ngraph-level tasks via encoding meaningful topological structures of networks in\nvarious domains, ranging from social to biological networks. However, repeated\naggregation operations lead to excessive mixing of node representations,\nparticularly in dense regions with multiple GNN layers, resulting in nearly\nindistinguishable embeddings. This phenomenon leads to the oversmoothing\nproblem that hampers downstream graph analytics tasks. To overcome this issue,\nwe propose a novel and flexible truss-based graph sparsification model that\nprunes edges from dense regions of the graph. Pruning redundant edges in dense\nregions helps to prevent the aggregation of excessive neighborhood information\nduring hierarchical message passing and pooling in GNN models. We then utilize\nour sparsification model in the state-of-the-art baseline GNNs and pooling\nmodels, such as GIN, SAGPool, GMT, DiffPool, MinCutPool, HGP-SL, DMonPool, and\nAdamGNN. Extensive experiments on different real-world datasets show that our\nmodel significantly improves the performance of the baseline GNN models in the\ngraph classification task.",
    "authors": [
      "Tanvir Hossain",
      "Khaled Mohammed Saifuddin",
      "Muhammad Ifte Khairul Islam",
      "Farhan Tanvir",
      "Esra Akbas"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11928v1",
    "category": "Machine Learning"
  },
  {
    "title": "What's Wrong? Refining Meeting Summaries with LLM Feedback",
    "abstract": "Meeting summarization has become a critical task since digital encounters\nhave become a common practice. Large language models (LLMs) show great\npotential in summarization, offering enhanced coherence and context\nunderstanding compared to traditional methods. However, they still struggle to\nmaintain relevance and avoid hallucination. We introduce a multi-LLM correction\napproach for meeting summarization using a two-phase process that mimics the\nhuman review process: mistake identification and summary refinement. We release\nQMSum Mistake, a dataset of 200 automatically generated meeting summaries\nannotated by humans on nine error types, including structural, omission, and\nirrelevance errors. Our experiments show that these errors can be identified\nwith high accuracy by an LLM. We transform identified mistakes into actionable\nfeedback to improve the quality of a given summary measured by relevance,\ninformativeness, conciseness, and coherence. This post-hoc refinement\neffectively improves summary quality by leveraging multiple LLMs to validate\noutput quality. Our multi-LLM approach for meeting summarization shows\npotential for similar complex text generation tasks requiring robustness,\naction planning, and discussion towards a goal.",
    "authors": [
      "Frederic Kirstein",
      "Terry Ruas",
      "Bela Gipp"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11919v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "Imitation of human motion achieves natural head movements for humanoid robots in an active-speaker detection task",
    "abstract": "Head movements are crucial for social human-human interaction. They can\ntransmit important cues (e.g., joint attention, speaker detection) that cannot\nbe achieved with verbal interaction alone. This advantage also holds for\nhuman-robot interaction. Even though modeling human motions through generative\nAI models has become an active research area within robotics in recent years,\nthe use of these methods for producing head movements in human-robot\ninteraction remains underexplored. In this work, we employed a generative AI\npipeline to produce human-like head movements for a Nao humanoid robot. In\naddition, we tested the system on a real-time active-speaker tracking task in a\ngroup conversation setting. Overall, the results show that the Nao robot\nsuccessfully imitates human head movements in a natural manner while actively\ntracking the speakers during the conversation. Code and data from this study\nare available at https://github.com/dingdingding60/Humanoids2024HRI",
    "authors": [
      "Bosong Ding",
      "Murat Kirtay",
      "Giacomo Spigler"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11915v1",
    "category": "Robotics"
  },
  {
    "title": "Bridging Weighted First Order Model Counting and Graph Polynomials",
    "abstract": "The Weighted First-Order Model Counting Problem (WFOMC) asks to compute the\nweighted sum of models of a given first-order logic sentence over a given\ndomain. It can be solved in time polynomial in the domain size for sentences\nfrom the two-variable fragment with counting quantifiers, known as $C^2$. This\npolynomial-time complexity is also retained when extending $C^2$ by one of the\nfollowing axioms: linear order axiom, tree axiom, forest axiom, directed\nacyclic graph axiom or connectedness axiom. An interesting question remains as\nto which other axioms can be added to the first-order sentences in this way. We\nprovide a new perspective on this problem by associating WFOMC with graph\npolynomials. Using WFOMC, we define Weak Connectedness Polynomial and Strong\nConnectedness Polynomials for first-order logic sentences. It turns out that\nthese polynomials have the following interesting properties. First, they can be\ncomputed in polynomial time in the domain size for sentences from $C^2$.\nSecond, we can use them to solve WFOMC with all of the existing axioms known to\nbe tractable as well as with new ones such as bipartiteness, strong\nconnectedness, being a spanning subgraph, having $k$ connected components, etc.\nThird, the well-known Tutte polynomial can be recovered as a special case of\nthe Weak Connectedness Polynomial, and the Strict and Non-Strict Directed\nChromatic Polynomials can be recovered from the Strong Connectedness\nPolynomials, which allows us to show that these important graph polynomials can\nbe computed in time polynomial in the number of vertices for any graph that can\nbe encoded by a fixed $C^2$ sentence and a conjunction of an arbitrary number\nof ground unary literals.",
    "authors": [
      "Qipeng Kuang",
      "Ond\u0159ej Ku\u017eelka",
      "Yuanhong Wang",
      "Yuyi Wang"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11877v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection",
    "abstract": "Grammatical Error Detection (GED) methods rely heavily on human annotated\nerror corpora. However, these annotations are unavailable in many low-resource\nlanguages. In this paper, we investigate GED in this context. Leveraging the\nzero-shot cross-lingual transfer capabilities of multilingual pre-trained\nlanguage models, we train a model using data from a diverse set of languages to\ngenerate synthetic errors in other languages. These synthetic error corpora are\nthen used to train a GED model. Specifically we propose a two-stage fine-tuning\npipeline where the GED model is first fine-tuned on multilingual synthetic data\nfrom target languages followed by fine-tuning on human-annotated GED corpora\nfrom source languages. This approach outperforms current state-of-the-art\nannotation-free GED methods. We also analyse the errors produced by our method\nand other strong baselines, finding that our approach produces errors that are\nmore diverse and more similar to human errors.",
    "authors": [
      "Gaetan Lopez Latouche",
      "Marc-Andr\u00e9 Carbonneau",
      "Ben Swanson"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11854v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "Schema Matching with Large Language Models: an Experimental Study",
    "abstract": "Large Language Models (LLMs) have shown useful applications in a variety of\ntasks, including data wrangling. In this paper, we investigate the use of an\noff-the-shelf LLM for schema matching. Our objective is to identify semantic\ncorrespondences between elements of two relational schemas using only names and\ndescriptions. Using a newly created benchmark from the health domain, we\npropose different so-called task scopes. These are methods for prompting the\nLLM to do schema matching, which vary in the amount of context information\ncontained in the prompt. Using these task scopes we compare LLM-based schema\nmatching against a string similarity baseline, investigating matching quality,\nverification effort, decisiveness, and complementarity of the approaches. We\nfind that matching quality suffers from a lack of context information, but also\nfrom providing too much context information. In general, using newer LLM\nversions increases decisiveness. We identify task scopes that have acceptable\nverification effort and succeed in identifying a significant number of true\nsemantic matches. Our study shows that LLMs have potential in bootstrapping the\nschema matching process and are able to assist data engineers in speeding up\nthis task solely based on schema element names and descriptions without the\nneed for data instances.",
    "authors": [
      "Marcel Parciak",
      "Brecht Vandevoort",
      "Frank Neven",
      "Liesbet M. Peeters",
      "Stijn Vansummeren"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11852v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "Variational Randomized Smoothing for Sample-Wise Adversarial Robustness",
    "abstract": "Randomized smoothing is a defensive technique to achieve enhanced robustness\nagainst adversarial examples which are small input perturbations that degrade\nthe performance of neural network models. Conventional randomized smoothing\nadds random noise with a fixed noise level for every input sample to smooth out\nadversarial perturbations. This paper proposes a new variational framework that\nuses a per-sample noise level suitable for each input by introducing a noise\nlevel selector. Our experimental results demonstrate enhancement of empirical\nrobustness against adversarial attacks. We also provide and analyze the\ncertified robustness for our sample-wise smoothing method.",
    "authors": [
      "Ryo Hase",
      "Ye Wang",
      "Toshiaki Koike-Akino",
      "Jing Liu",
      "Kieran Parsons"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11844v1",
    "category": "Machine Learning"
  },
  {
    "title": "InferAct: Inferring Safe Actions for LLM-Based Agents Through Preemptive Evaluation and Human Feedback",
    "abstract": "A crucial requirement for deploying LLM-based agents in real-life\napplications is robustness against risky or irreversible mistakes. However,\nexisting research lacks a focus on the preemptive evaluation of reasoning\ntrajectories performed by LLM agents, leading to a gap in ensuring safe and\nreliable operations. To explore better solutions, this paper introduces\nInferAct, a novel approach that leverages the Theory-of-Mind capability of LLMs\nto proactively detect potential errors before critical actions are executed\n(e.g., \"buy-now\" in automatic online trading or web shopping). InferAct is also\ncapable of integrating human feedback to prevent irreversible risks and enhance\nthe actor agent's decision-making process. Experiments on three widely used\ntasks demonstrate the effectiveness of InferAct. The proposed solution presents\na novel approach and concrete contributions toward developing LLM agents that\ncan be safely deployed in different environments involving critical\ndecision-making.",
    "authors": [
      "Haishuo Fang",
      "Xiaodan Zhu",
      "Iryna Gurevych"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11843v1",
    "category": "Robotics"
  },
  {
    "title": "Personalized Conversational Travel Assistant powered by Generative AI",
    "abstract": "The Tourism and Destination Management Organization (DMO) industry is rapidly\nevolving to adapt to new technologies and traveler expectations. Generative\nArtificial Intelligence (AI) offers an astonishing and innovative opportunity\nto enhance the tourism experience by providing personalized, interactive and\nengaging assistance. In this article, we propose a generative AI-based chatbot\nfor tourism assistance. The chatbot leverages AI ability to generate realistic\nand creative texts, adopting the friendly persona of the well-known Italian\nall-knowledgeable aunties, to provide tourists with personalized information,\ntailored and dynamic pre, during and post recommendations and trip plans and\npersonalized itineraries, using both text and voice commands, and supporting\ndifferent languages to satisfy Italian and foreign tourists expectations. This\nwork is under development in the Molise CTE research project, funded by the\nItalian Minister of the Economic Growth (MIMIT), with the aim to leverage the\nbest emerging technologies available, such as Cloud and AI to produce state of\nthe art solutions in the Smart City environment.",
    "authors": [
      "Alexio Cassani",
      "Michele Ruberl",
      "Antonio Salis",
      "Giacomo Giannese",
      "Gianluca Boanelli"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11830v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "GPT Assisted Annotation of Rhetorical and Linguistic Features for Interpretable Propaganda Technique Detection in News Text",
    "abstract": "While the use of machine learning for the detection of propaganda techniques\nin text has garnered considerable attention, most approaches focus on\n\"black-box\" solutions with opaque inner workings. Interpretable approaches\nprovide a solution, however, they depend on careful feature engineering and\ncostly expert annotated data. Additionally, language features specific to\npropagandistic text are generally the focus of rhetoricians or linguists, and\nthere is no data set labeled with such features suitable for machine learning.\nThis study codifies 22 rhetorical and linguistic features identified in\nliterature related to the language of persuasion for the purpose of annotating\nan existing data set labeled with propaganda techniques. To help human experts\nannotate natural language sentences with these features, RhetAnn, a web\napplication, was specifically designed to minimize an otherwise considerable\nmental effort. Finally, a small set of annotated data was used to fine-tune\nGPT-3.5, a generative large language model (LLM), to annotate the remaining\ndata while optimizing for financial cost and classification accuracy. This\nstudy demonstrates how combining a small number of human annotated examples\nwith GPT can be an effective strategy for scaling the annotation process at a\nfraction of the cost of traditional annotation relying solely on human experts.\nThe results are on par with the best performing model at the time of writing,\nnamely GPT-4, at 10x less the cost. Our contribution is a set of features,\ntheir properties, definitions, and examples in a machine-readable format, along\nwith the code for RhetAnn and the GPT prompts and fine-tuning procedures for\nadvancing state-of-the-art interpretable propaganda technique detection.",
    "authors": [
      "Kyle Hamilton",
      "Luca Longo",
      "Bojan Bozic"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11827v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "The Future of Data Science Education",
    "abstract": "The definition of Data Science is a hotly debated topic. For many, the\ndefinition is a simple shortcut to Artificial Intelligence or Machine Learning.\nHowever, there is far more depth and nuance to the field of Data Science than a\nsimple shortcut can provide. The School of Data Science at the University of\nVirginia has developed a novel model for the definition of Data Science. This\nmodel is based on identifying a unified understanding of the data work done\nacross all areas of Data Science. It represents a generational leap forward in\nhow we understand and teach Data Science. In this paper we will present the\ncore features of the model and explain how it unifies various concepts going\nfar beyond the analytics component of AI. From this foundation we will present\nour Undergraduate Major curriculum in Data Science and demonstrate how it\nprepares students to be well-rounded Data Science team members and leaders. The\npaper will conclude with an in-depth overview of the Foundations of Data\nScience course designed to introduce students to the field while also\nimplementing proven STEM oriented pedagogical methods. These include, for\nexample, specifications grading, active learning lectures, guest lectures from\nindustry experts and weekly gamification labs.",
    "authors": [
      "Brian Wright",
      "Peter Alonzi",
      "Ali Riveria"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11824v1",
    "category": "Machine Learning"
  },
  {
    "title": "Approximating Probabilistic Inference in Statistical EL with Knowledge Graph Embeddings",
    "abstract": "Statistical information is ubiquitous but drawing valid conclusions from it\nis prohibitively hard. We explain how knowledge graph embeddings can be used to\napproximate probabilistic inference efficiently using the example of\nStatistical EL (SEL), a statistical extension of the lightweight Description\nLogic EL. We provide proofs for runtime and soundness guarantees, and\nempirically evaluate the runtime and approximation quality of our approach.",
    "authors": [
      "Yuqicheng Zhu",
      "Nico Potyka",
      "Bo Xiong",
      "Trung-Kien Tran",
      "Mojtaba Nayyeri",
      "Evgeny Kharlamov",
      "Steffen Staab"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11821v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "Stepping Stones: A Progressive Training Strategy for Audio-Visual Semantic Segmentation",
    "abstract": "Audio-Visual Segmentation (AVS) aims to achieve pixel-level localization of\nsound sources in videos, while Audio-Visual Semantic Segmentation (AVSS), as an\nextension of AVS, further pursues semantic understanding of audio-visual\nscenes. However, since the AVSS task requires the establishment of audio-visual\ncorrespondence and semantic understanding simultaneously, we observe that\nprevious methods have struggled to handle this mashup of objectives in\nend-to-end training, resulting in insufficient learning and sub-optimization.\nTherefore, we propose a two-stage training strategy called \\textit{Stepping\nStones}, which decomposes the AVSS task into two simple subtasks from\nlocalization to semantic understanding, which are fully optimized in each stage\nto achieve step-by-step global optimization. This training strategy has also\nproved its generalization and effectiveness on existing methods. To further\nimprove the performance of AVS tasks, we propose a novel framework Adaptive\nAudio Visual Segmentation, in which we incorporate an adaptive audio query\ngenerator and integrate masked attention into the transformer decoder,\nfacilitating the adaptive fusion of visual and audio features. Extensive\nexperiments demonstrate that our methods achieve state-of-the-art results on\nall three AVS benchmarks. The project homepage can be accessed at\nhttps://gewu-lab.github.io/stepping_stones/.",
    "authors": [
      "Juncheng Ma",
      "Peiwen Sun",
      "Yaoting Wang",
      "Di Hu"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11820v1",
    "category": "Speech Recognition"
  },
  {
    "title": "Invariant Consistency for Knowledge Distillation",
    "abstract": "Knowledge distillation (KD) involves transferring the knowledge from one\nneural network to another, often from a larger, well-trained model (teacher) to\na smaller, more efficient model (student). Traditional KD methods minimize the\nKullback-Leibler (KL) divergence between the probabilistic outputs of the\nteacher and student networks. However, this approach often overlooks crucial\nstructural knowledge embedded within the teacher's network. In this paper, we\nintroduce Invariant Consistency Distillation (ICD), a novel methodology\ndesigned to enhance KD by ensuring that the student model's representations are\nconsistent with those of the teacher. Our approach combines contrastive\nlearning with an explicit invariance penalty, capturing significantly more\ninformation from the teacher's representation of the data. Our results on\nCIFAR-100 demonstrate that ICD outperforms traditional KD techniques and\nsurpasses 13 state-of-the-art methods. In some cases, the student model even\nexceeds the teacher model in terms of accuracy. Furthermore, we successfully\ntransfer our method to other datasets, including Tiny ImageNet and STL-10. The\ncode will be made public soon.",
    "authors": [
      "Nikolaos Giakoumoglou",
      "Tania Stathaki"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11802v1",
    "category": "Machine Learning"
  },
  {
    "title": "Click-Gaussian: Interactive Segmentation to Any 3D Gaussians",
    "abstract": "Interactive segmentation of 3D Gaussians opens a great opportunity for\nreal-time manipulation of 3D scenes thanks to the real-time rendering\ncapability of 3D Gaussian Splatting. However, the current methods suffer from\ntime-consuming post-processing to deal with noisy segmentation output. Also,\nthey struggle to provide detailed segmentation, which is important for\nfine-grained manipulation of 3D scenes. In this study, we propose\nClick-Gaussian, which learns distinguishable feature fields of two-level\ngranularity, facilitating segmentation without time-consuming post-processing.\nWe delve into challenges stemming from inconsistently learned feature fields\nresulting from 2D segmentation obtained independently from a 3D scene. 3D\nsegmentation accuracy deteriorates when 2D segmentation results across the\nviews, primary cues for 3D segmentation, are in conflict. To overcome these\nissues, we propose Global Feature-guided Learning (GFL). GFL constructs the\nclusters of global feature candidates from noisy 2D segments across the views,\nwhich smooths out noises when training the features of 3D Gaussians. Our method\nruns in 10 ms per click, 15 to 130 times as fast as the previous methods, while\nalso significantly improving segmentation accuracy. Our project page is\navailable at https://seokhunchoi.github.io/Click-Gaussian",
    "authors": [
      "Seokhun Choi",
      "Hyeonseop Song",
      "Jaechul Kim",
      "Taehyeong Kim",
      "Hoseok Do"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11793v1",
    "category": "Computer Vision"
  },
  {
    "title": "Characterizing and Understanding HGNN Training on GPUs",
    "abstract": "Owing to their remarkable representation capabilities for heterogeneous graph\ndata, Heterogeneous Graph Neural Networks (HGNNs) have been widely adopted in\nmany critical real-world domains such as recommendation systems and medical\nanalysis. Prior to their practical application, identifying the optimal HGNN\nmodel parameters tailored to specific tasks through extensive training is a\ntime-consuming and costly process. To enhance the efficiency of HGNN training,\nit is essential to characterize and analyze the execution semantics and\npatterns within the training process to identify performance bottlenecks. In\nthis study, we conduct an in-depth quantification and analysis of two\nmainstream HGNN training scenarios, including single-GPU and multi-GPU\ndistributed training. Based on the characterization results, we disclose the\nperformance bottlenecks and their underlying causes in different HGNN training\nscenarios and provide optimization guidelines from both software and hardware\nperspectives.",
    "authors": [
      "Dengke Han",
      "Mingyu Yan",
      "Xiaochun Ye",
      "Dongrui Fan",
      "Ninghui Sun"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11790v1",
    "category": "Machine Learning"
  },
  {
    "title": "Large Language Models as Misleading Assistants in Conversation",
    "abstract": "Large Language Models (LLMs) are able to provide assistance on a wide range\nof information-seeking tasks. However, model outputs may be misleading, whether\nunintentionally or in cases of intentional deception. We investigate the\nability of LLMs to be deceptive in the context of providing assistance on a\nreading comprehension task, using LLMs as proxies for human users. We compare\noutcomes of (1) when the model is prompted to provide truthful assistance, (2)\nwhen it is prompted to be subtly misleading, and (3) when it is prompted to\nargue for an incorrect answer. Our experiments show that GPT-4 can effectively\nmislead both GPT-3.5-Turbo and GPT-4, with deceptive assistants resulting in up\nto a 23% drop in accuracy on the task compared to when a truthful assistant is\nused. We also find that providing the user model with additional context from\nthe passage partially mitigates the influence of the deceptive model. This work\nhighlights the ability of LLMs to produce misleading information and the\neffects this may have in real-world situations.",
    "authors": [
      "Betty Li Hou",
      "Kejian Shi",
      "Jason Phang",
      "James Aung",
      "Steven Adler",
      "Rosie Campbell"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11789v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "Data-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model Co-development",
    "abstract": "The emergence of large-scale multi-modal generative models has drastically\nadvanced artificial intelligence, introducing unprecedented levels of\nperformance and functionality. However, optimizing these models remains\nchallenging due to historically isolated paths of model-centric and\ndata-centric developments, leading to suboptimal outcomes and inefficient\nresource utilization. In response, we present a novel sandbox suite tailored\nfor integrated data-model co-development. This sandbox provides a comprehensive\nexperimental platform, enabling rapid iteration and insight-driven refinement\nof both data and models. Our proposed \"Probe-Analyze-Refine\" workflow,\nvalidated through applications on state-of-the-art LLaVA-like and DiT based\nmodels, yields significant performance boosts, such as topping the VBench\nleaderboard. We also uncover fruitful insights gleaned from exhaustive\nbenchmarks, shedding light on the critical interplay between data quality,\ndiversity, and model behavior. With the hope of fostering deeper understanding\nand future progress in multi-modal data and generative modeling, our codes,\ndatasets, and models are maintained and accessible at\nhttps://github.com/modelscope/data-juicer/blob/main/docs/Sandbox.md.",
    "authors": [
      "Daoyuan Chen",
      "Haibin Wang",
      "Yilun Huang",
      "Ce Ge",
      "Yaliang Li",
      "Bolin Ding",
      "Jingren Zhou"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11784v1",
    "category": "Machine Learning"
  },
  {
    "title": "SwitchCIT: Switching for Continual Instruction Tuning of Large Language Models",
    "abstract": "Large language models (LLMs) have exhibited impressive capabilities in\nvarious domains, particularly in general language understanding. However these\nmodels, trained on massive text data, may not be finely optimized for specific\ntasks triggered by instructions. Continual instruction tuning is crucial to\nadapt LLMs to evolving tasks and domains, ensuring their effectiveness and\nrelevance across a wide range of applications. In the context of continual\ninstruction tuning, where models are sequentially trained on different tasks,\ncatastrophic forgetting can occur, leading to performance degradation on\npreviously learned tasks. This work addresses the catastrophic forgetting in\ncontinual instruction learning for LLMs through a switching mechanism for\nrouting computations to parameter-efficient tuned models. We demonstrate the\neffectiveness of our method through experiments on continual instruction tuning\nof different natural language generation tasks.",
    "authors": [
      "Xinbo Wu",
      "Max Hartman",
      "Vidhata Arjun Jayaraman",
      "Lav R. Varshney"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11780v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "Sharif-MGTD at SemEval-2024 Task 8: A Transformer-Based Approach to Detect Machine Generated Text",
    "abstract": "Detecting Machine-Generated Text (MGT) has emerged as a significant area of\nstudy within Natural Language Processing. While language models generate text,\nthey often leave discernible traces, which can be scrutinized using either\ntraditional feature-based methods or more advanced neural language models. In\nthis research, we explore the effectiveness of fine-tuning a RoBERTa-base\ntransformer, a powerful neural architecture, to address MGT detection as a\nbinary classification task. Focusing specifically on Subtask A\n(Monolingual-English) within the SemEval-2024 competition framework, our\nproposed system achieves an accuracy of 78.9% on the test dataset, positioning\nus at 57th among participants. Our study addresses this challenge while\nconsidering the limited hardware resources, resulting in a system that excels\nat identifying human-written texts but encounters challenges in accurately\ndiscerning MGTs.",
    "authors": [
      "Seyedeh Fatemeh Ebrahimi",
      "Karim Akhavan Azari",
      "Amirmasoud Iravani",
      "Arian Qazvini",
      "Pouya Sadeghi",
      "Zeinab Sadat Taghavi",
      "Hossein Sameti"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11774v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "XEdgeAI: A Human-centered Industrial Inspection Framework with Data-centric Explainable Edge AI Approach",
    "abstract": "Recent advancements in deep learning have significantly improved visual\nquality inspection and predictive maintenance within industrial settings.\nHowever, deploying these technologies on low-resource edge devices poses\nsubstantial challenges due to their high computational demands and the inherent\ncomplexity of Explainable AI (XAI) methods. This paper addresses these\nchallenges by introducing a novel XAI-integrated Visual Quality Inspection\nframework that optimizes the deployment of semantic segmentation models on\nlow-resource edge devices. Our framework incorporates XAI and the Large Vision\nLanguage Model to deliver human-centered interpretability through visual and\ntextual explanations to end-users. This is crucial for end-user trust and model\ninterpretability. We outline a comprehensive methodology consisting of six\nfundamental modules: base model fine-tuning, XAI-based explanation generation,\nevaluation of XAI approaches, XAI-guided data augmentation, development of an\nedge-compatible model, and the generation of understandable visual and textual\nexplanations. Through XAI-guided data augmentation, the enhanced model\nincorporating domain expert knowledge with visual and textual explanations is\nsuccessfully deployed on mobile devices to support end-users in real-world\nscenarios. Experimental results showcase the effectiveness of the proposed\nframework, with the mobile model achieving competitive accuracy while\nsignificantly reducing model size. This approach paves the way for the broader\nadoption of reliable and interpretable AI tools in critical industrial\napplications, where decisions must be both rapid and justifiable.",
    "authors": [
      "Truong Thanh Hung Nguyen",
      "Phuc Truong Loc Nguyen",
      "Hung Cao"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11771v1",
    "category": "Machine Learning"
  },
  {
    "title": "Vectoring Languages",
    "abstract": "Recent breakthroughs in large language models (LLM) have stirred up global\nattention, and the research has been accelerating non-stop since then.\nPhilosophers and psychologists have also been researching the structure of\nlanguage for decades, but they are having a hard time finding a theory that\ndirectly benefits from the breakthroughs of LLMs. In this article, we propose a\nnovel structure of language that reflects well on the mechanisms behind\nlanguage models and go on to show that this structure is also better at\ncapturing the diverse nature of language compared to previous methods. An\nanalogy of linear algebra is adapted to strengthen the basis of this\nperspective. We further argue about the difference between this perspective and\nthe design philosophy for current language models. Lastly, we discuss how this\nperspective can lead us to research directions that may accelerate the\nimprovements of science fastest.",
    "authors": [
      "Joseph Chen"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11766v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "A Channel Attention-Driven Hybrid CNN Framework for Paddy Leaf Disease Detection",
    "abstract": "Farmers face various challenges when it comes to identifying diseases in rice\nleaves during their early stages of growth, which is a major reason for poor\nproduce. Therefore, early and accurate disease identification is important in\nagriculture to avoid crop loss and improve cultivation. In this research, we\npropose a novel hybrid deep learning (DL) classifier designed by extending the\nSqueeze-and-Excitation network architecture with a channel attention mechanism\nand the Swish ReLU activation function. The channel attention mechanism in our\nproposed model identifies the most important feature channels required for\nclassification during feature extraction and selection. The dying ReLU problem\nis mitigated by utilizing the Swish ReLU activation function, and the\nSqueeze-andExcitation blocks improve information propagation and cross-channel\ninteraction. Upon evaluation, our model achieved a high F1-score of 99.76% and\nan accuracy of 99.74%, surpassing the performance of existing models. These\noutcomes demonstrate the potential of state-of-the-art DL techniques in\nagriculture, contributing to the advancement of more efficient and reliable\ndisease detection systems.",
    "authors": [
      "Pandiyaraju V",
      "Shravan Venkatraman",
      "Abeshek A",
      "Pavan Kumar S",
      "Aravintakshan S A",
      "Senthil Kumar A M",
      "Kannan A"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11753v1",
    "category": "Machine Learning"
  },
  {
    "title": "Universal Sound Separation with Self-Supervised Audio Masked Autoencoder",
    "abstract": "Universal sound separation (USS) is a task of separating mixtures of\narbitrary sound sources. Typically, universal separation models are trained\nfrom scratch in a supervised manner, using labeled data. Self-supervised\nlearning (SSL) is an emerging deep learning approach that leverages unlabeled\ndata to obtain task-agnostic representations, which can benefit many downstream\ntasks. In this paper, we propose integrating a self-supervised pre-trained\nmodel, namely the audio masked autoencoder (A-MAE), into a universal sound\nseparation system to enhance its separation performance. We employ two\nstrategies to utilize SSL embeddings: freezing or updating the parameters of\nA-MAE during fine-tuning. The SSL embeddings are concatenated with the\nshort-time Fourier transform (STFT) to serve as input features for the\nseparation model. We evaluate our methods on the AudioSet dataset, and the\nexperimental results indicate that the proposed methods successfully enhance\nthe separation performance of a state-of-the-art ResUNet-based USS model.",
    "authors": [
      "Junqi Zhao",
      "Xubo Liu",
      "Jinzheng Zhao",
      "Yi Yuan",
      "Qiuqiang Kong",
      "Mark D. Plumbley",
      "Wenwu Wang"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11745v1",
    "category": "Machine Learning"
  },
  {
    "title": "NITRO-D: Native Integer-only Training of Deep Convolutional Neural Networks",
    "abstract": "Quantization has become increasingly pivotal in addressing the steadily\nincreasing computational and memory requirements of Deep Neural Networks\n(DNNs). By reducing the number of bits used to represent weights and\nactivations (typically from 32-bit floating-point to 16-bit or 8-bit integers),\nquantization reduces the memory footprint, energy consumption, and execution\ntime of DNN models. However, traditional quantization methods typically focus\non the inference of DNNs, while the training process still relies on\nfloating-point operations. To date, only one work in the literature has\naddressed integer-only training for Multi-Layer Perceptron (MLP) architectures.\nThis work introduces NITRO-D, a new framework for training arbitrarily deep\ninteger-only Convolutional Neural Networks (CNNs) that operate entirely< in the\ninteger-only domain for both training and inference. NITRO-D is the first\nframework in the literature enabling the training of integer-only CNNs without\nthe need to introduce a quantization scheme. Specifically, NITRO-D introduces a\nnovel architecture integrating multiple integer local-loss blocks, which\ninclude the proposed NITRO Scaling Layer and the NITRO-ReLU activation\nfunction. Additionally, it introduces a novel integer-only learning algorithm\nderived from Local Error Signals (LES), utilizing IntegerSGD, an optimizer\nspecifically designed to operate in an integer-only context. NITRO-D is\nimplemented in an open-source Python library. Extensive experimental\nevaluations demonstrate its effectiveness across several state-of-the-art image\nrecognition datasets. Results show significant performance improvements from\n2.47% to 5.96% for integer-only MLP architectures over the state-of-the-art\nsolution, and the capability of training integer-only CNN architectures with\nminimal accuracy degradation from -0.15% to -4.22% compared to floating-point\nLES.",
    "authors": [
      "Alberto Pirillo",
      "Luca Colombo",
      "Manuel Roveri"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11698v1",
    "category": "Machine Learning"
  },
  {
    "title": "CCoE: A Compact LLM with Collaboration of Experts",
    "abstract": "In the domain of Large Language Model (LLM), LLMs demonstrate significant\ncapabilities in natural language understanding and generation. With the growing\nneeds of applying LLMs on various domains, it is a research question that how\nto efficiently train and build a model that has expertise in different domains\nbut with a low training cost. We propose CCoE architecture, a framework of\neasily coupling multiple strong domain experts together to fuse into a big LLM,\nprovides a collective way of utilizing the different domain expert LLMs.\nBesides, training a large collaborative of multiple expert LLMs requires a high\nrequirements on training sources. CCoE bypasses this problem through isolating\nother experts and train each expert separately. The design of CCoE assembles\nmultiple expert LLMs through the CoE (Collaboration of Experts) layer. Each CoE\nlayer could have one or more expert LLMs. Expert LLMs have different number of\nlayers and have been well-trained for different domain tasks. Each expert is\nfine-tuned to be able to achieve the comparable results with SOTA domain LLMs.\nWe start from 5 experts in the domain of Code, Math, Law, text-to-SQL and\nMedical. The results indicate that our CCoE framework can easily and\nefficiently boost nearly 10%-20% performance on original base model in\ndifferent domains but using less resources on training, as well as inference.",
    "authors": [
      "Shaomang Huang",
      "Jianfeng Pan",
      "Hanzhong Zheng"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11686v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "SKADA-Bench: Benchmarking Unsupervised Domain Adaptation Methods with Realistic Validation",
    "abstract": "Unsupervised Domain Adaptation (DA) consists of adapting a model trained on a\nlabeled source domain to perform well on an unlabeled target domain with some\ndata distribution shift. While many methods have been proposed in the\nliterature, fair and realistic evaluation remains an open question,\nparticularly due to methodological difficulties in selecting hyperparameters in\nthe unsupervised setting. With SKADA-Bench, we propose a framework to evaluate\nDA methods and present a fair evaluation of existing shallow algorithms,\nincluding reweighting, mapping, and subspace alignment. Realistic\nhyperparameter selection is performed with nested cross-validation and various\nunsupervised model selection scores, on both simulated datasets with controlled\nshifts and real-world datasets across diverse modalities, such as images, text,\nbiomedical, and tabular data with specific feature extraction. Our benchmark\nhighlights the importance of realistic validation and provides practical\nguidance for real-life applications, with key insights into the choice and\nimpact of model selection approaches. SKADA-Bench is open-source, reproducible,\nand can be easily extended with novel DA methods, datasets, and model selection\ncriteria without requiring re-evaluating competitors. SKADA-Bench is available\non GitHub at https://github.com/scikit-adaptation/skada-bench.",
    "authors": [
      "Yanis Lalou",
      "Th\u00e9o Gnassounou",
      "Antoine Collas",
      "Antoine de Mathelin",
      "Oleksii Kachaiev",
      "Ambroise Odonnat",
      "Alexandre Gramfort",
      "Thomas Moreau",
      "R\u00e9mi Flamary"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11676v1",
    "category": "Machine Learning"
  },
  {
    "title": "R-SFLLM: Jamming Resilient Framework for Split Federated Learning with Large Language Models",
    "abstract": "Split federated learning (SFL) is a compute-efficient paradigm in distributed\nmachine learning (ML), where components of large ML models are outsourced to\nremote servers. A significant challenge in SFL, particularly when deployed over\nwireless channels, is the susceptibility of transmitted model parameters to\nadversarial jamming that could jeopardize the learning process. This is\nparticularly pronounced for word embedding parameters in large language models\n(LLMs), which are crucial for language understanding. In this paper, rigorous\ninsights are provided into the influence of jamming LLM word embeddings in SFL\nby deriving an expression for the ML training loss divergence and showing that\nit is upper-bounded by the mean squared error (MSE). Based on this analysis, a\nphysical layer framework is developed for resilient SFL with LLMs (R-SFLLM)\nover wireless networks. R-SFLLM leverages wireless sensing data to gather\ninformation on the jamming directions-of-arrival (DoAs) for the purpose of\ndevising a novel, sensing-assisted anti-jamming strategy while jointly\noptimizing beamforming, user scheduling, and resource allocation. Extensive\nexperiments using BERT and RoBERTa models demonstrate R-SFLLM's effectiveness,\nachieving close-to-baseline performance across various natural language\nprocessing (NLP) tasks and datasets. The proposed methodology further\nintroduces an adversarial training component, where controlled noise exposure\nsignificantly enhances the LLM's resilience to perturbed parameters during\ntraining. The results show that more noise-sensitive models, such as RoBERTa,\nbenefit from this feature, especially when resource allocation is unfair. It is\nalso shown that worst-case jamming in particular translates into worst-case\nmodel outcomes, thereby necessitating the need for jamming-resilient SFL\nprotocols.",
    "authors": [
      "Aladin Djuhera",
      "Vlad C. Andrei",
      "Xinyang Li",
      "Ullrich J. M\u00f6nich",
      "Holger Boche",
      "Walid Saad"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11654v1",
    "category": "Machine Learning"
  },
  {
    "title": "CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging",
    "abstract": "Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.",
    "authors": [
      "Sunny Gupta",
      "Amit Sethi"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11652v1",
    "category": "Machine Learning"
  },
  {
    "title": "Rethinking Fair Graph Neural Networks from Re-balancing",
    "abstract": "Driven by the powerful representation ability of Graph Neural Networks\n(GNNs), plentiful GNN models have been widely deployed in many real-world\napplications. Nevertheless, due to distribution disparities between different\ndemographic groups, fairness in high-stake decision-making systems is receiving\nincreasing attention. Although lots of recent works devoted to improving the\nfairness of GNNs and achieved considerable success, they all require\nsignificant architectural changes or additional loss functions requiring more\nhyper-parameter tuning. Surprisingly, we find that simple re-balancing methods\ncan easily match or surpass existing fair GNN methods. We claim that the\nimbalance across different demographic groups is a significant source of\nunfairness, resulting in imbalanced contributions from each group to the\nparameters updating. However, these simple re-balancing methods have their own\nshortcomings during training. In this paper, we propose FairGB, Fair Graph\nNeural Network via re-Balancing, which mitigates the unfairness of GNNs by\ngroup balancing. Technically, FairGB consists of two modules: counterfactual\nnode mixup and contribution alignment loss. Firstly, we select counterfactual\npairs across inter-domain and inter-class, and interpolate the ego-networks to\ngenerate new samples. Guided by analysis, we can reveal the debiasing mechanism\nof our model by the causal view and prove that our strategy can make sensitive\nattributes statistically independent from target labels. Secondly, we reweigh\nthe contribution of each group according to gradients. By combining these two\nmodules, they can mutually promote each other. Experimental results on\nbenchmark datasets show that our method can achieve state-of-the-art results\nconcerning both utility and fairness metrics. Code is available at\nhttps://github.com/ZhixunLEE/FairGB.",
    "authors": [
      "Zhixun Li",
      "Yushun Dong",
      "Qiang Liu",
      "Jeffrey Xu Yu"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11624v1",
    "category": "Machine Learning"
  },
  {
    "title": "Graph Dimension Attention Networks for Enterprise Credit Assessment",
    "abstract": "Enterprise credit assessment is critical for evaluating financial risk, and\nGraph Neural Networks (GNNs), with their advanced capability to model\ninter-entity relationships, are a natural tool to get a deeper understanding of\nthese financial networks. However, existing GNN-based methodologies\npredominantly emphasize entity-level attention mechanisms for contagion risk\naggregation, often overlooking the heterogeneous importance of different\nfeature dimensions, thus falling short in adequately modeling credit risk\nlevels. To address this issue, we propose a novel architecture named Graph\nDimension Attention Network (GDAN), which incorporates a dimension-level\nattention mechanism to capture fine-grained risk-related characteristics.\nFurthermore, we explore the interpretability of the GNN-based method in\nfinancial scenarios and propose a simple but effective data-centric explainer\nfor GDAN, called GDAN-DistShift. DistShift provides edge-level interpretability\nby quantifying distribution shifts during the message-passing process.\nMoreover, we collected a real-world, multi-source Enterprise Credit Assessment\nDataset (ECAD) and have made it accessible to the research community since\nhigh-quality datasets are lacking in this field. Extensive experiments\nconducted on ECAD demonstrate the effectiveness of our methods. In addition, we\nran GDAN on the well-known datasets SMEsD and DBLP, also with excellent\nresults.",
    "authors": [
      "Shaopeng Wei",
      "Beni Egressy",
      "Xingyan Chen",
      "Yu Zhao",
      "Fuzhen Zhuang",
      "Roger Wattenhofer",
      "Gang Kou"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11615v1",
    "category": "Machine Learning"
  },
  {
    "title": "Bringing AI Participation Down to Scale: A Comment on Open AIs Democratic Inputs to AI Project",
    "abstract": "This commentary piece reviews the recent Open AI Democratic Inputs programme,\nwhich funded 10 teams to design procedures for public participation in\ngenerative AI. While applauding the technical innovations in these projects, we\nidentify several shared assumptions including the generality of LLMs,\nextracting abstract values, soliciting solutions not problems and equating\nparticipation with democracy. We call instead for AI participation which\ninvolves specific communities and use cases and solicits concrete problems to\nbe remedied. We also find it important that these communities have a stake in\nthe outcome, including ownership of data or models.",
    "authors": [
      "David Moats",
      "Chandrima Ganguly"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11613v1",
    "category": "Robotics"
  },
  {
    "title": "Improving Engagement and Efficacy of mHealth Micro-Interventions for Stress Coping: an In-The-Wild Study",
    "abstract": "Sustaining long-term user engagement with mobile health (mHealth)\ninterventions while preserving their high efficacy remains an ongoing challenge\nin real-world well-being applications. To address this issue, we introduce a\nnew algorithm, the Personalized, Context-Aware Recommender (PCAR), for\nintervention selection and evaluate its performance in a field experiment. In a\nfour-week, in-the-wild experiment involving 29 parents of young children, we\ndelivered personalized stress-reducing micro-interventions through a mobile\nchatbot. We assessed their impact on stress reduction using momentary stress\nlevel ecological momentary assessments (EMAs) before and after each\nintervention. Our findings demonstrate the superiority of PCAR intervention\nselection in enhancing the engagement and efficacy of mHealth\nmicro-interventions to stress coping compared to random intervention selection\nand a control group that did not receive any intervention. Furthermore, we show\nthat even brief, one-minute interventions can significantly reduce perceived\nstress levels (p=0.001). We observe that individuals are most receptive to\none-minute interventions during transitional periods between activities, such\nas transitioning from afternoon activities to bedtime routines. Our study\ncontributes to the literature by introducing a personalized context-aware\nintervention selection algorithm that improves engagement and efficacy of\nmHealth interventions, identifying key timing for stress interventions, and\noffering insights into mechanisms to improve stress coping.",
    "authors": [
      "Chaya Ben Yehuda",
      "Ran Gilad-Bachrach",
      "Yarin Udi"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11612v1",
    "category": "Machine Learning"
  },
  {
    "title": "Statistical Reachability Analysis of Stochastic Cyber-Physical Systems under Distribution Shift",
    "abstract": "Reachability analysis is a popular method to give safety guarantees for\nstochastic cyber-physical systems (SCPSs) that takes in a symbolic description\nof the system dynamics and uses set-propagation methods to compute an\noverapproximation of the set of reachable states over a bounded time horizon.\nIn this paper, we investigate the problem of performing reachability analysis\nfor an SCPS that does not have a symbolic description of the dynamics, but\ninstead is described using a digital twin model that can be simulated to\ngenerate system trajectories. An important challenge is that the simulator\nimplicitly models a probability distribution over the set of trajectories of\nthe SCPS; however, it is typical to have a sim2real gap, i.e., the actual\ndistribution of the trajectories in a deployment setting may be shifted from\nthe distribution assumed by the simulator. We thus propose a statistical\nreachability analysis technique that, given a user-provided threshold\n$1-\\epsilon$, provides a set that guarantees that any reachable state during\ndeployment lies in this set with probability not smaller than this threshold.\nOur method is based on three main steps: (1) learning a deterministic surrogate\nmodel from sampled trajectories, (2) conducting reachability analysis over the\nsurrogate model, and (3) employing {\\em robust conformal inference} using an\nadditional set of sampled trajectories to quantify the surrogate model's\ndistribution shift with respect to the deployed SCPS. To counter conservatism\nin reachable sets, we propose a novel method to train surrogate models that\nminimizes a quantile loss term (instead of the usual mean squared loss), and a\nnew method that provides tighter guarantees using conformal inference using a\nnormalized surrogate error. We demonstrate the effectiveness of our technique\non various case studies.",
    "authors": [
      "Navid Hashemi",
      "Lars Lindemann",
      "Jyotirmoy V. Deshmukh"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11609v1",
    "category": "Robotics"
  },
  {
    "title": "The Foundations of Tokenization: Statistical and Computational Concerns",
    "abstract": "Tokenization - the practice of converting strings of characters over an\nalphabet into sequences of tokens over a vocabulary - is a critical yet\nunder-theorized step in the NLP pipeline. Notably, it remains the only major\nstep not fully integrated into widely used end-to-end neural models. This paper\naims to address this theoretical gap by laying the foundations of tokenization\nfrom a formal perspective. By articulating and extending basic properties about\nthe category of stochastic maps, we propose a unified framework for\nrepresenting and analyzing tokenizer models. This framework allows us to\nestablish general conditions for the use of tokenizers. In particular, we\nformally establish the necessary and sufficient conditions for a tokenizer\nmodel to preserve the consistency of statistical estimators. Additionally, we\ndiscuss statistical and computational concerns crucial for the design and\nimplementation of tokenizer models. The framework and results advanced in this\npaper represent a step toward a robust theoretical foundation for neural\nlanguage modeling.",
    "authors": [
      "Juan Luis Gastaldi",
      "John Terilla",
      "Luca Malagutti",
      "Brian DuSell",
      "Tim Vieira",
      "Ryan Cotterell"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11606v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "Enhancing TinyML Security: Study of Adversarial Attack Transferability",
    "abstract": "The recent strides in artificial intelligence (AI) and machine learning (ML)\nhave propelled the rise of TinyML, a paradigm enabling AI computations at the\nedge without dependence on cloud connections. While TinyML offers real-time\ndata analysis and swift responses critical for diverse applications, its\ndevices' intrinsic resource limitations expose them to security risks. This\nresearch delves into the adversarial vulnerabilities of AI models on\nresource-constrained embedded hardware, with a focus on Model Extraction and\nEvasion Attacks. Our findings reveal that adversarial attacks from powerful\nhost machines could be transferred to smaller, less secure devices like ESP32\nand Raspberry Pi. This illustrates that adversarial attacks could be extended\nto tiny devices, underscoring vulnerabilities, and emphasizing the necessity\nfor reinforced security measures in TinyML deployments. This exploration\nenhances the comprehension of security challenges in TinyML and offers insights\nfor safeguarding sensitive data and ensuring device dependability in AI-powered\nedge computing settings.",
    "authors": [
      "Parin Shah",
      "Yuvaraj Govindarajulu",
      "Pavan Kulkarni",
      "Manojkumar Parmar"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11599v1",
    "category": "Machine Learning"
  },
  {
    "title": "DiNO-Diffusion. Scaling Medical Diffusion via Self-Supervised Pre-Training",
    "abstract": "Diffusion models (DMs) have emerged as powerful foundation models for a\nvariety of tasks, with a large focus in synthetic image generation. However,\ntheir requirement of large annotated datasets for training limits their\napplicability in medical imaging, where datasets are typically smaller and\nsparsely annotated. We introduce DiNO-Diffusion, a self-supervised method for\ntraining latent diffusion models (LDMs) that conditions the generation process\non image embeddings extracted from DiNO. By eliminating the reliance on\nannotations, our training leverages over 868k unlabelled images from public\nchest X-Ray (CXR) datasets. Despite being self-supervised, DiNO-Diffusion shows\ncomprehensive manifold coverage, with FID scores as low as 4.7, and emerging\nproperties when evaluated in downstream tasks. It can be used to generate\nsemantically-diverse synthetic datasets even from small data pools,\ndemonstrating up to 20% AUC increase in classification performance when used\nfor data augmentation. Images were generated with different sampling strategies\nover the DiNO embedding manifold and using real images as a starting point.\nResults suggest, DiNO-Diffusion could facilitate the creation of large datasets\nfor flexible training of downstream AI models from limited amount of real data,\nwhile also holding potential for privacy preservation. Additionally,\nDiNO-Diffusion demonstrates zero-shot segmentation performance of up to 84.4%\nDice score when evaluating lung lobe segmentation. This evidences good CXR\nimage-anatomy alignment, akin to segmenting using textual descriptors on\nvanilla DMs. Finally, DiNO-Diffusion can be easily adapted to other medical\nimaging modalities or state-of-the-art diffusion models, opening the door for\nlarge-scale, multi-domain image generation pipelines for medical imaging.",
    "authors": [
      "Guillermo Jimenez-Perez",
      "Pedro Osorio",
      "Josef Cersovsky",
      "Javier Montalt-Tordera",
      "Jens Hooge",
      "Steffen Vogler",
      "Sadegh Mohammadi"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11594v1",
    "category": "Machine Learning"
  },
  {
    "title": "QVD: Post-training Quantization for Video Diffusion Models",
    "abstract": "Recently, video diffusion models (VDMs) have garnered significant attention\ndue to their notable advancements in generating coherent and realistic video\ncontent. However, processing multiple frame features concurrently, coupled with\nthe considerable model size, results in high latency and extensive memory\nconsumption, hindering their broader application. Post-training quantization\n(PTQ) is an effective technique to reduce memory footprint and improve\ncomputational efficiency. Unlike image diffusion, we observe that the temporal\nfeatures, which are integrated into all frame features, exhibit pronounced\nskewness. Furthermore, we investigate significant inter-channel disparities and\nasymmetries in the activation of video diffusion models, resulting in low\ncoverage of quantization levels by individual channels and increasing the\nchallenge of quantization. To address these issues, we introduce the first PTQ\nstrategy tailored for video diffusion models, dubbed QVD. Specifically, we\npropose the High Temporal Discriminability Quantization (HTDQ) method, designed\nfor temporal features, which retains the high discriminability of quantized\nfeatures, providing precise temporal guidance for all video frames. In\naddition, we present the Scattered Channel Range Integration (SCRI) method\nwhich aims to improve the coverage of quantization levels across individual\nchannels. Experimental validations across various models, datasets, and\nbit-width settings demonstrate the effectiveness of our QVD in terms of diverse\nmetrics. In particular, we achieve near-lossless performance degradation on\nW8A8, outperforming the current methods by 205.12 in FVD.",
    "authors": [
      "Shilong Tian",
      "Hong Chen",
      "Chengtao Lv",
      "Yu Liu",
      "Jinyang Guo",
      "Xianglong Liu",
      "Shengxi Li",
      "Hao Yang",
      "Tao Xie"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11585v1",
    "category": "Computer Vision"
  },
  {
    "title": "Probing the Efficacy of Federated Parameter-Efficient Fine-Tuning of Vision Transformers for Medical Image Classification",
    "abstract": "With the advent of large pre-trained transformer models, fine-tuning these\nmodels for various downstream tasks is a critical problem. Paucity of training\ndata, the existence of data silos, and stringent privacy constraints exacerbate\nthis fine-tuning problem in the medical imaging domain, creating a strong need\nfor algorithms that enable collaborative fine-tuning of pre-trained models.\nMoreover, the large size of these models necessitates the use of\nparameter-efficient fine-tuning (PEFT) to reduce the communication burden in\nfederated learning. In this work, we systematically investigate various\nfederated PEFT strategies for adapting a Vision Transformer (ViT) model\n(pre-trained on a large natural image dataset) for medical image\nclassification. Apart from evaluating known PEFT techniques, we introduce new\nfederated variants of PEFT algorithms such as visual prompt tuning (VPT),\nlow-rank decomposition of visual prompts, stochastic block attention\nfine-tuning, and hybrid PEFT methods like low-rank adaptation (LoRA)+VPT.\nMoreover, we perform a thorough empirical analysis to identify the optimal PEFT\nmethod for the federated setting and understand the impact of data distribution\non federated PEFT, especially for out-of-domain (OOD) and non-IID data. The key\ninsight of this study is that while most federated PEFT methods work well for\nin-domain transfer, there is a substantial accuracy vs. efficiency trade-off\nwhen dealing with OOD and non-IID scenarios, which is commonly the case in\nmedical imaging. Specifically, every order of magnitude reduction in\nfine-tuned/exchanged parameters can lead to a 4% drop in accuracy. Thus, the\ninitial model choice is crucial for federated PEFT. It is preferable to use\nmedical foundation models learned from in-domain medical image data (if\navailable) rather than general vision models.",
    "authors": [
      "Naif Alkhunaizi",
      "Faris Almalik",
      "Rouqaiah Al-Refai",
      "Muzammal Naseer",
      "Karthik Nandakumar"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11573v1",
    "category": "Machine Learning"
  },
  {
    "title": "TGIF: Text-Guided Inpainting Forgery Dataset",
    "abstract": "Digital image manipulation has become increasingly accessible and realistic\nwith the advent of generative AI technologies. Recent developments allow for\ntext-guided inpainting, making sophisticated image edits possible with minimal\neffort. This poses new challenges for digital media forensics. For example,\ndiffusion model-based approaches could either splice the inpainted region into\nthe original image, or regenerate the entire image. In the latter case,\ntraditional image forgery localization (IFL) methods typically fail. This paper\nintroduces the Text-Guided Inpainting Forgery (TGIF) dataset, a comprehensive\ncollection of images designed to support the training and evaluation of image\nforgery localization and synthetic image detection (SID) methods. The TGIF\ndataset includes approximately 80k forged images, originating from popular\nopen-source and commercial methods; SD2, SDXL, and Adobe Firefly. Using this\ndata, we benchmark several state-of-the-art IFL and SID methods. Whereas\ntraditional IFL methods can detect spliced images, they fail to detect\nregenerated inpainted images. Moreover, traditional SID may detect the\nregenerated inpainted images to be fake, but cannot localize the inpainted\narea. Finally, both types of methods fail when exposed to stronger compression,\nwhile they are less robust to modern compression algorithms, such as WEBP. As\nsuch, this work demonstrates the inefficiency of state-of-the-art detectors on\nlocal manipulations performed by modern generative approaches, and aspires to\nhelp with the development of more capable IFL and SID methods. The dataset can\nbe downloaded at https://github.com/IDLabMedia/tgif-dataset.",
    "authors": [
      "Hannes Mareen",
      "Dimitrios Karageorgiou",
      "Glenn Van Wallendael",
      "Peter Lambert",
      "Symeon Papadopoulos"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11566v1",
    "category": "Computer Vision"
  },
  {
    "title": "RobotKeyframing: Learning Locomotion with High-Level Objectives via Mixture of Dense and Sparse Rewards",
    "abstract": "This paper presents a novel learning-based control framework that uses\nkeyframing to incorporate high-level objectives in natural locomotion for\nlegged robots. These high-level objectives are specified as a variable number\nof partial or complete pose targets that are spaced arbitrarily in time. Our\nproposed framework utilizes a multi-critic reinforcement learning algorithm to\neffectively handle the mixture of dense and sparse rewards. Additionally, it\nemploys a transformer-based encoder to accommodate a variable number of input\ntargets, each associated with specific time-to-arrivals. Throughout simulation\nand hardware experiments, we demonstrate that our framework can effectively\nsatisfy the target keyframe sequence at the required times. In the experiments,\nthe multi-critic method significantly reduces the effort of hyperparameter\ntuning compared to the standard single-critic alternative. Moreover, the\nproposed transformer-based architecture enables robots to anticipate future\ngoals, which results in quantitative improvements in their ability to reach\ntheir targets.",
    "authors": [
      "Fatemeh Zargarbashi",
      "Jin Cheng",
      "Dongho Kang",
      "Robert Sumner",
      "Stelian Coros"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11562v1",
    "category": "Robotics"
  },
  {
    "title": "Self-Guided Generation of Minority Samples Using Diffusion Models",
    "abstract": "We present a novel approach for generating minority samples that live on\nlow-density regions of a data manifold. Our framework is built upon diffusion\nmodels, leveraging the principle of guided sampling that incorporates an\narbitrary energy-based guidance during inference time. The key defining feature\nof our sampler lies in its \\emph{self-contained} nature, \\ie, implementable\nsolely with a pretrained model. This distinguishes our sampler from existing\ntechniques that require expensive additional components (like external\nclassifiers) for minority generation. Specifically, we first estimate the\nlikelihood of features within an intermediate latent sample by evaluating a\nreconstruction loss w.r.t. its posterior mean. The generation then proceeds\nwith the minimization of the estimated likelihood, thereby encouraging the\nemergence of minority features in the latent samples of subsequent timesteps.\nTo further improve the performance of our sampler, we provide several\ntime-scheduling techniques that properly manage the influence of guidance over\ninference steps. Experiments on benchmark real datasets demonstrate that our\napproach can greatly improve the capability of creating realistic\nlow-likelihood minority instances over the existing techniques without the\nreliance on costly additional elements. Code is available at\n\\url{https://github.com/soobin-um/sg-minority}.",
    "authors": [
      "Soobin Um",
      "Jong Chul Ye"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11555v1",
    "category": "Machine Learning"
  },
  {
    "title": "Learning Global and Local Features of Power Load Series Through Transformer and 2D-CNN: An image-based Multi-step Forecasting Approach Incorporating Phase Space Reconstruction",
    "abstract": "As modern power systems continue to evolve, accurate power load forecasting\nremains a critical issue. The phase space reconstruction method can effectively\nretain the chaotic characteristics of power load from a system dynamics\nperspective and thus is a promising knowledge-based preprocessing method for\npower load forecasting. However, limited by its fundamental theory, there is\nstill a gap in implementing a multi-step forecasting scheme in current studies.\nTo bridge this gap, this study proposes a novel multi-step forecasting approach\nby integrating the PSR with neural networks. Firstly, the useful features in\nthe phase trajectory obtained from the preprocessing of PSR are discussed in\ndetail. Through mathematical derivation, the equivalent characterization of the\nPSR and another time series preprocessing method, patch segmentation, is\ndemonstrated for the first time. Based on this prior knowledge, an image-based\nmodeling perspective with the global and local feature extraction strategy is\nintroduced. Subsequently, a novel deep learning model, namely PSR-GALIEN, is\ndesigned for end-to-end processing, in which the Transformer Encoder and\n2D-convolutional neural networks are employed for the extraction of the global\nand local patterns in the image, and a multi-layer perception based predictor\nis used for the efficient correlation modeling. Then, extensive experiments are\nconducted on five real-world benchmark datasets to verify the effectiveness as\nwell as to have an insight into the detailed properties. The results show that,\ncomparing it with six state-of-the-art deep learning models, the forecasting\nperformance of PSR-GALIEN consistently surpasses these baselines, which\nachieves superior accuracy in both intra-day and day-ahead forecasting\nscenarios. At the same time, a visualization-based method is proposed to\nexplain the attributions of the forecasting results.",
    "authors": [
      "Zihan Tang",
      "Tianyao Ji",
      "Wenhu Tang"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11553v1",
    "category": "Machine Learning"
  },
  {
    "title": "Optimizing KV Cache Eviction in LLMs: Adaptive Allocation for Enhanced Budget Utilization",
    "abstract": "Large Language Models have excelled in various fields but encounter\nefficiency limitations due to the extensive KV cache required for long\nsequences inference. Many efforts try to evict non-critical cache elements\nduring runtime, thereby reducing cache size within a given memory budget while\npreserving generation quality. Our reexamination of their underlying principles\ndiscerns that prevailing strategies essentially aim to minimize an upper bound\nof eviction loss within a specific budget allocation. However, we observe that\nthe current practice of uniformly allocating budgets across different attention\nheads during the eviction procedure tends to degrade the quality of generation\nposten-eviction. In light of these findings, we propose a simple yet effective\nadaptive allocation algorithm that not only theoretically ensures its loss\nupper bound does not exceed that of previous uniform allocation methods, but\nalso effectively aligns with the characteristics of the self-attention\nmechanism, thus practically reducing the upper bound. Further, integrating this\nalgorithm with two of the most advanced methods yields Ada-SnapKV and\nAda-Pyramid. Extensive experimental validation across 16 datasets and the\nNeedle-in-a-Haystack test confirm that Ada-SnapKV and Ada-Pyramid achieve\nfurther enhancements, establishing new benchmarks in state-of-the-art\nperformance.",
    "authors": [
      "Yuan Feng",
      "Junlin Lv",
      "Yukun Cao",
      "Xike Xie",
      "S. Kevin Zhou"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11550v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models",
    "abstract": "Psychological evidence reveals the influence of personality traits on\ndecision-making. For instance, agreeableness is generally associated with\npositive outcomes in negotiations, whereas neuroticism is often linked to less\nfavorable outcomes. This paper introduces a simulation framework centered on\nLarge Language Model (LLM) agents endowed with synthesized personality traits.\nThe agents negotiate within bargaining domains and possess customizable\npersonalities and objectives. The experimental results show that the behavioral\ntendencies of LLM-based simulations could reproduce behavioral patterns\nobserved in human negotiations. The contribution is twofold. First, we propose\na simulation methodology that investigates the alignment between the linguistic\nand economic capabilities of LLM agents. Secondly, we offer empirical insights\ninto the strategic impact of Big-Five personality traits on the outcomes of\nbilateral negotiations. We also provide a case study based on synthesized\nbargaining dialogues to reveal intriguing behaviors, including deceitful and\ncompromising behaviors.",
    "authors": [
      "Yin Jou Huang",
      "Rafik Hadfi"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11549v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "AEMIM: Adversarial Examples Meet Masked Image Modeling",
    "abstract": "Masked image modeling (MIM) has gained significant traction for its\nremarkable prowess in representation learning. As an alternative to the\ntraditional approach, the reconstruction from corrupted images has recently\nemerged as a promising pretext task. However, the regular corrupted images are\ngenerated using generic generators, often lacking relevance to the specific\nreconstruction task involved in pre-training. Hence, reconstruction from\nregular corrupted images cannot ensure the difficulty of the pretext task,\npotentially leading to a performance decline. Moreover, generating corrupted\nimages might introduce an extra generator, resulting in a notable computational\nburden. To address these issues, we propose to incorporate adversarial examples\ninto masked image modeling, as the new reconstruction targets. Adversarial\nexamples, generated online using only the trained models, can directly aim to\ndisrupt tasks associated with pre-training. Therefore, the incorporation not\nonly elevates the level of challenge in reconstruction but also enhances\nefficiency, contributing to the acquisition of superior representations by the\nmodel. In particular, we introduce a novel auxiliary pretext task that\nreconstructs the adversarial examples corresponding to the original images. We\nalso devise an innovative adversarial attack to craft more suitable adversarial\nexamples for MIM pre-training. It is noted that our method is not restricted to\nspecific model architectures and MIM strategies, rendering it an adaptable\nplug-in capable of enhancing all MIM methods. Experimental findings\nsubstantiate the remarkable capability of our approach in amplifying the\ngeneralization and robustness of existing MIM methods. Notably, our method\nsurpasses the performance of baselines on various tasks, including ImageNet,\nits variants, and other downstream tasks.",
    "authors": [
      "Wenzhao Xiang",
      "Chang Liu",
      "Hang Su",
      "Hongyang Yu"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11537v1",
    "category": "Machine Learning"
  },
  {
    "title": "Fine-Tuning Medical Language Models for Enhanced Long-Contextual Understanding and Domain Expertise",
    "abstract": "Large Language Models (LLMs) have been widely applied in various professional\nfields. By fine-tuning the models using domain specific question and answer\ndatasets, the professional domain knowledge and Q\\&A abilities of these models\nhave significantly improved, for example, medical professional LLMs that use\nfine-tuning of doctor-patient Q\\&A data exhibit extraordinary disease\ndiagnostic abilities. However, we observed that despite improvements in\nspecific domain knowledge, the performance of medical LLM in long-context\nunderstanding has significantly declined, especially compared to general\nlanguage models with similar parameters. The purpose of this study is to\ninvestigate the phenomenon of reduced performance in understanding long-context\nin medical LLM. We designed a series of experiments to conduct open-book\nprofessional knowledge exams on all models to evaluate their ability to read\nlong-context. By adjusting the proportion and quantity of general data and\nmedical data in the process of fine-tuning, we can determine the best data\ncomposition to optimize the professional model and achieve a balance between\nlong-context performance and specific domain knowledge.",
    "authors": [
      "Qimin Yang",
      "Rongsheng Wang",
      "Jiexin Chen",
      "Runqi Su",
      "Tao Tan"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11536v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "LRQ: Optimizing Post-Training Quantization for Large Language Models by Learning Low-Rank Weight-Scaling Matrices",
    "abstract": "With the commercialization of large language models (LLMs), weight-activation\nquantization has emerged to compress and accelerate LLMs, achieving high\nthroughput while reducing inference costs. However, existing post-training\nquantization (PTQ) techniques for quantizing weights and activations of LLMs\nstill suffer from non-negligible accuracy drops, especially on massive\nmultitask language understanding. To address this issue, we propose Low-Rank\nQuantization (LRQ) $-$ a simple yet effective post-training weight quantization\nmethod for LLMs that reconstructs the outputs of an intermediate Transformer\nblock by leveraging low-rank weight-scaling matrices, replacing the\nconventional full weight-scaling matrices that entail as many learnable scales\nas their associated weights. Thanks to parameter sharing via low-rank\nstructure, LRQ only needs to learn significantly fewer parameters while\nenabling the individual scaling of weights, thus boosting the generalization\ncapability of quantized LLMs. We show the superiority of LRQ over prior LLM PTQ\nworks under (i) $8$-bit weight and per-tensor activation quantization, (ii)\n$4$-bit weight and $8$-bit per-token activation quantization, and (iii) low-bit\nweight-only quantization schemes. Our code is available at\n\\url{https://github.com/onliwad101/FlexRound_LRQ} to inspire LLM researchers\nand engineers.",
    "authors": [
      "Jung Hyun Lee",
      "Jeonghoon Kim",
      "June Yong Yang",
      "Se Jung Kwon",
      "Eunho Yang",
      "Kang Min Yoo",
      "Dongsoo Lee"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11534v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "Cross-Phase Mutual Learning Framework for Pulmonary Embolism Identification on Non-Contrast CT Scans",
    "abstract": "Pulmonary embolism (PE) is a life-threatening condition where rapid and\naccurate diagnosis is imperative yet difficult due to predominantly atypical\nsymptomatology. Computed tomography pulmonary angiography (CTPA) is\nacknowledged as the gold standard imaging tool in clinics, yet it can be\ncontraindicated for emergency department (ED) patients and represents an\nonerous procedure, thus necessitating PE identification through non-contrast CT\n(NCT) scans. In this work, we explore the feasibility of applying a\ndeep-learning approach to NCT scans for PE identification. We propose a novel\nCross-Phase Mutual learNing framework (CPMN) that fosters knowledge transfer\nfrom CTPA to NCT, while concurrently conducting embolism segmentation and\nabnormality classification in a multi-task manner. The proposed CPMN leverages\nthe Inter-Feature Alignment (IFA) strategy that enhances spatial contiguity and\nmutual learning between the dual-pathway network, while the Intra-Feature\nDiscrepancy (IFD) strategy can facilitate precise segmentation of PE against\ncomplex backgrounds for single-pathway networks. For a comprehensive assessment\nof the proposed approach, a large-scale dual-phase dataset containing 334 PE\npatients and 1,105 normal subjects has been established. Experimental results\ndemonstrate that CPMN achieves the leading identification performance, which is\n95.4\\% and 99.6\\% in patient-level sensitivity and specificity on NCT scans,\nindicating the potential of our approach as an economical, accessible, and\nprecise tool for PE identification in clinical practice.",
    "authors": [
      "Bizhe Bai",
      "Yan-Jie Zhou",
      "Yujian Hu",
      "Tony C. W. Mok",
      "Yilang Xiang",
      "Le Lu",
      "Hongkun Zhang",
      "Minfeng Xu"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11529v1",
    "category": "Machine Learning"
  },
  {
    "title": "Reasoning with Large Language Models, a Survey",
    "abstract": "Scaling up language models to billions of parameters has opened up\npossibilities for in-context learning, allowing instruction tuning and few-shot\nlearning on tasks that the model was not specifically trained for. This has\nachieved breakthrough performance on language tasks such as translation,\nsummarization, and question-answering. Furthermore, in addition to these\nassociative \"System 1\" tasks, recent advances in Chain-of-thought prompt\nlearning have demonstrated strong \"System 2\" reasoning abilities, answering a\nquestion in the field of artificial general intelligence whether LLMs can\nreason. The field started with the question whether LLMs can solve grade school\nmath word problems. This paper reviews the rapidly expanding field of\nprompt-based reasoning with LLMs. Our taxonomy identifies different ways to\ngenerate, evaluate, and control multi-step reasoning. We provide an in-depth\ncoverage of core approaches and open problems, and we propose a research agenda\nfor the near future. Finally, we highlight the relation between reasoning and\nprompt-based learning, and we discuss the relation between reasoning,\nsequential decision processes, and reinforcement learning. We find that\nself-improvement, self-reflection, and some metacognitive abilities of the\nreasoning processes are possible through the judicious use of prompts. True\nself-improvement and self-reasoning, to go from reasoning with LLMs to\nreasoning by LLMs, remains future work.",
    "authors": [
      "Aske Plaat",
      "Annie Wong",
      "Suzan Verberne",
      "Joost Broekens",
      "Niki van Stein",
      "Thomas Back"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11511v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "Diff-MTS: Temporal-Augmented Conditional Diffusion-based AIGC for Industrial Time Series Towards the Large Model Era",
    "abstract": "Industrial Multivariate Time Series (MTS) is a critical view of the\nindustrial field for people to understand the state of machines. However, due\nto data collection difficulty and privacy concerns, available data for building\nindustrial intelligence and industrial large models is far from sufficient.\nTherefore, industrial time series data generation is of great importance.\nExisting research usually applies Generative Adversarial Networks (GANs) to\ngenerate MTS. However, GANs suffer from unstable training process due to the\njoint training of the generator and discriminator. This paper proposes a\ntemporal-augmented conditional adaptive diffusion model, termed Diff-MTS, for\nMTS generation. It aims to better handle the complex temporal dependencies and\ndynamics of MTS data. Specifically, a conditional Adaptive Maximum-Mean\nDiscrepancy (Ada-MMD) method has been proposed for the controlled generation of\nMTS, which does not require a classifier to control the generation. It improves\nthe condition consistency of the diffusion model. Moreover, a Temporal\nDecomposition Reconstruction UNet (TDR-UNet) is established to capture complex\ntemporal patterns and further improve the quality of the synthetic time series.\nComprehensive experiments on the C-MAPSS and FEMTO datasets demonstrate that\nthe proposed Diff-MTS performs substantially better in terms of diversity,\nfidelity, and utility compared with GAN-based methods. These results show that\nDiff-MTS facilitates the generation of industrial data, contributing to\nintelligent maintenance and the construction of industrial large models.",
    "authors": [
      "Lei Ren",
      "Haiteng Wang",
      "Yuanjun Laili"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11501v1",
    "category": "Machine Learning"
  },
  {
    "title": "An AI System for Continuous Knee Osteoarthritis Severity Grading Using Self-Supervised Anomaly Detection with Limited Data",
    "abstract": "The diagnostic accuracy and subjectivity of existing Knee Osteoarthritis (OA)\nordinal grading systems has been a subject of on-going debate and concern.\nExisting automated solutions are trained to emulate these imperfect systems,\nwhilst also being reliant on large annotated databases for fully-supervised\ntraining. This work proposes a three stage approach for automated continuous\ngrading of knee OA that is built upon the principles of Anomaly Detection (AD);\nlearning a robust representation of healthy knee X-rays and grading disease\nseverity based on its distance to the centre of normality. In the first stage,\nSS-FewSOME is proposed, a self-supervised AD technique that learns the 'normal'\nrepresentation, requiring only examples of healthy subjects and <3% of the\nlabels that existing methods require. In the second stage, this model is used\nto pseudo label a subset of unlabelled data as 'normal' or 'anomalous',\nfollowed by denoising of pseudo labels with CLIP. The final stage involves\nretraining on labelled and pseudo labelled data using the proposed Dual Centre\nRepresentation Learning (DCRL) which learns the centres of two representation\nspaces; normal and anomalous. Disease severity is then graded based on the\ndistance to the learned centres. The proposed methodology outperforms existing\ntechniques by margins of up to 24% in terms of OA detection and the disease\nseverity scores correlate with the Kellgren-Lawrence grading system at the same\nlevel as human expert performance. Code available at\nhttps://github.com/niamhbelton/SS-FewSOME_Disease_Severity_Knee_Osteoarthritis.",
    "authors": [
      "Niamh Belton",
      "Aonghus Lawlor",
      "Kathleen M. Curran"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11500v1",
    "category": "Machine Learning"
  },
  {
    "title": "A Meta-Learning Approach for Multi-Objective Reinforcement Learning in Sustainable Home Environments",
    "abstract": "Effective residential appliance scheduling is crucial for sustainable living.\nWhile multi-objective reinforcement learning (MORL) has proven effective in\nbalancing user preferences in appliance scheduling, traditional MORL struggles\nwith limited data in non-stationary residential settings characterized by\nrenewable generation variations. Significant context shifts that can invalidate\npreviously learned policies. To address these challenges, we extend\nstate-of-the-art MORL algorithms with the meta-learning paradigm, enabling\nrapid, few-shot adaptation to shifting contexts. Additionally, we employ an\nauto-encoder (AE)-based unsupervised method to detect environment context\nchanges. We have also developed a residential energy environment to evaluate\nour method using real-world data from London residential settings. This study\nnot only assesses the application of MORL in residential appliance scheduling\nbut also underscores the effectiveness of meta-learning in energy management.\nOur top-performing method significantly surpasses the best baseline, while the\ntrained model saves 3.28% on electricity bills, a 2.74% increase in user\ncomfort, and a 5.9% improvement in expected utility. Additionally, it reduces\nthe sparsity of solutions by 62.44%. Remarkably, these gains were accomplished\nusing 96.71% less training data and 61.1% fewer training steps.",
    "authors": [
      "Junlin Lu",
      "Patrick Mannion",
      "Karl Mason"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11489v1",
    "category": "Machine Learning"
  },
  {
    "title": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models",
    "abstract": "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers.",
    "authors": [
      "Nuo Chen",
      "Y. Wang",
      "Yang Deng",
      "Jia Li"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11484v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "Multi-Channel Masked Autoencoder and Comprehensive Evaluations for Reconstructing 12-Lead ECG from Arbitrary Single-Lead ECG",
    "abstract": "In the context of cardiovascular diseases (CVD) that exhibit an elevated\nprevalence and mortality, the electrocardiogram (ECG) is a popular and standard\ndiagnostic tool for doctors, commonly utilizing a 12-lead configuration in\nclinical practice. However, the 10 electrodes placed on the surface would cause\na lot of inconvenience and discomfort, while the rapidly advancing wearable\ndevices adopt the reduced-lead or single-lead ECG to reduce discomfort as a\nsolution in long-term monitoring. Since the single-lead ECG is a subset of\n12-lead ECG, it provides insufficient cardiac health information and plays a\nsubstandard role in real-world healthcare applications. Hence, it is necessary\nto utilize signal generation technologies to reduce their clinical importance\ngap by reconstructing 12-lead ECG from the real single-lead ECG. Specifically,\nthis study proposes a multi-channel masked autoencoder (MCMA) for this goal. In\nthe experimental results, the visualized results between the generated and real\nsignals can demonstrate the effectiveness of the proposed framework. At the\nsame time, this study introduces a comprehensive evaluation benchmark named\nECGGenEval, encompassing the signal-level, feature-level, and diagnostic-level\nevaluations, providing a holistic assessment of 12-lead ECG signals and\ngenerative model. Further, the quantitative experimental results are as\nfollows, the mean square errors of 0.0178 and 0.0658, correlation coefficients\nof 0.7698 and 0.7237 in the signal-level evaluation, the average F1-score with\ntwo generated 12-lead ECG is 0.8319 and 0.7824 in the diagnostic-level\nevaluation, achieving the state-of-the-art performance. The open-source code is\npublicly available at \\url{https://github.com/CHENJIAR3/MCMA}.",
    "authors": [
      "Jiarong Chen",
      "Wanqing Wu",
      "Tong Liu",
      "Shenda Hong"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11481v1",
    "category": "Machine Learning"
  },
  {
    "title": "AIGC for Industrial Time Series: From Deep Generative Models to Large Generative Models",
    "abstract": "With the remarkable success of generative models like ChatGPT, Artificial\nIntelligence Generated Content (AIGC) is undergoing explosive development. Not\nlimited to text and images, generative models can generate industrial time\nseries data, addressing challenges such as the difficulty of data collection\nand data annotation. Due to their outstanding generation ability, they have\nbeen widely used in Internet of Things, metaverse, and cyber-physical-social\nsystems to enhance the efficiency of industrial production. In this paper, we\npresent a comprehensive overview of generative models for industrial time\nseries from deep generative models (DGMs) to large generative models (LGMs).\nFirst, a DGM-based AIGC framework is proposed for industrial time series\ngeneration. Within this framework, we survey advanced industrial DGMs and\npresent a multi-perspective categorization. Furthermore, we systematically\nanalyze the critical technologies required to construct industrial LGMs from\nfour aspects: large-scale industrial dataset, LGMs architecture for complex\nindustrial characteristics, self-supervised training for industrial time\nseries, and fine-tuning of industrial downstream tasks. Finally, we conclude\nthe challenges and future directions to enable the development of generative\nmodels in industry.",
    "authors": [
      "Lei Ren",
      "Haiteng Wang",
      "Yang Tang",
      "Chunhua Yang"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11480v1",
    "category": "Machine Learning"
  },
  {
    "title": "XTraffic: A Dataset Where Traffic Meets Incidents with Explainability and More",
    "abstract": "Long-separated research has been conducted on two highly correlated tracks:\ntraffic and incidents. Traffic track witnesses complicating deep learning\nmodels, e.g., to push the prediction a few percent more accurate, and the\nincident track only studies the incidents alone, e.g., to infer the incident\nrisk. We, for the first time, spatiotemporally aligned the two tracks in a\nlarge-scale region (16,972 traffic nodes) over the whole year of 2023: our\nXTraffic dataset includes traffic, i.e., time-series indexes on traffic flow,\nlane occupancy, and average vehicle speed, and incidents, whose records are\nspatiotemporally-aligned with traffic data, with seven different incident\nclasses. Additionally, each node includes detailed physical and policy-level\nmeta-attributes of lanes. Our data can revolutionalize traditional\ntraffic-related tasks towards higher interpretability and practice: instead of\ntraditional prediction or classification tasks, we conduct: (1) post-incident\ntraffic forecasting to quantify the impact of different incidents on traffic\nindexes; (2) incident classification using traffic indexes to determine the\nincidents types for precautions measures; (3) global causal analysis among the\ntraffic indexes, meta-attributes, and incidents to give high-level guidance of\nthe interrelations of various factors; (4) local causal analysis within road\nnodes to examine how different incidents affect the road segments' relations.\nThe dataset is available at http://xaitraffic.github.io.",
    "authors": [
      "Xiaochuan Gou",
      "Ziyue Li",
      "Tian Lan",
      "Junpeng Lin",
      "Zhishuai Li",
      "Bingyu Zhao",
      "Chen Zhang",
      "Di Wang",
      "Xiangliang Zhang"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11477v1",
    "category": "Machine Learning"
  },
  {
    "title": "DynSyn: Dynamical Synergistic Representation for Efficient Learning and Control in Overactuated Embodied Systems",
    "abstract": "Learning an effective policy to control high-dimensional, overactuated\nsystems is a significant challenge for deep reinforcement learning algorithms.\nSuch control scenarios are often observed in the neural control of vertebrate\nmusculoskeletal systems. The study of these control mechanisms will provide\ninsights into the control of high-dimensional, overactuated systems. The\ncoordination of actuators, known as muscle synergies in neuromechanics, is\nconsidered a presumptive mechanism that simplifies the generation of motor\ncommands. The dynamical structure of a system is the basis of its function,\nallowing us to derive a synergistic representation of actuators. Motivated by\nthis theory, we propose the Dynamical Synergistic Representation (DynSyn)\nalgorithm. DynSyn aims to generate synergistic representations from dynamical\nstructures and perform task-specific, state-dependent adaptation to the\nrepresentations to improve motor control. We demonstrate DynSyn's efficiency\nacross various tasks involving different musculoskeletal models, achieving\nstate-of-the-art sample efficiency and robustness compared to baseline\nalgorithms. DynSyn generates interpretable synergistic representations that\ncapture the essential features of dynamical structures and demonstrates\ngeneralizability across diverse motor tasks.",
    "authors": [
      "Kaibo He",
      "Chenhui Zuo",
      "Chengtian Ma",
      "Yanan Sui"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11472v1",
    "category": "Robotics"
  },
  {
    "title": "Beyond Correctness: Benchmarking Multi-dimensional Code Generation for Large Language Models",
    "abstract": "In recent years, researchers have proposed numerous benchmarks to evaluate\nthe impressive coding capabilities of large language models (LLMs). However,\nexisting benchmarks primarily focus on assessing the correctness of code\ngenerated by LLMs, while neglecting other critical dimensions that also\nsignificantly impact code quality. Therefore, this paper proposes the RACE\nbenchmark, which comprehensively evaluates the quality of code generated by\nLLMs across 4 dimensions: Readability, mAintainability, Correctness, and\nEfficiency. Specifically, considering the demand-dependent nature of dimensions\nbeyond correctness, we design various types of user requirements for each\ndimension to assess the model's ability to generate correct code that also\nmeets user demands. We evaluate 18 representative LLMs on RACE and find that:\n1) the current LLMs' ability to generate high-quality code on demand does not\nyet meet the requirements of software development; 2) readability serves as a\ncritical indicator of the overall quality of generated code; 3) most LLMs\nexhibit an inherent preference for specific coding style. These findings can\nhelp researchers gain a deeper understanding of the coding capabilities of\ncurrent LLMs and shed light on future directions for model improvement.",
    "authors": [
      "Jiasheng Zheng",
      "Boxi Cao",
      "Zhengzhao Ma",
      "Ruotong Pan",
      "Hongyu Lin",
      "Yaojie Lu",
      "Xianpei Han",
      "Le Sun"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11470v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "Investigating Imperceptibility of Adversarial Attacks on Tabular Data: An Empirical Analysis",
    "abstract": "Adversarial attacks are a potential threat to machine learning models, as\nthey can cause the model to make incorrect predictions by introducing\nimperceptible perturbations to the input data. While extensively studied in\nunstructured data like images, their application to structured data like\ntabular data presents unique challenges due to the heterogeneity and intricate\nfeature interdependencies of tabular data. Imperceptibility in tabular data\ninvolves preserving data integrity while potentially causing misclassification,\nunderscoring the need for tailored imperceptibility criteria for tabular data.\nHowever, there is currently a lack of standardised metrics for assessing\nadversarial attacks specifically targeted at tabular data. To address this gap,\nwe derive a set of properties for evaluating the imperceptibility of\nadversarial attacks on tabular data. These properties are defined to capture\nseven perspectives of perturbed data: proximity to original inputs, sparsity of\nalterations, deviation to datapoints in the original dataset, sensitivity of\naltering sensitive features, immutability of perturbation, feasibility of\nperturbed values and intricate feature interdepencies among tabular features.\nFurthermore, we conduct both quantitative empirical evaluation and case-based\nqualitative examples analysis for seven properties. The evaluation reveals a\ntrade-off between attack success and imperceptibility, particularly concerning\nproximity, sensitivity, and deviation. Although no evaluated attacks can\nachieve optimal effectiveness and imperceptibility simultaneously, unbounded\nattacks prove to be more promised for tabular data in crafting imperceptible\nadversarial examples. The study also highlights the limitation of evaluated\nalgorithms in controlling sparsity effectively. We suggest incorporating a\nsparsity metric in future attack design to regulate the number of perturbed\nfeatures.",
    "authors": [
      "Zhipeng He",
      "Chun Ouyang",
      "Laith Alzubaidi",
      "Alistair Barros",
      "Catarina Moreira"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11463v1",
    "category": "Machine Learning"
  },
  {
    "title": "Graceful task adaptation with a bi-hemispheric RL agent",
    "abstract": "In humans, responsibility for performing a task gradually shifts from the\nright hemisphere to the left. The Novelty-Routine Hypothesis (NRH) states that\nthe right and left hemispheres are used to perform novel and routine tasks\nrespectively, enabling us to learn a diverse range of novel tasks while\nperforming the task capably. Drawing on the NRH, we develop a reinforcement\nlearning agent with specialised hemispheres that can exploit generalist\nknowledge from the right-hemisphere to avoid poor initial performance on novel\ntasks. In addition, we find that this design has minimal impact on its ability\nto learn novel tasks. We conclude by identifying improvements to our agent and\nexploring potential expansion to the continual learning setting.",
    "authors": [
      "Grant Nicholas",
      "Levin Kuhlmann",
      "Gideon Kowadlo"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11456v1",
    "category": "Robotics"
  },
  {
    "title": "Controllable Contextualized Image Captioning: Directing the Visual Narrative through User-Defined Highlights",
    "abstract": "Contextualized Image Captioning (CIC) evolves traditional image captioning\ninto a more complex domain, necessitating the ability for multimodal reasoning.\nIt aims to generate image captions given specific contextual information. This\npaper further introduces a novel domain of Controllable Contextualized Image\nCaptioning (Ctrl-CIC). Unlike CIC, which solely relies on broad context,\nCtrl-CIC accentuates a user-defined highlight, compelling the model to tailor\ncaptions that resonate with the highlighted aspects of the context. We present\ntwo approaches, Prompting-based Controller (P-Ctrl) and Recalibration-based\nController (R-Ctrl), to generate focused captions. P-Ctrl conditions the model\ngeneration on highlight by prepending captions with highlight-driven prefixes,\nwhereas R-Ctrl tunes the model to selectively recalibrate the encoder\nembeddings for highlighted tokens. Additionally, we design a GPT-4V empowered\nevaluator to assess the quality of the controlled captions alongside standard\nassessment methods. Extensive experimental results demonstrate the efficient\nand effective controllability of our method, charting a new direction in\nachieving user-adaptive image captioning. Code is available at\nhttps://github.com/ShunqiM/Ctrl-CIC .",
    "authors": [
      "Shunqi Mao",
      "Chaoyi Zhang",
      "Hang Su",
      "Hwanjun Song",
      "Igor Shalyminov",
      "Weidong Cai"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11449v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "EARN Fairness: Explaining, Asking, Reviewing and Negotiating Artificial Intelligence Fairness Metrics Among Stakeholders",
    "abstract": "Numerous fairness metrics have been proposed and employed by artificial\nintelligence (AI) experts to quantitatively measure bias and define fairness in\nAI models. Recognizing the need to accommodate stakeholders' diverse fairness\nunderstandings, efforts are underway to solicit their input. However, conveying\nAI fairness metrics to stakeholders without AI expertise, capturing their\npersonal preferences, and seeking a collective consensus remain challenging and\nunderexplored. To bridge this gap, we propose a new framework, EARN Fairness,\nwhich facilitates collective metric decisions among stakeholders without\nrequiring AI expertise. The framework features an adaptable interactive system\nand a stakeholder-centered EARN Fairness process to Explain fairness metrics,\nAsk stakeholders' personal metric preferences, Review metrics collectively, and\nNegotiate a consensus on metric selection. To gather empirical results, we\napplied the framework to a credit rating scenario and conducted a user study\ninvolving 18 decision subjects without AI knowledge. We identify their personal\nmetric preferences and their acceptable level of unfairness in individual\nsessions. Subsequently, we uncovered how they reached metric consensus in team\nsessions. Our work shows that the EARN Fairness framework enables stakeholders\nto express personal preferences and reach consensus, providing practical\nguidance for implementing human-centered AI fairness in high-risk contexts.\nThrough this approach, we aim to harmonize fairness expectations of diverse\nstakeholders, fostering more equitable and inclusive AI fairness.",
    "authors": [
      "Lin Luo",
      "Yuri Nakao",
      "Mathieu Chollet",
      "Hiroya Inakoshi",
      "Simone Stumpf"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11442v1",
    "category": "Machine Learning"
  },
  {
    "title": "Repurformer: Transformers for Repurposing-Aware Molecule Generation",
    "abstract": "Generating as diverse molecules as possible with desired properties is\ncrucial for drug discovery research, which invokes many approaches based on\ndeep generative models today. Despite recent advancements in these models,\nparticularly in variational autoencoders (VAEs), generative adversarial\nnetworks (GANs), Transformers, and diffusion models, a significant challenge\nknown as \\textit{the sample bias problem} remains. This problem occurs when\ngenerated molecules targeting the same protein tend to be structurally similar,\nreducing the diversity of generation. To address this, we propose leveraging\nmulti-hop relationships among proteins and compounds. Our model, Repurformer,\nintegrates bi-directional pretraining with Fast Fourier Transform (FFT) and\nlow-pass filtering (LPF) to capture complex interactions and generate diverse\nmolecules. A series of experiments on BindingDB dataset confirm that\nRepurformer successfully creates substitutes for anchor compounds that resemble\npositive compounds, increasing diversity between the anchor and generated\ncompounds.",
    "authors": [
      "Changhun Lee",
      "Gyumin Lee"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11439v1",
    "category": "Machine Learning"
  },
  {
    "title": "Generally-Occurring Model Change for Robust Counterfactual Explanations",
    "abstract": "With the increasing impact of algorithmic decision-making on human lives, the\ninterpretability of models has become a critical issue in machine learning.\nCounterfactual explanation is an important method in the field of interpretable\nmachine learning, which can not only help users understand why machine learning\nmodels make specific decisions, but also help users understand how to change\nthese decisions. Naturally, it is an important task to study the robustness of\ncounterfactual explanation generation algorithms to model changes. Previous\nliterature has proposed the concept of Naturally-Occurring Model Change, which\nhas given us a deeper understanding of robustness to model change. In this\npaper, we first further generalize the concept of Naturally-Occurring Model\nChange, proposing a more general concept of model parameter changes,\nGenerally-Occurring Model Change, which has a wider range of applicability. We\nalso prove the corresponding probabilistic guarantees. In addition, we consider\na more specific problem, data set perturbation, and give relevant theoretical\nresults by combining optimization theory.",
    "authors": [
      "Ao Xu",
      "Tieru Wu"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11426v1",
    "category": "Machine Learning"
  },
  {
    "title": "LOTUS: Enabling Semantic Queries with LLMs Over Tables of Unstructured and Structured Data",
    "abstract": "The semantic capabilities of language models (LMs) have the potential to\nenable rich analytics and reasoning over vast knowledge corpora. Unfortunately,\nexisting systems lack high-level abstractions to perform semantic queries at\nscale. We introduce semantic operators, a declarative programming interface\nthat extends the relational model with composable AI-based operations for\nsemantic queries over datasets (e.g., sorting or aggregating records using\nnatural language criteria). Each operator can be implemented and optimized in\nmultiple ways, opening a rich space for execution plans similar to relational\noperators. We implement our operators and several optimizations for them in\nLOTUS, an open-source query engine with a Pandas-like API.\n  We demonstrate LOTUS' effectiveness across a series of real applications,\nincluding fact-checking, extreme multi-label classification, and search. We\nfind that LOTUS' programming model is highly expressive, capturing\nstate-of-the-art query pipelines with low development overhead. Specifically,\non the FEVER dataset, LOTUS' programs can reproduce FacTool, a recent\nstate-of-the-art fact-checking pipeline, in few lines of code, and implement a\nnew pipeline that improves accuracy by $9.5\\%$, while offering $7-34\\times$\nlower execution time. In the extreme multi-label classification task on the\nBioDEX dataset, LOTUS reproduces state-of-the art result quality with its join\noperator, while providing an efficient algorithm that runs $800\\times$ faster\nthan a naive join. In the search and ranking application, LOTUS allows a simple\ncomposition of operators to achieve $5.9 - 49.4\\%$ higher nDCG@10 than the\nvanilla retriever and re-ranker, while also providing query efficiency, with\n$1.67 - 10\\times$ lower execution time than LM-based ranking methods used by\nprior works. LOTUS is publicly available at\nhttps://github.com/stanford-futuredata/lotus.",
    "authors": [
      "Liana Patel",
      "Siddharth Jha",
      "Carlos Guestrin",
      "Matei Zaharia"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11418v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "DreamCatalyst: Fast and High-Quality 3D Editing via Controlling Editability and Identity Preservation",
    "abstract": "Score distillation sampling (SDS) has emerged as an effective framework in\ntext-driven 3D editing tasks due to its inherent 3D consistency. However,\nexisting SDS-based 3D editing methods suffer from extensive training time and\nlead to low-quality results, primarily because these methods deviate from the\nsampling dynamics of diffusion models. In this paper, we propose DreamCatalyst,\na novel framework that interprets SDS-based editing as a diffusion reverse\nprocess. Our objective function considers the sampling dynamics, thereby making\nthe optimization process of DreamCatalyst an approximation of the diffusion\nreverse process in editing tasks. DreamCatalyst aims to reduce training time\nand improve editing quality. DreamCatalyst presents two modes: (1) a faster\nmode, which edits the NeRF scene in only about 25 minutes, and (2) a\nhigh-quality mode, which produces superior results in less than 70 minutes.\nSpecifically, our high-quality mode outperforms current state-of-the-art NeRF\nediting methods both in terms of speed and quality. See more extensive results\non our project page: https://dream-catalyst.github.io.",
    "authors": [
      "Jiwook Kim",
      "Seonho Lee",
      "Jaeyo Shin",
      "Jiho Choi",
      "Hyunjung Shim"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11394v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "CIC-BART-SSA: Controllable Image Captioning with Structured Semantic Augmentation",
    "abstract": "Controllable Image Captioning (CIC) aims at generating natural language\ndescriptions for an image, conditioned on information provided by end users,\ne.g., regions, entities or events of interest. However, available\nimage--language datasets mainly contain captions that describe the entirety of\nan image, making them ineffective for training CIC models that can potentially\nattend to any subset of regions or relationships. To tackle this challenge, we\npropose a novel, fully automatic method to sample additional focused and\nvisually grounded captions using a unified structured semantic representation\nbuilt on top of the existing set of captions associated with an image. We\nleverage Abstract Meaning Representation (AMR), a cross-lingual graph-based\nsemantic formalism, to encode all possible spatio-semantic relations between\nentities, beyond the typical spatial-relations-only focus of current methods.\nWe use this Structured Semantic Augmentation (SSA) framework to augment\nexisting image--caption datasets with the grounded controlled captions,\nincreasing their spatial and semantic diversity and focal coverage. We then\ndevelop a new model, CIC-BART-SSA, specifically tailored for the CIC task, that\nsources its control signals from SSA-diversified datasets. We empirically show\nthat, compared to SOTA CIC models, CIC-BART-SSA generates captions that are\nsuperior in diversity and text quality, are competitive in controllability,\nand, importantly, minimize the gap between broad and highly focused controlled\ncaptioning performance by efficiently generalizing to the challenging highly\nfocused scenarios. Code is available at\nhttps://github.com/SamsungLabs/CIC-BART-SSA.",
    "authors": [
      "Kalliopi Basioti",
      "Mohamed A. Abdelsalam",
      "Federico Fancellu",
      "Vladimir Pavlovic",
      "Afsaneh Fazly"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11393v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "TM-PATHVQA:90000+ Textless Multilingual Questions for Medical Visual Question Answering",
    "abstract": "In healthcare and medical diagnostics, Visual Question Answering (VQA)\nmayemergeasapivotal tool in scenarios where analysis of intricate medical\nimages becomes critical for accurate diagnoses. Current text-based VQA systems\nlimit their utility in scenarios where hands-free interaction and accessibility\nare crucial while performing tasks. A speech-based VQA system may provide a\nbetter means of interaction where information can be accessed while performing\ntasks simultaneously. To this end, this work implements a speech-based VQA\nsystem by introducing a Textless Multilingual Pathological VQA (TMPathVQA)\ndataset, an expansion of the PathVQA dataset, containing spoken questions in\nEnglish, German & French. This dataset comprises 98,397 multilingual spoken\nquestions and answers based on 5,004 pathological images along with 70 hours of\naudio. Finally, this work benchmarks and compares TMPathVQA systems implemented\nusing various combinations of acoustic and visual features.",
    "authors": [
      "Tonmoy Rajkhowa",
      "Amartya Roy Chowdhury",
      "Sankalp Nagaonkar",
      "Achyut Mani Tripathi"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11383v1",
    "category": "Speech Recognition"
  },
  {
    "title": "Segment, Lift and Fit: Automatic 3D Shape Labeling from 2D Prompts",
    "abstract": "This paper proposes an algorithm for automatically labeling 3D objects from\n2D point or box prompts, especially focusing on applications in autonomous\ndriving. Unlike previous arts, our auto-labeler predicts 3D shapes instead of\nbounding boxes and does not require training on a specific dataset. We propose\na Segment, Lift, and Fit (SLF) paradigm to achieve this goal. Firstly, we\nsegment high-quality instance masks from the prompts using the Segment Anything\nModel (SAM) and transform the remaining problem into predicting 3D shapes from\ngiven 2D masks. Due to the ill-posed nature of this problem, it presents a\nsignificant challenge as multiple 3D shapes can project into an identical mask.\nTo tackle this issue, we then lift 2D masks to 3D forms and employ gradient\ndescent to adjust their poses and shapes until the projections fit the masks\nand the surfaces conform to surrounding LiDAR points. Notably, since we do not\ntrain on a specific dataset, the SLF auto-labeler does not overfit to biased\nannotation patterns in the training set as other methods do. Thus, the\ngeneralization ability across different datasets improves. Experimental results\non the KITTI dataset demonstrate that the SLF auto-labeler produces\nhigh-quality bounding box annotations, achieving an AP@0.5 IoU of nearly 90\\%.\nDetectors trained with the generated pseudo-labels perform nearly as well as\nthose trained with actual ground-truth annotations. Furthermore, the SLF\nauto-labeler shows promising results in detailed shape predictions, providing a\npotential alternative for the occupancy annotation of dynamic objects.",
    "authors": [
      "Jianhao Li",
      "Tianyu Sun",
      "Zhongdao Wang",
      "Enze Xie",
      "Bailan Feng",
      "Hongbo Zhang",
      "Ze Yuan",
      "Ke Xu",
      "Jiaheng Liu",
      "Ping Luo"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11382v1",
    "category": "Robotics"
  },
  {
    "title": "Reliable Reasoning Beyond Natural Language",
    "abstract": "Despite their linguistic competence, Large Language models (LLMs) often\nexhibit limitations in their ability to reason reliably and flexibly. To\naddress this, we propose a neurosymbolic approach that prompts LLMs to extract\nand encode all relevant information from a problem statement as logical code\nstatements, and then use a logic programming language (Prolog) to conduct the\niterative computations of explicit deductive reasoning. Our approach\nsignificantly enhances the performance of LLMs on the standard mathematical\nreasoning benchmark, GSM8k, and the Navigate dataset from the BIG-bench\ndataset. Additionally, we introduce a novel dataset, the Non-Linear Reasoning\n(NLR) dataset, consisting of 55 unique word problems that target the\nshortcomings of the next token prediction paradigm of LLMs and require complex\nnon-linear reasoning but only basic arithmetic skills to solve. Our findings\ndemonstrate that the integration of Prolog enables LLMs to achieve high\nperformance on the NLR dataset, which even the most advanced language models\n(including GPT4) fail to solve using text only.",
    "authors": [
      "Nasim Borazjanizadeh",
      "Steven T. Piantadosi"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11373v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "Thorns and Algorithms: Navigating Generative AI Challenges Inspired by Giraffes and Acacias",
    "abstract": "The interplay between humans and Generative AI (Gen AI) draws an insightful\nparallel with the dynamic relationship between giraffes and acacias on the\nAfrican Savannah. Just as giraffes navigate the acacia's thorny defenses to\ngain nourishment, humans engage with Gen AI, maneuvering through ethical and\noperational challenges to harness its benefits. This paper explores how, like\nyoung giraffes that are still mastering their environment, humans are in the\nearly stages of adapting to and shaping Gen AI. It delves into the strategies\nhumans are developing and refining to help mitigate risks such as bias,\nmisinformation, and privacy breaches, that influence and shape Gen AI's\nevolution. While the giraffe-acacia analogy aptly frames human-AI relations, it\ncontrasts nature's evolutionary perfection with the inherent flaws of\nhuman-made technology and the tendency of humans to misuse it, giving rise to\nmany ethical dilemmas. Through the HHH framework we identify pathways to embed\nvalues of helpfulness, honesty, and harmlessness in AI development, fostering\nsafety-aligned agents that resonate with human values. This narrative presents\na cautiously optimistic view of human resilience and adaptability, illustrating\nour capacity to harness technologies and implement safeguards effectively,\nwithout succumbing to their perils. It emphasises a symbiotic relationship\nwhere humans and AI continually shape each other for mutual benefit.",
    "authors": [
      "Waqar Hussain"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11360v1",
    "category": "Robotics"
  },
  {
    "title": "Feature Inference Attack on Shapley Values",
    "abstract": "As a solution concept in cooperative game theory, Shapley value is highly\nrecognized in model interpretability studies and widely adopted by the leading\nMachine Learning as a Service (MLaaS) providers, such as Google, Microsoft, and\nIBM. However, as the Shapley value-based model interpretability methods have\nbeen thoroughly studied, few researchers consider the privacy risks incurred by\nShapley values, despite that interpretability and privacy are two foundations\nof machine learning (ML) models.\n  In this paper, we investigate the privacy risks of Shapley value-based model\ninterpretability methods using feature inference attacks: reconstructing the\nprivate model inputs based on their Shapley value explanations. Specifically,\nwe present two adversaries. The first adversary can reconstruct the private\ninputs by training an attack model based on an auxiliary dataset and black-box\naccess to the model interpretability services. The second adversary, even\nwithout any background knowledge, can successfully reconstruct most of the\nprivate features by exploiting the local linear correlations between the model\ninputs and outputs. We perform the proposed attacks on the leading MLaaS\nplatforms, i.e., Google Cloud, Microsoft Azure, and IBM aix360. The\nexperimental results demonstrate the vulnerability of the state-of-the-art\nShapley value-based model interpretability methods used in the leading MLaaS\nplatforms and highlight the significance and necessity of designing\nprivacy-preserving model interpretability methods in future studies. To our\nbest knowledge, this is also the first work that investigates the privacy risks\nof Shapley values.",
    "authors": [
      "Xinjian Luo",
      "Yangfan Jiang",
      "Xiaokui Xiao"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11359v1",
    "category": "Machine Learning"
  },
  {
    "title": "SES: Bridging the Gap Between Explainability and Prediction of Graph Neural Networks",
    "abstract": "Despite the Graph Neural Networks' (GNNs) proficiency in analyzing graph\ndata, achieving high-accuracy and interpretable predictions remains\nchallenging. Existing GNN interpreters typically provide post-hoc explanations\ndisjointed from GNNs' predictions, resulting in misrepresentations.\nSelf-explainable GNNs offer built-in explanations during the training process.\nHowever, they cannot exploit the explanatory outcomes to augment prediction\nperformance, and they fail to provide high-quality explanations of node\nfeatures and require additional processes to generate explainable subgraphs,\nwhich is costly. To address the aforementioned limitations, we propose a\nself-explained and self-supervised graph neural network (SES) to bridge the gap\nbetween explainability and prediction. SES comprises two processes: explainable\ntraining and enhanced predictive learning. During explainable training, SES\nemploys a global mask generator co-trained with a graph encoder and directly\nproduces crucial structure and feature masks, reducing time consumption and\nproviding node feature and subgraph explanations. In the enhanced predictive\nlearning phase, mask-based positive-negative pairs are constructed utilizing\nthe explanations to compute a triplet loss and enhance the node representations\nby contrastive learning.",
    "authors": [
      "Zhenhua Huang",
      "Kunhao Li",
      "Shaojie Wang",
      "Zhaohong Jia",
      "Wentao Zhu",
      "Sharad Mehrotra"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11358v1",
    "category": "Machine Learning"
  },
  {
    "title": "COMET: \"Cone of experience\" enhanced large multimodal model for mathematical problem generation",
    "abstract": "The automatic generation of high-quality mathematical problems is practically\nvaluable in many educational scenarios. Large multimodal model provides a novel\ntechnical approach for the mathematical problem generation because of its wide\nsuccess in cross-modal data scenarios. However, the traditional method of\nseparating problem solving from problem generation and the mainstream\nfine-tuning framework of monotonous data structure with homogeneous training\nobjectives limit the application of large multimodal model in mathematical\nproblem generation. Addressing these challenges, this paper proposes COMET, a\n\"Cone of Experience\" enhanced large multimodal model for mathematical problem\ngeneration. Firstly, from the perspective of mutual ability promotion and\napplication logic, we unify stem generation and problem solving into\nmathematical problem generation. Secondly, a three-stage fine-turning framework\nguided by the \"Cone of Experience\" is proposed. The framework divides the\nfine-tuning data into symbolic experience, iconic experience, and direct\nexperience to draw parallels with experiences in the career growth of teachers.\nSeveral fine-grained data construction and injection methods are designed in\nthis framework. Finally, we construct a Chinese multimodal mathematical problem\ndataset to fill the vacancy of Chinese multimodal data in this field. Combined\nwith objective and subjective indicators, experiments on multiple datasets\nfully verify the effectiveness of the proposed framework and model.",
    "authors": [
      "Sannyuya Liu",
      "Jintian Feng",
      "Zongkai Yang",
      "Yawei Luo",
      "Qian Wan",
      "Xiaoxuan Shen",
      "Jianwen Sun"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11315v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "Large Vision-Language Models as Emotion Recognizers in Context Awareness",
    "abstract": "Context-aware emotion recognition (CAER) is a complex and significant task\nthat requires perceiving emotions from various contextual cues. Previous\napproaches primarily focus on designing sophisticated architectures to extract\nemotional cues from images. However, their knowledge is confined to specific\ntraining datasets and may reflect the subjective emotional biases of the\nannotators. Furthermore, acquiring large amounts of labeled data is often\nchallenging in real-world applications. In this paper, we systematically\nexplore the potential of leveraging Large Vision-Language Models (LVLMs) to\nempower the CAER task from three paradigms: 1) We fine-tune LVLMs on two CAER\ndatasets, which is the most common way to transfer large models to downstream\ntasks. 2) We design zero-shot and few-shot patterns to evaluate the performance\nof LVLMs in scenarios with limited data or even completely unseen. In this\ncase, a training-free framework is proposed to fully exploit the In-Context\nLearning (ICL) capabilities of LVLMs. Specifically, we develop an image\nsimilarity-based ranking algorithm to retrieve examples; subsequently, the\ninstructions, retrieved examples, and the test example are combined to feed\nLVLMs to obtain the corresponding sentiment judgment. 3) To leverage the rich\nknowledge base of LVLMs, we incorporate Chain-of-Thought (CoT) into our\nframework to enhance the model's reasoning ability and provide interpretable\nresults. Extensive experiments and analyses demonstrate that LVLMs achieve\ncompetitive performance in the CAER task across different paradigms. Notably,\nthe superior performance in few-shot settings indicates the feasibility of\nLVLMs for accomplishing specific tasks without extensive training.",
    "authors": [
      "Yuxuan Lei",
      "Dingkang Yang",
      "Zhaoyu Chen",
      "Jiawei Chen",
      "Peng Zhai",
      "Lihua Zhang"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11300v1",
    "category": "Natural Language Processing"
  },
  {
    "title": "Zero-Shot Adaptation for Approximate Posterior Sampling of Diffusion Models in Inverse Problems",
    "abstract": "Diffusion models have emerged as powerful generative techniques for solving\ninverse problems. Despite their success in a variety of inverse problems in\nimaging, these models require many steps to converge, leading to slow inference\ntime. Recently, there has been a trend in diffusion models for employing\nsophisticated noise schedules that involve more frequent iterations of\ntimesteps at lower noise levels, thereby improving image generation and\nconvergence speed. However, application of these ideas for solving inverse\nproblems with diffusion models remain challenging, as these noise schedules do\nnot perform well when using empirical tuning for the forward model\nlog-likelihood term weights. To tackle these challenges, we propose zero-shot\napproximate posterior sampling (ZAPS) that leverages connections to zero-shot\nphysics-driven deep learning. ZAPS fixes the number of sampling steps, and uses\nzero-shot training with a physics-guided loss function to learn log-likelihood\nweights at each irregular timestep. We apply ZAPS to the recently proposed\ndiffusion posterior sampling method as baseline, though ZAPS can also be used\nwith other posterior sampling diffusion models. We further approximate the\nHessian of the logarithm of the prior using a diagonalization approach with\nlearnable diagonal entries for computational efficiency. These parameters are\noptimized over a fixed number of epochs with a given computational budget. Our\nresults for various noisy inverse problems, including Gaussian and motion\ndeblurring, inpainting, and super-resolution show that ZAPS reduces inference\ntime, provides robustness to irregular noise schedules and improves\nreconstruction quality. Code is available at https://github.com/ualcalar17/ZAPS",
    "authors": [
      "Ya\u015far Utku Al\u00e7alar",
      "Mehmet Ak\u00e7akaya"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11288v1",
    "category": "Machine Learning"
  },
  {
    "title": "Vibravox: A Dataset of French Speech Captured with Body-conduction Audio Sensors",
    "abstract": "Vibravox is a dataset compliant with the General Data Protection Regulation\n(GDPR) containing audio recordings using five different body-conduction audio\nsensors : two in-ear microphones, two bone conduction vibration pickups and a\nlaryngophone. The data set also includes audio data from an airborne microphone\nused as a reference. The Vibravox corpus contains 38 hours of speech samples\nand physiological sounds recorded by 188 participants under different acoustic\nconditions imposed by an high order ambisonics 3D spatializer. Annotations\nabout the recording conditions and linguistic transcriptions are also included\nin the corpus. We conducted a series of experiments on various speech-related\ntasks, including speech recognition, speech enhancement and speaker\nverification. These experiments were carried out using state-of-the-art models\nto evaluate and compare their performances on signals captured by the different\naudio sensors offered by the Vibravox dataset, with the aim of gaining a better\ngrasp of their individual characteristics.",
    "authors": [
      "Julien Hauret",
      "Malo Olivier",
      "Thomas Joubaud",
      "Christophe Langrenne",
      "Sarah Poir\u00e9e",
      "V\u00e9ronique Zimpfer",
      "\u00c9ric Bavu"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11828v1",
    "category": "Speech Recognition"
  },
  {
    "title": "Statistics-aware Audio-visual Deepfake Detector",
    "abstract": "In this paper, we propose an enhanced audio-visual deep detection method.\nRecent methods in audio-visual deepfake detection mostly assess the\nsynchronization between audio and visual features. Although they have shown\npromising results, they are based on the maximization/minimization of isolated\nfeature distances without considering feature statistics. Moreover, they rely\non cumbersome deep learning architectures and are heavily dependent on\nempirically fixed hyperparameters. Herein, to overcome these limitations, we\npropose: (1) a statistical feature loss to enhance the discrimination\ncapability of the model, instead of relying solely on feature distances; (2)\nusing the waveform for describing the audio as a replacement of frequency-based\nrepresentations; (3) a post-processing normalization of the fakeness score; (4)\nthe use of shallower network for reducing the computational complexity.\nExperiments on the DFDC and FakeAVCeleb datasets demonstrate the relevance of\nthe proposed method.",
    "authors": [
      "Marcella Astrid",
      "Enjie Ghorbel",
      "Djamila Aouada"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11650v1",
    "category": "Computer Vision"
  },
  {
    "title": "Investigating the Effect of Label Topology and Training Criterion on ASR Performance and Alignment Quality",
    "abstract": "The ongoing research scenario for automatic speech recognition (ASR)\nenvisions a clear division between end-to-end approaches and classic modular\nsystems. Even though a high-level comparison between the two approaches in\nterms of their requirements and (dis)advantages is commonly addressed, a closer\ncomparison under similar conditions is not readily available in the literature.\nIn this work, we present a comparison focused on the label topology and\ntraining criterion. We compare two discriminative alignment models with hidden\nMarkov model (HMM) and connectionist temporal classification topology, and two\nfirst-order label context ASR models utilizing factored HMM and strictly\nmonotonic recurrent neural network transducer, respectively. We use different\nmeasurements for the evaluation of the alignment quality, and compare word\nerror rate and real time factor of our best systems. Experiments are conducted\non the LibriSpeech 960h and Switchboard 300h tasks.",
    "authors": [
      "Tina Raissi",
      "Christoph L\u00fcscher",
      "Simon Berger",
      "Ralf Schl\u00fcter",
      "Hermann Ney"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11641v1",
    "category": "Speech Recognition"
  },
  {
    "title": "The VoicePrivacy 2022 Challenge: Progress and Perspectives in Voice Anonymisation",
    "abstract": "The VoicePrivacy Challenge promotes the development of voice anonymisation\nsolutions for speech technology. In this paper we present a systematic overview\nand analysis of the second edition held in 2022. We describe the voice\nanonymisation task and datasets used for system development and evaluation,\npresent the different attack models used for evaluation, and the associated\nobjective and subjective metrics. We describe three anonymisation baselines,\nprovide a summary description of the anonymisation systems developed by\nchallenge participants, and report objective and subjective evaluation results\nfor all. In addition, we describe post-evaluation analyses and a summary of\nrelated work reported in the open literature. Results show that solutions based\non voice conversion better preserve utility, that an alternative which combines\nautomatic speech recognition with synthesis achieves greater privacy, and that\na privacy-utility trade-off remains inherent to current anonymisation\nsolutions. Finally, we present our ideas and priorities for future VoicePrivacy\nChallenge editions.",
    "authors": [
      "Michele Panariello",
      "Natalia Tomashenko",
      "Xin Wang",
      "Xiaoxiao Miao",
      "Pierre Champion",
      "Hubert Nourtel",
      "Massimiliano Todisco",
      "Nicholas Evans",
      "Emmanuel Vincent",
      "Junichi Yamagishi"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11516v1",
    "category": "Speech Recognition"
  },
  {
    "title": "VoxBlink2: A 100K+ Speaker Recognition Corpus and the Open-Set Speaker-Identification Benchmark",
    "abstract": "In this paper, we provide a large audio-visual speaker recognition dataset,\nVoxBlink2, which includes approximately 10M utterances with videos from 110K+\nspeakers in the wild. This dataset represents a significant expansion over the\nVoxBlink dataset, encompassing a broader diversity of speakers and scenarios by\nthe grace of an optimized data collection pipeline. Afterward, we explore the\nimpact of training strategies, data scale, and model complexity on speaker\nverification and finally establish a new single-model state-of-the-art EER at\n0.170% and minDCF at 0.006% on the VoxCeleb1-O test set. Such remarkable\nresults motivate us to explore speaker recognition from a new challenging\nperspective. We raise the Open-Set Speaker-Identification task, which is\ndesigned to either match a probe utterance with a known gallery speaker or\ncategorize it as an unknown query. Associated with this task, we design\nconcrete benchmark and evaluation protocols. The data and model resources can\nbe found in http://voxblink2.github.io.",
    "authors": [
      "Yuke Lin",
      "Ming Cheng",
      "Fulin Zhang",
      "Yingying Gao",
      "Shilei Zhang",
      "Ming Li"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11510v1",
    "category": "Speech Recognition"
  },
  {
    "title": "TM-PATHVQA:90000+ Textless Multilingual Questions for Medical Visual Question Answering",
    "abstract": "In healthcare and medical diagnostics, Visual Question Answering (VQA)\nmayemergeasapivotal tool in scenarios where analysis of intricate medical\nimages becomes critical for accurate diagnoses. Current text-based VQA systems\nlimit their utility in scenarios where hands-free interaction and accessibility\nare crucial while performing tasks. A speech-based VQA system may provide a\nbetter means of interaction where information can be accessed while performing\ntasks simultaneously. To this end, this work implements a speech-based VQA\nsystem by introducing a Textless Multilingual Pathological VQA (TMPathVQA)\ndataset, an expansion of the PathVQA dataset, containing spoken questions in\nEnglish, German & French. This dataset comprises 98,397 multilingual spoken\nquestions and answers based on 5,004 pathological images along with 70 hours of\naudio. Finally, this work benchmarks and compares TMPathVQA systems implemented\nusing various combinations of acoustic and visual features.",
    "authors": [
      "Tonmoy Rajkhowa",
      "Amartya Roy Chowdhury",
      "Sankalp Nagaonkar",
      "Achyut Mani Tripathi"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11383v1",
    "category": "Speech Recognition"
  },
  {
    "title": "Team HYU ASML ROBOVOX SP Cup 2024 System Description",
    "abstract": "This report describes the submission of HYU ASML team to the IEEE Signal\nProcessing Cup 2024 (SP Cup 2024). This challenge, titled \"ROBOVOX: Far-Field\nSpeaker Recognition by a Mobile Robot,\" focuses on speaker recognition using a\nmobile robot in noisy and reverberant conditions. Our solution combines the\nresult of deep residual neural networks and time-delay neural network-based\nspeaker embedding models. These models were trained on a diverse dataset that\nincludes French speech. To account for the challenging evaluation environment\ncharacterized by high noise, reverberation, and short speech conditions, we\nfocused on data augmentation and training speech duration for the speaker\nembedding model. Our submission achieved second place on the SP Cup 2024 public\nleaderboard, with a detection cost function of 0.5245 and an equal error rate\nof 6.46%.",
    "authors": [
      "Jeong-Hwan Choi",
      "Gaeun Kim",
      "Hee-Jae Lee",
      "Seyun Ahn",
      "Hyun-Soo Kim",
      "Joon-Hyuk Chang"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11365v1",
    "category": "Speech Recognition"
  },
  {
    "title": "Beyond Binary: Multiclass Paraphasia Detection with Generative Pretrained Transformers and End-to-End Models",
    "abstract": "Aphasia is a language disorder that can lead to speech errors known as\nparaphasias, which involve the misuse, substitution, or invention of words.\nAutomatic paraphasia detection can help those with Aphasia by facilitating\nclinical assessment and treatment planning options. However, most automatic\nparaphasia detection works have focused solely on binary detection, which\ninvolves recognizing only the presence or absence of a paraphasia. Multiclass\nparaphasia detection represents an unexplored area of research that focuses on\nidentifying multiple types of paraphasias and where they occur in a given\nspeech segment. We present novel approaches that use a generative pretrained\ntransformer (GPT) to identify paraphasias from transcripts as well as two\nend-to-end approaches that focus on modeling both automatic speech recognition\n(ASR) and paraphasia classification as multiple sequences vs. a single\nsequence. We demonstrate that a single sequence model outperforms GPT baselines\nfor multiclass paraphasia detection.",
    "authors": [
      "Matthew Perez",
      "Aneesha Sampath",
      "Minxue Niu",
      "Emily Mower Provost"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11345v1",
    "category": "Speech Recognition"
  },
  {
    "title": "The VoicePrivacy 2022 Challenge: Progress and Perspectives in Voice Anonymisation",
    "abstract": "The VoicePrivacy Challenge promotes the development of voice anonymisation\nsolutions for speech technology. In this paper we present a systematic overview\nand analysis of the second edition held in 2022. We describe the voice\nanonymisation task and datasets used for system development and evaluation,\npresent the different attack models used for evaluation, and the associated\nobjective and subjective metrics. We describe three anonymisation baselines,\nprovide a summary description of the anonymisation systems developed by\nchallenge participants, and report objective and subjective evaluation results\nfor all. In addition, we describe post-evaluation analyses and a summary of\nrelated work reported in the open literature. Results show that solutions based\non voice conversion better preserve utility, that an alternative which combines\nautomatic speech recognition with synthesis achieves greater privacy, and that\na privacy-utility trade-off remains inherent to current anonymisation\nsolutions. Finally, we present our ideas and priorities for future VoicePrivacy\nChallenge editions.",
    "authors": [
      "Michele Panariello",
      "Natalia Tomashenko",
      "Xin Wang",
      "Xiaoxiao Miao",
      "Pierre Champion",
      "Hubert Nourtel",
      "Massimiliano Todisco",
      "Nicholas Evans",
      "Emmanuel Vincent",
      "Junichi Yamagishi"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11516v1",
    "category": "Speech Recognition"
  },
  {
    "title": "Disentangled Acoustic Fields For Multimodal Physical Scene Understanding",
    "abstract": "We study the problem of multimodal physical scene understanding, where an\nembodied agent needs to find fallen objects by inferring object properties,\ndirection, and distance of an impact sound source. Previous works adopt\nfeed-forward neural networks to directly regress the variables from sound,\nleading to poor generalization and domain adaptation issues. In this paper, we\nillustrate that learning a disentangled model of acoustic formation, referred\nto as disentangled acoustic field (DAF), to capture the sound generation and\npropagation process, enables the embodied agent to construct a spatial\nuncertainty map over where the objects may have fallen. We demonstrate that our\nanalysis-by-synthesis framework can jointly infer sound properties by\nexplicitly decomposing and factorizing the latent space of the disentangled\nmodel. We further show that the spatial uncertainty map can significantly\nimprove the success rate for the localization of fallen objects by proposing\nmultiple plausible exploration locations.",
    "authors": [
      "Jie Yin",
      "Andrew Luo",
      "Yilun Du",
      "Anoop Cherian",
      "Tim K. Marks",
      "Jonathan Le Roux",
      "Chuang Gan"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.11333v1",
    "category": "Machine Learning"
  }
]