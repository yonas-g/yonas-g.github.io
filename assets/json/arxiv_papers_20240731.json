[
  {
    "title": "Toward Efficient Permutation for Hierarchical N:M Sparsity on GPUs",
    "abstract": "N:M sparsity pruning is a powerful technique for compressing deep neural\nnetworks, utilizing NVIDIA's Sparse Tensor Core technology. This method\nbenefits from hardware support for sparse indexing, enabling the adoption of\nfine-grained sparsity to maintain model accuracy while minimizing the overhead\ntypically associated with irregular data access. Although restricted to a fixed\nlevel of sparsity due to its reliance on hardware, N:M sparsity can be combined\nwith coarser sparsity techniques to achieve diverse compression ratios.\nInitially, column-wise vector sparsity is applied to a dense model, followed by\nrow-wise N:M sparsity on the preserved column vectors. We call this multi-level\napproach as hierarchical N:M (HiNM) sparsity. Similar to earlier single-level\nsparsity techniques, HiNM sparsity necessitates an effective channel\npermutation strategy to maximize the accuracy of the compressed networks.\nHowever, it introduces further complexities by requiring the rearrangement of\nboth input and output channels, addressing challenges such as permutation\nsequence, HiNM-sparsity-aware permutation, and maintaining consistency in\nchannel ordering across layers. In this paper, we introduce a channel\npermutation method designed specifically for HiNM sparsity, named\ngyro-permutation. This method is crafted to exploit the unique characteristics\nof HiNM pruning, incorporating a strategic policy in each permutation phase,\nincluding channel sampling, clustering, and assignment, to circumvent local\nminima. Additionally, we have developed a GPU kernel that facilitates\nindependent layer permutation during the execution of HiNM sparse networks. Our\nextensive experimental evaluations on various DNN models demonstrate that our\ngyro-permutation significantly enhances the accuracy of HiNM sparse networks,\nallowing them to reach performance levels comparable to those of unstructured\nsparse networks.",
    "authors": [
      "Seungmin Yu",
      "Xiaodie Yi",
      "Hayun Lee",
      "Dongkun Shin"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20496v1",
    "category": [
      "LLMs",
      "Speech Synthesis"
    ]
  },
  {
    "title": "A federated large language model for long-term time series forecasting",
    "abstract": "Long-term time series forecasting in centralized environments poses unique\nchallenges regarding data privacy, communication overhead, and scalability. To\naddress these challenges, we propose FedTime, a federated large language model\n(LLM) tailored for long-range time series prediction. Specifically, we\nintroduce a federated pre-trained LLM with fine-tuning and alignment\nstrategies. Prior to the learning process, we employ K-means clustering to\npartition edge devices or clients into distinct clusters, thereby facilitating\nmore focused model training. We also incorporate channel independence and\npatching to better preserve local semantic information, ensuring that important\ncontextual details are retained while minimizing the risk of information loss.\nWe demonstrate the effectiveness of our FedTime model through extensive\nexperiments on various real-world forecasting benchmarks, showcasing\nsubstantial improvements over recent approaches. In addition, we demonstrate\nthe efficiency of FedTime in streamlining resource usage, resulting in reduced\ncommunication overhead.",
    "authors": [
      "Raed Abdel-Sater",
      "A. Ben Hamza"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20503v1",
    "category": [
      "LLMs",
      "Datasets"
    ]
  },
  {
    "title": "Boosting Efficiency in Task-Agnostic Exploration through Causal Knowledge",
    "abstract": "The effectiveness of model training heavily relies on the quality of\navailable training resources. However, budget constraints often impose\nlimitations on data collection efforts. To tackle this challenge, we introduce\ncausal exploration in this paper, a strategy that leverages the underlying\ncausal knowledge for both data collection and model training. We, in\nparticular, focus on enhancing the sample efficiency and reliability of the\nworld model learning within the domain of task-agnostic reinforcement learning.\nDuring the exploration phase, the agent actively selects actions expected to\nyield causal insights most beneficial for world model training. Concurrently,\nthe causal knowledge is acquired and incrementally refined with the ongoing\ncollection of data. We demonstrate that causal exploration aids in learning\naccurate world models using fewer data and provide theoretical guarantees for\nits convergence. Empirical experiments, on both synthetic data and real-world\napplications, further validate the benefits of causal exploration.",
    "authors": [
      "Yupei Yang",
      "Biwei Huang",
      "Shikui Tu",
      "Lei Xu"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20506v1",
    "category": [
      "Reinforcement Learning",
      "Explainable AI"
    ]
  },
  {
    "title": "Leveraging Multi-facet Paths for Heterogeneous Graph Representation Learning",
    "abstract": "Recent advancements in graph neural networks (GNNs) and heterogeneous GNNs\n(HGNNs) have advanced node embeddings and relationship learning for various\ntasks. However, existing methods often rely on domain-specific predefined\nmeta-paths, which are coarse-grained and focus solely on aspects like node\ntype, limiting their ability to capture complex interactions. We introduce\nMF2Vec, a model that uses multi-faceted (fine-grained) paths instead of\npredefined meta-paths. MF2Vec extracts paths via random walks and generates\nmulti-faceted vectors, ignoring predefined schemas. This method learns diverse\naspects of nodes and their relationships, constructs a homogeneous network, and\ncreates node embeddings for classification, link prediction, and clustering.\nExtensive experiments show that MF2Vec outperforms existing methods, offering a\nmore flexible and comprehensive framework for analyzing complex networks. The\ncode is available at https://anonymous.4open.science/r/MF2Vec-6ABC.",
    "authors": [
      "JongWoo Kim",
      "SeongYeub Chu",
      "HyeongMin Park",
      "Bryan Wong",
      "MunYong Yi"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20648v1",
    "category": [
      "LLMs",
      "Multimodal Learning"
    ]
  },
  {
    "title": "Rethinking the Function of Neurons in KANs",
    "abstract": "The neurons of Kolmogorov-Arnold Networks (KANs) perform a simple summation\nmotivated by the Kolmogorov-Arnold representation theorem, which asserts that\nsum is the only fundamental multivariate function. In this work, we investigate\nthe potential for identifying an alternative multivariate function for KAN\nneurons that may offer increased practical utility. Our empirical research\ninvolves testing various multivariate functions in KAN neurons across a range\nof benchmark Machine Learning tasks.\n  Our findings indicate that substituting the sum with the average function in\nKAN neurons results in significant performance enhancements compared to\ntraditional KANs. Our study demonstrates that this minor modification\ncontributes to the stability of training by confining the input to the spline\nwithin the effective range of the activation function. Our implementation and\nexperiments are available at: \\url{https://github.com/Ghaith81/dropkan}",
    "authors": [
      "Mohammed Ghaith Altarabichi"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20667v1",
    "category": [
      "Reinforcement Learning",
      "Multimodal Learning"
    ]
  },
  {
    "title": "RevGNN: Negative Sampling Enhanced Contrastive Graph Learning for Academic Reviewer Recommendation",
    "abstract": "Acquiring reviewers for academic submissions is a challenging recommendation\nscenario. Recent graph learning-driven models have made remarkable progress in\nthe field of recommendation, but their performance in the academic reviewer\nrecommendation task may suffer from a significant false negative issue. This\narises from the assumption that unobserved edges represent negative samples. In\nfact, the mechanism of anonymous review results in inadequate exposure of\ninteractions between reviewers and submissions, leading to a higher number of\nunobserved interactions compared to those caused by reviewers declining to\nparticipate. Therefore, investigating how to better comprehend the negative\nlabeling of unobserved interactions in academic reviewer recommendations is a\nsignificant challenge. This study aims to tackle the ambiguous nature of\nunobserved interactions in academic reviewer recommendations. Specifically, we\npropose an unsupervised Pseudo Neg-Label strategy to enhance graph contrastive\nlearning (GCL) for recommending reviewers for academic submissions, which we\ncall RevGNN. RevGNN utilizes a two-stage encoder structure that encodes both\nscientific knowledge and behavior using Pseudo Neg-Label to approximate review\npreference. Extensive experiments on three real-world datasets demonstrate that\nRevGNN outperforms all baselines across four metrics. Additionally, detailed\nfurther analyses confirm the effectiveness of each component in RevGNN.",
    "authors": [
      "Weibin Liao",
      "Yifan Zhu",
      "Yanyan Li",
      "Qi Zhang",
      "Zhonghong Ou",
      "Xuesong Li"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20684v1",
    "category": [
      "Datasets",
      "Explainable AI"
    ]
  },
  {
    "title": "Integer-Valued Training and Spike-Driven Inference Spiking Neural Network for High-performance and Energy-efficient Object Detection",
    "abstract": "Brain-inspired Spiking Neural Networks (SNNs) have bio-plausibility and\nlow-power advantages over Artificial Neural Networks (ANNs). Applications of\nSNNs are currently limited to simple classification tasks because of their poor\nperformance. In this work, we focus on bridging the performance gap between\nANNs and SNNs on object detection. Our design revolves around network\narchitecture and spiking neuron. First, the overly complex module design causes\nspike degradation when the YOLO series is converted to the corresponding\nspiking version. We design a SpikeYOLO architecture to solve this problem by\nsimplifying the vanilla YOLO and incorporating meta SNN blocks. Second, object\ndetection is more sensitive to quantization errors in the conversion of\nmembrane potentials into binary spikes by spiking neurons. To address this\nchallenge, we design a new spiking neuron that activates Integer values during\ntraining while maintaining spike-driven by extending virtual timesteps during\ninference. The proposed method is validated on both static and neuromorphic\nobject detection datasets. On the static COCO dataset, we obtain 66.2% mAP@50\nand 48.9% mAP@50:95, which is +15.0% and +18.7% higher than the prior\nstate-of-the-art SNN, respectively. On the neuromorphic Gen1 dataset, we\nachieve 67.2% mAP@50, which is +2.5% greater than the ANN with equivalent\narchitecture, and the energy efficiency is improved by 5.7*. Code:\nhttps://github.com/BICLab/SpikeYOLO",
    "authors": [
      "Xinhao Luo",
      "Man Yao",
      "Yuhong Chou",
      "Bo Xu",
      "Guoqi Li"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20708v2",
    "category": [
      "Multimodal Learning",
      "Reinforcement Learning"
    ]
  },
  {
    "title": "Exploring Loss Landscapes through the Lens of Spin Glass Theory",
    "abstract": "In the past decade, significant strides in deep learning have led to numerous\ngroundbreaking applications. Despite these advancements, the understanding of\nthe high generalizability of deep learning, especially in such an\nover-parametrized space, remains limited. Successful applications are often\nconsidered as empirical rather than scientific achievements. For instance, deep\nneural networks' (DNNs) internal representations, decision-making mechanism,\nabsence of overfitting in an over-parametrized space, high generalizability,\netc., remain less understood. This paper delves into the loss landscape of DNNs\nthrough the lens of spin glass in statistical physics, i.e. a system\ncharacterized by a complex energy landscape with numerous metastable states, to\nbetter understand how DNNs work. We investigated a single hidden layer\nRectified Linear Unit (ReLU) neural network model, and introduced several\nprotocols to examine the analogy between DNNs (trained with datasets including\nMNIST and CIFAR10) and spin glass. Specifically, we used (1) random walk in the\nparameter space of DNNs to unravel the structures in their loss landscape; (2)\na permutation-interpolation protocol to study the connection between copies of\nidentical regions in the loss landscape due to the permutation symmetry in the\nhidden layers; (3) hierarchical clustering to reveal the hierarchy among\ntrained solutions of DNNs, reminiscent of the so-called Replica Symmetry\nBreaking (RSB) phenomenon (i.e. the Parisi solution) in analogy to spin glass;\n(4) finally, we examine the relationship between the degree of the ruggedness\nof the loss landscape of the DNN and its generalizability, showing an\nimprovement of flattened minima.",
    "authors": [
      "Hao Liao",
      "Wei Zhang",
      "Zhanyi Huang",
      "Zexiao Long",
      "Mingyang Zhou",
      "Xiaoqun Wu",
      "Rui Mao",
      "Chi Ho Yeung"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20724v1",
    "category": [
      "Explainable AI",
      "Reinforcement Learning"
    ]
  },
  {
    "title": "Architectural Influence on Variational Quantum Circuits in Multi-Agent Reinforcement Learning: Evolutionary Strategies for Optimization",
    "abstract": "In recent years, Multi-Agent Reinforcement Learning (MARL) has found\napplication in numerous areas of science and industry, such as autonomous\ndriving, telecommunications, and global health. Nevertheless, MARL suffers\nfrom, for instance, an exponential growth of dimensions. Inherent properties of\nquantum mechanics help to overcome these limitations, e.g., by significantly\nreducing the number of trainable parameters. Previous studies have developed an\napproach that uses gradient-free quantum Reinforcement Learning and\nevolutionary optimization for variational quantum circuits (VQCs) to reduce the\ntrainable parameters and avoid barren plateaus as well as vanishing gradients.\nThis leads to a significantly better performance of VQCs compared to classical\nneural networks with a similar number of trainable parameters and a reduction\nin the number of parameters by more than 97 \\% compared to similarly good\nneural networks. We extend an approach of K\\\"olle et al. by proposing a\nGate-Based, a Layer-Based, and a Prototype-Based concept to mutate and\nrecombine VQCs. Our results show the best performance for mutation-only\nstrategies and the Gate-Based approach. In particular, we observe a\nsignificantly better score, higher total and own collected coins, as well as a\nsuperior own coin rate for the best agent when evaluated in the Coin Game\nenvironment.",
    "authors": [
      "Michael K\u00f6lle",
      "Karola Schneider",
      "Sabrina Egger",
      "Felix Topp",
      "Thomy Phan",
      "Philipp Altmann",
      "Jonas N\u00fc\u00dflein",
      "Claudia Linnhoff-Popien"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20739v1",
    "category": [
      "Reinforcement Learning",
      "Explainable AI"
    ]
  },
  {
    "title": "Efficient Quantum One-Class Support Vector Machines for Anomaly Detection Using Randomized Measurements and Variable Subsampling",
    "abstract": "Quantum one-class support vector machines leverage the advantage of quantum\nkernel methods for semi-supervised anomaly detection. However, their quadratic\ntime complexity with respect to data size poses challenges when dealing with\nlarge datasets. In recent work, quantum randomized measurements kernels and\nvariable subsampling were proposed, as two independent methods to address this\nproblem. The former achieves higher average precision, but suffers from\nvariance, while the latter achieves linear complexity to data size and has\nlower variance. The current work focuses instead on combining these two\nmethods, along with rotated feature bagging, to achieve linear time complexity\nboth to data size and to number of features. Despite their instability, the\nresulting models exhibit considerably higher performance and faster training\nand testing times.",
    "authors": [
      "Michael K\u00f6lle",
      "Afrae Ahouzi",
      "Pascal Debus",
      "Elif \u00c7etiner",
      "Robert M\u00fcller",
      "Dani\u00eblle Schuman",
      "Claudia Linnhoff-Popien"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20753v1",
    "category": [
      "Datasets",
      "Reinforcement Learning"
    ]
  },
  {
    "title": "Metaheuristic Enhanced with Feature-Based Guidance and Diversity Management for Solving the Capacitated Vehicle Routing Problem",
    "abstract": "We propose a metaheuristic algorithm enhanced with feature-based guidance\nthat is designed to solve the Capacitated Vehicle Routing Problem (CVRP). To\nformulate the proposed guidance, we developed and explained a supervised\nMachine Learning (ML) model, that is used to formulate the guidance and control\nthe diversity of the solution during the optimization process. We propose a\nmetaheuristic algorithm combining neighborhood search and a novel mechanism of\nhybrid split and path relinking to implement the proposed guidance. The\nproposed guidance has proven to give a statistically significant improvement to\nthe proposed metaheuristic algorithm when solving CVRP. Moreover, the proposed\nguided metaheuristic is also capable of producing competitive solutions among\nstate-of-the-art metaheuristic algorithms.",
    "authors": [
      "Bachtiar Herdianto",
      "Romain Billot",
      "Flavien Lucas",
      "Marc Sevaux"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20777v1",
    "category": [
      "Datasets",
      "Reinforcement Learning"
    ]
  },
  {
    "title": "ARCLE: The Abstraction and Reasoning Corpus Learning Environment for Reinforcement Learning",
    "abstract": "This paper introduces ARCLE, an environment designed to facilitate\nreinforcement learning research on the Abstraction and Reasoning Corpus (ARC).\nAddressing this inductive reasoning benchmark with reinforcement learning\npresents these challenges: a vast action space, a hard-to-reach goal, and a\nvariety of tasks. We demonstrate that an agent with proximal policy\noptimization can learn individual tasks through ARCLE. The adoption of\nnon-factorial policies and auxiliary losses led to performance enhancements,\neffectively mitigating issues associated with action spaces and goal\nattainment. Based on these insights, we propose several research directions and\nmotivations for using ARCLE, including MAML, GFlowNets, and World Models.",
    "authors": [
      "Hosung Lee",
      "Sejin Kim",
      "Seungpil Lee",
      "Sanha Hwang",
      "Jihwan Lee",
      "Byung-Jun Lee",
      "Sundong Kim"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20806v1",
    "category": [
      "Reinforcement Learning",
      "Explainable AI"
    ]
  },
  {
    "title": "Adding Circumscription to Decidable Fragments of First-Order Logic: A Complexity Rollercoaster",
    "abstract": "We study extensions of expressive decidable fragments of first-order logic\nwith circumscription, in particular the two-variable fragment FO$^2$, its\nextension C$^2$ with counting quantifiers, and the guarded fragment GF. We\nprove that if only unary predicates are minimized (or fixed) during\ncircumscription, then decidability of logical consequence is preserved. For\nFO$^2$ the complexity increases from $\\textrm{coNexp}$ to\n$\\textrm{coNExp}^\\textrm{NP}$-complete, for GF it (remarkably!) increases from\n$\\textrm{2Exp}$ to $\\textrm{Tower}$-complete, and for C$^2$ the complexity\nremains open. We also consider querying circumscribed knowledge bases whose\nontology is a GF sentence, showing that the problem is decidable for unions of\nconjunctive queries, $\\textrm{Tower}$-complete in combined complexity, and\nelementary in data complexity. Already for atomic queries and ontologies that\nare sets of guarded existential rules, however, for every $k \\geq 0$ there is\nan ontology and query that are $k$-$\\textrm{Exp}$-hard in data complexity.",
    "authors": [
      "Carsten Lutz",
      "Quentin Mani\u00e8re"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20822v1",
    "category": [
      "Explainable AI",
      "Datasets"
    ]
  },
  {
    "title": "Learn by Selling: Equipping Large Language Models with Product Knowledge for Context-Driven Recommendations",
    "abstract": "The rapid evolution of large language models (LLMs) has opened up new\npossibilities for applications such as context-driven product recommendations.\nHowever, the effectiveness of these models in this context is heavily reliant\non their comprehensive understanding of the product inventory. This paper\npresents a novel approach to equipping LLMs with product knowledge by training\nthem to respond contextually to synthetic search queries that include product\nIDs. We delve into an extensive analysis of this method, evaluating its\neffectiveness, outlining its benefits, and highlighting its constraints. The\npaper also discusses the potential improvements and future directions for this\napproach, providing a comprehensive understanding of the role of LLMs in\nproduct recommendations.",
    "authors": [
      "Sarthak Anand",
      "Yutong Jiang",
      "Giorgi Kokaia"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20856v1",
    "category": [
      "LLMs",
      "Speech Recognition"
    ]
  },
  {
    "title": "A Scalable Tool For Analyzing Genomic Variants Of Humans Using Knowledge Graphs and Machine Learning",
    "abstract": "The integration of knowledge graphs and graph machine learning (GML) in\ngenomic data analysis offers several opportunities for understanding complex\ngenetic relationships, especially at the RNA level. We present a comprehensive\napproach for leveraging these technologies to analyze genomic variants,\nspecifically in the context of RNA sequencing (RNA-seq) data from COVID-19\npatient samples. The proposed method involves extracting variant-level genetic\ninformation, annotating the data with additional metadata using SnpEff, and\nconverting the enriched Variant Call Format (VCF) files into Resource\nDescription Framework (RDF) triples. The resulting knowledge graph is further\nenhanced with patient metadata and stored in a graph database, facilitating\nefficient querying and indexing. We utilize the Deep Graph Library (DGL) to\nperform graph machine learning tasks, including node classification with\nGraphSAGE and Graph Convolutional Networks (GCNs). Our approach demonstrates\nsignificant utility using our proposed tool, VariantKG, in three key scenarios:\nenriching graphs with new VCF data, creating subgraphs based on user-defined\nfeatures, and conducting graph machine learning for node classification.",
    "authors": [
      "Shivika Prasanna",
      "Ajay Kumar",
      "Deepthi Rao",
      "Eduardo Simoes",
      "Praveen Rao"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20879v1",
    "category": [
      "AI in Healthcare",
      "Datasets"
    ]
  },
  {
    "title": "MambaCapsule: Towards Transparent Cardiac Disease Diagnosis with Electrocardiography Using Mamba Capsule Network",
    "abstract": "Cardiac arrhythmia, a condition characterized by irregular heartbeats, often\nserves as an early indication of various heart ailments. With the advent of\ndeep learning, numerous innovative models have been introduced for diagnosing\narrhythmias using Electrocardiogram (ECG) signals. However, recent studies\nsolely focus on the performance of models, neglecting the interpretation of\ntheir results. This leads to a considerable lack of transparency, posing a\nsignificant risk in the actual diagnostic process. To solve this problem, this\npaper introduces MambaCapsule, a deep neural networks for ECG arrhythmias\nclassification, which increases the explainability of the model while enhancing\nthe accuracy.Our model utilizes Mamba for feature extraction and Capsule\nnetworks for prediction, providing not only a confidence score but also signal\nfeatures. Akin to the processing mechanism of human brain, the model learns\nsignal features and their relationship between them by reconstructing ECG\nsignals in the predicted selection. The model evaluation was conducted on\nMIT-BIH and PTB dataset, following the AAMI standard. MambaCapsule has achieved\na total accuracy of 99.54% and 99.59% on the test sets respectively. These\nresults demonstrate the promising performance of under the standard test\nprotocol.",
    "authors": [
      "Yinlong Xu",
      "Xiaoqiang Liu",
      "Zitai Kong",
      "Yixuan Wu",
      "Yue Wang",
      "Yingzhou Lu",
      "Honghao Gao",
      "Jian Wu",
      "Hongxia Xu"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20893v1",
    "category": [
      "AI in Healthcare",
      "Explainable AI"
    ]
  },
  {
    "title": "The Realizability of Revision and Contraction Operators in Epistemic Spaces",
    "abstract": "This paper studies the realizability of belief revision and belief\ncontraction operators in epistemic spaces. We observe that AGM revision and AGM\ncontraction operators for epistemic spaces are only realizable in precisely\ndetermined epistemic spaces. We define the class of linear change operators, a\nspecial kind of maxichoice operator. When AGM revision, respectively, AGM\ncontraction, is realizable, linear change operators are a canonical\nrealization.",
    "authors": [
      "Kai Sauerwald",
      "Matthias Thimm"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20918v1",
    "category": [
      "Explainable AI",
      "Multimodal Learning"
    ]
  },
  {
    "title": "Emotion-driven Piano Music Generation via Two-stage Disentanglement and Functional Representation",
    "abstract": "Managing the emotional aspect remains a challenge in automatic music\ngeneration. Prior works aim to learn various emotions at once, leading to\ninadequate modeling. This paper explores the disentanglement of emotions in\npiano performance generation through a two-stage framework. The first stage\nfocuses on valence modeling of lead sheet, and the second stage addresses\narousal modeling by introducing performance-level attributes. To further\ncapture features that shape valence, an aspect less explored by previous\napproaches, we introduce a novel functional representation of symbolic music.\nThis representation aims to capture the emotional impact of major-minor\ntonality, as well as the interactions among notes, chords, and key signatures.\nObjective and subjective experiments validate the effectiveness of our\nframework in both emotional valence and arousal modeling. We further leverage\nour framework in a novel application of emotional controls, showing a broad\npotential in emotion-driven music generation.",
    "authors": [
      "Jingyue Huang",
      "Ke Chen",
      "Yi-Hsuan Yang"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20955v1",
    "category": [
      "Speech Synthesis",
      "Explainable AI"
    ]
  },
  {
    "title": "An Effective Dynamic Gradient Calibration Method for Continual Learning",
    "abstract": "Continual learning (CL) is a fundamental topic in machine learning, where the\ngoal is to train a model with continuously incoming data and tasks. Due to the\nmemory limit, we cannot store all the historical data, and therefore confront\nthe ``catastrophic forgetting'' problem, i.e., the performance on the previous\ntasks can substantially decrease because of the missing information in the\nlatter period. Though a number of elegant methods have been proposed, the\ncatastrophic forgetting phenomenon still cannot be well avoided in practice. In\nthis paper, we study the problem from the gradient perspective, where our aim\nis to develop an effective algorithm to calibrate the gradient in each updating\nstep of the model; namely, our goal is to guide the model to be updated in the\nright direction under the situation that a large amount of historical data are\nunavailable. Our idea is partly inspired by the seminal stochastic variance\nreduction methods (e.g., SVRG and SAGA) for reducing the variance of gradient\nestimation in stochastic gradient descent algorithms. Another benefit is that\nour approach can be used as a general tool, which is able to be incorporated\nwith several existing popular CL methods to achieve better performance. We also\nconduct a set of experiments on several benchmark datasets to evaluate the\nperformance in practice.",
    "authors": [
      "Weichen Lin",
      "Jiaxiang Chen",
      "Ruomin Huang",
      "Hu Ding"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20956v1",
    "category": [
      "Reinforcement Learning",
      "Datasets"
    ]
  },
  {
    "title": "MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning",
    "abstract": "Recently, large language models (LLMs) have demonstrated remarkable\ncapabilities in a wide range of tasks. Typically, an LLM is pre-trained on\nlarge corpora and subsequently fine-tuned on task-specific datasets. However,\nduring fine-tuning, LLMs may forget the knowledge acquired in the pre-training\nstage, leading to a decline in general capabilities. To address this issue, we\npropose a new fine-tuning algorithm termed Momentum-Filtered Optimizer (MoFO).\nThe key idea of MoFO is to iteratively select and update the model parameters\nwith the largest momentum magnitudes. Compared to full-parameter training, MoFO\nachieves similar fine-tuning performance while keeping parameters closer to the\npre-trained model, thereby mitigating knowledge forgetting. Unlike most\nexisting methods for forgetting mitigation, MoFO combines the following two\nadvantages. First, MoFO does not require access to pre-training data. This\nmakes MoFO particularly suitable for fine-tuning scenarios where pre-training\ndata is unavailable, such as fine-tuning checkpoint-only open-source LLMs.\nSecond, MoFO does not alter the original loss function. This could avoid\nimpairing the model performance on the fine-tuning tasks. We validate MoFO\nthrough rigorous convergence analysis and extensive experiments, demonstrating\nits superiority over existing methods in mitigating forgetting and enhancing\nfine-tuning performance.",
    "authors": [
      "Yupeng Chen",
      "Senmiao Wang",
      "Zhihang Lin",
      "Zeyu Qin",
      "Yushun Zhang",
      "Tian Ding",
      "Ruoyu Sun"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.20999v2",
    "category": [
      "LLMs",
      "Speech Recognition"
    ]
  },
  {
    "title": "AI-Assisted Generation of Difficult Math Questions",
    "abstract": "Current LLM training positions mathematical reasoning as a core capability.\nWith publicly available sources fully tapped, there is unmet demand for diverse\nand challenging math questions. Relying solely on human experts is both\ntime-consuming and costly, while LLM-generated questions often lack the\nrequisite diversity and difficulty. We present a design framework that combines\nthe strengths of LLMs with a human-in-the-loop approach to generate a diverse\narray of challenging math questions. We leverage LLM metacognition skills\n[Didolkar et al., 2024] of a strong LLM to extract core \"skills\" from existing\nmath datasets. These skills serve as the basis for generating novel and\ndifficult questions by prompting the LLM with random pairs of core skills. The\nuse of two different skills within each question makes finding such questions\nan \"out of distribution\" task for both LLMs and humans. Our pipeline employs\nLLMs to iteratively generate and refine questions and solutions through\nmultiturn prompting. Human annotators then verify and further refine the\nquestions, with their efficiency enhanced via further LLM interactions.\nApplying this pipeline on skills extracted from the MATH dataset [Hendrycks et\nal., 2021] resulted in MATH$^2$ - a dataset of higher-quality math questions,\nas evidenced by: (a) Lower performance of all models on MATH$^2$ than on MATH\n(b) Higher performance on MATH when using MATH$^2$ questions as in-context\nexamples. Although focused on mathematics, our methodology seems applicable to\nother domains requiring structured reasoning, and potentially as a component of\nscalable oversight. Also of interest is a striking relationship observed\nbetween models' performance on the new dataset: the success rate on MATH$^2$ is\nthe square on MATH, suggesting that successfully solving the question in\nMATH$^2$ requires a nontrivial combination of two distinct math skills.",
    "authors": [
      "Vedant Shah",
      "Dingli Yu",
      "Kaifeng Lyu",
      "Simon Park",
      "Nan Rosemary Ke",
      "Michael Mozer",
      "Yoshua Bengio",
      "Sanjeev Arora",
      "Anirudh Goyal"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21009v1",
    "category": [
      "Explainable AI",
      "Datasets"
    ]
  },
  {
    "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
    "abstract": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
    "authors": [
      "Chi-Chih Chang",
      "Wei-Cheng Lin",
      "Chien-Yu Lin",
      "Chong-Yan Chen",
      "Yu-Fang Hu",
      "Pei-Shuo Wang",
      "Ning-Chi Huang",
      "Luis Ceze",
      "Kai-Chiang Wu"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21118v1",
    "category": [
      "LLMs",
      "Speech Synthesis"
    ]
  },
  {
    "title": "LoRaWAN Based Dynamic Noise Mapping with Machine Learning for Urban Noise Enforcement",
    "abstract": "Static noise maps depicting long-term noise levels over wide areas are\nvaluable urban planning assets for municipalities in decreasing noise exposure\nof residents. However, non-traffic noise sources with transient behavior, which\npeople complain frequently, are usually ignored by static maps. We propose here\na dynamic noise mapping approach using the data collected via low-power\nwide-area network (LPWAN, specifically LoRaWAN) based internet of things (IoT)\ninfrastructure, which is one of the most common communication backbones for\nsmart cities. Noise mapping based on LPWAN is challenging due to the low data\nrates of these protocols. The proposed dynamic noise mapping approach\ndiminishes the negative implications of data rate limitations using machine\nlearning (ML) for event and location prediction of non-traffic sources based on\nthe scarce data. The strength of these models lies in their consideration of\nthe spatial variance in acoustic behavior caused by the buildings in urban\nsettings. The effectiveness of the proposed method and the accuracy of the\nresulting dynamic maps are evaluated in field tests. The results show that the\nproposed system can decrease the map error caused by non-traffic sources up to\n51% and can stay effective under significant packet losses.",
    "authors": [
      "H. Emre Erdem",
      "Henry Leung"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21204v1",
    "category": [
      "Datasets",
      "Speech Recognition"
    ]
  },
  {
    "title": "Bug Analysis Towards Bug Resolution Time Prediction",
    "abstract": "Bugs are inevitable in software development, and their reporting in open\nrepositories can enhance software transparency and reliability assessment. This\nstudy aims to extract information from the issue tracking system Jira and\nproposes a methodology to estimate resolution time for new bugs. The\nmethodology is applied to network project ONAP, addressing concerns of network\noperators and manufacturers. This research provides insights into bug\nresolution times and related aspects in network softwarization projects.",
    "authors": [
      "Hasan Yagiz Ozkan",
      "Poul Einer Heegaard",
      "Wolfgang Kellerer",
      "Carmen Mas-Machuca"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21241v1",
    "category": [
      "Benchmarking",
      "Datasets"
    ]
  },
  {
    "title": "Tractable and Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation",
    "abstract": "Distributional reinforcement learning improves performance by effectively\ncapturing environmental stochasticity, but a comprehensive theoretical\nunderstanding of its effectiveness remains elusive. In this paper, we present a\nregret analysis for distributional reinforcement learning with general value\nfunction approximation in a finite episodic Markov decision process setting. We\nfirst introduce a key notion of Bellman unbiasedness for a tractable and\nexactly learnable update via statistical functional dynamic programming. Our\ntheoretical results show that approximating the infinite-dimensional return\ndistribution with a finite number of moment functionals is the only method to\nlearn the statistical information unbiasedly, including nonlinear statistical\nfunctionals. Second, we propose a provably efficient algorithm,\n$\\texttt{SF-LSVI}$, achieving a regret bound of $\\tilde{O}(d_E\nH^{\\frac{3}{2}}\\sqrt{K})$ where $H$ is the horizon, $K$ is the number of\nepisodes, and $d_E$ is the eluder dimension of a function class.",
    "authors": [
      "Taehyun Cho",
      "Seungyub Han",
      "Kyungjae Lee",
      "Seokhun Ju",
      "Dohyeong Kim",
      "Jungwoo Lee"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21260v1",
    "category": [
      "Reinforcement Learning",
      "Multimodal Learning"
    ]
  },
  {
    "title": "A Vectorization Method Induced By Maximal Margin Classification For Persistent Diagrams",
    "abstract": "Persistent homology is an effective method for extracting topological\ninformation, represented as persistent diagrams, of spatial structure data.\nHence it is well-suited for the study of protein structures. Attempts to\nincorporate Persistent homology in machine learning methods of protein function\nprediction have resulted in several techniques for vectorizing persistent\ndiagrams. However, current vectorization methods are excessively artificial and\ncannot ensure the effective utilization of information or the rationality of\nthe methods. To address this problem, we propose a more geometrical\nvectorization method of persistent diagrams based on maximal margin\nclassification for Banach space, and additionaly propose a framework that\nutilizes topological data analysis to identify proteins with specific\nfunctions. We evaluated our vectorization method using a binary classification\ntask on proteins and compared it with the statistical methods that exhibit the\nbest performance among thirteen commonly used vectorization methods. The\nexperimental results indicate that our approach surpasses the statistical\nmethods in both robustness and precision.",
    "authors": [
      "An Wu",
      "Yu Pan",
      "Fuqi Zhou",
      "Jinghui Yan",
      "Chuanlu Liu"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21298v1",
    "category": [
      "Multimodal Learning",
      "Datasets"
    ]
  },
  {
    "title": "Who should I trust? A Visual Analytics Approach for Comparing Net Load Forecasting Models",
    "abstract": "Net load forecasting is crucial for energy planning and facilitating informed\ndecision-making regarding trade and load distributions. However, evaluating\nforecasting models' performance against benchmark models remains challenging,\nthereby impeding experts' trust in the model's performance. In this context,\nthere is a demand for technological interventions that allow scientists to\ncompare models across various timeframes and solar penetration levels. This\npaper introduces a visual analytics-based application designed to compare the\nperformance of deep-learning-based net load forecasting models with other\nmodels for probabilistic net load forecasting. This application employs\ncarefully selected visual analytic interventions, enabling users to discern\ndifferences in model performance across different solar penetration levels,\ndataset resolutions, and hours of the day over multiple months. We also present\nobservations made using our application through a case study, demonstrating the\neffectiveness of visualizations in aiding scientists in making informed\ndecisions and enhancing trust in net load forecasting models.",
    "authors": [
      "Kaustav Bhattacharjee",
      "Soumya Kundu",
      "Indrasis Chakraborty",
      "Aritra Dasgupta"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21299v1",
    "category": [
      "Explainable AI",
      "Datasets"
    ]
  },
  {
    "title": "Implementing Streaming algorithm and k-means clusters to RAG",
    "abstract": "Retrieval-augmented generation (RAG) has achieved great success in\ninformation retrieval to assist large models because it builds an external\nknowledge database. However, it also has many problems: it consumes a lot of\nmemory because of the huge database. When faced with massive streaming data, it\nis unable to update the established index database in time. To save the memory\nof building the database and maintain accuracy simultaneously, we proposed a\nnew approach combining a streaming algorithm and k-means cluster with RAG. Our\napproach applies a streaming algorithm to update the index and reduce memory\nconsumption. Then use the k-means algorithm to cluster documents with high\nsimilarities together, the query time will be shortened by doing this. We\nconducted comparative experiments on four methods, and the results show that\nRAG with streaming algorithm and k-means cluster performs well in accuracy and\nmemory. For massive streaming data, we find that our method behaves better than\ntraditional RAG",
    "authors": [
      "Haoyu Kang",
      "Yuzhou Zhu",
      "Yukun Zhong",
      "Ke Wang"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21300v1",
    "category": [
      "Datasets",
      "Multimodal Learning"
    ]
  },
  {
    "title": "Big Cooperative Learning",
    "abstract": "Cooperation plays a pivotal role in the evolution of human intelligence;\nmoreover, it also underlies the recent revolutionary advancement of artificial\nintelligence (AI) that is driven by foundation models. Specifically, we reveal\nthat the training of foundation models can be interpreted as a form of big\ncooperative learning (\\textit{abbr.} big learning), where massive learning\nindividuals/tasks \\emph{cooperate} to approach the unique essence of data from\ndiverse perspectives of data prediction, leveraging a universal model. The\npresented big learning therefore unifies most training objectives of foundation\nmodels within a consistent framework, where their underlying assumptions are\nexposed simultaneously. We design tailored simulations to demonstrate the\nprinciple of big learning, based on which we provide learning-perspective\njustifications for the successes of foundation models, with interesting\nside-products. Furthermore, we reveal that big learning is a new dimension for\nupgrading conventional machine learning paradigms, valuable for endowing\nreinvigorations to associated applications; as an illustrative example, we\npropose the BigLearn-GAN, which is a novel adversarially-trained foundation\nmodel with versatile data sampling capabilities. Code is available at\n\\texttt{https://github.com/YulaiCong/BigCooperativeLearning}.",
    "authors": [
      "Yulai Cong"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21319v1",
    "category": [
      "Explainable AI",
      "Reinforcement Learning"
    ]
  },
  {
    "title": "Dual-Constrained Dynamical Neural ODEs for Ambiguity-aware Continuous Emotion Prediction",
    "abstract": "There has been a significant focus on modelling emotion ambiguity in recent\nyears, with advancements made in representing emotions as distributions to\ncapture ambiguity. However, there has been comparatively less effort devoted to\nthe consideration of temporal dependencies in emotion distributions which\nencodes ambiguity in perceived emotions that evolve smoothly over time.\nRecognizing the benefits of using constrained dynamical neural ordinary\ndifferential equations (CD-NODE) to model time series as dynamic processes, we\npropose an ambiguity-aware dual-constrained Neural ODE approach to model the\ndynamics of emotion distributions on arousal and valence. In our approach, we\nutilize ODEs parameterised by neural networks to estimate the distribution\nparameters, and we integrate additional constraints to restrict the range of\nthe system outputs to ensure the validity of predicted distributions. We\nevaluated our proposed system on the publicly available RECOLA dataset and\nobserved very promising performance across a range of evaluation metrics.",
    "authors": [
      "Jingyao Wu",
      "Ting Dang",
      "Vidhyasaharan Sethu",
      "Eliathamby Ambikairajah"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21344v1",
    "category": [
      "Explainable AI",
      "Speech Synthesis"
    ]
  },
  {
    "title": "Differentially Private Block-wise Gradient Shuffle for Deep Learning",
    "abstract": "Traditional Differentially Private Stochastic Gradient Descent (DP-SGD)\nintroduces statistical noise on top of gradients drawn from a Gaussian\ndistribution to ensure privacy. This paper introduces the novel Differentially\nPrivate Block-wise Gradient Shuffle (DP-BloGS) algorithm for deep learning.\nBloGS builds off of existing private deep learning literature, but makes a\ndefinitive shift by taking a probabilistic approach to gradient noise\nintroduction through shuffling modeled after information theoretic privacy\nanalyses. The theoretical results presented in this paper show that the\ncombination of shuffling, parameter-specific block size selection, batch layer\nclipping, and gradient accumulation allows DP-BloGS to achieve training times\nclose to that of non-private training while maintaining similar privacy and\nutility guarantees to DP-SGD. DP-BloGS is found to be significantly more\nresistant to data extraction attempts than DP-SGD. The theoretical results are\nvalidated by the experimental findings.",
    "authors": [
      "David Zagardo"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21347v1",
    "category": [
      "Datasets",
      "Explainable AI"
    ]
  },
  {
    "title": "Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs",
    "abstract": "Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing\nreliable, structured, domain-specific, and up-to-date external knowledge.\nHowever, KGs and LLMs are often developed separately and must be integrated\nafter training. We introduce Tree-of-Traversals, a novel zero-shot reasoning\nalgorithm that enables augmentation of black-box LLMs with one or more KGs. The\nalgorithm equips a LLM with actions for interfacing a KG and enables the LLM to\nperform tree search over possible thoughts and actions to find high confidence\nreasoning paths. We evaluate on two popular benchmark datasets. Our results\nshow that Tree-of-Traversals significantly improves performance on question\nanswering and KG question answering tasks. Code is available at\n\\url{https://github.com/amazon-science/tree-of-traversals}",
    "authors": [
      "Elan Markowitz",
      "Anil Ramakrishna",
      "Jwala Dhamala",
      "Ninareh Mehrabi",
      "Charith Peris",
      "Rahul Gupta",
      "Kai-Wei Chang",
      "Aram Galstyan"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21358v1",
    "category": [
      "LLMs",
      "Explainable AI"
    ]
  },
  {
    "title": "ProSpec RL: Plan Ahead, then Execute",
    "abstract": "Imagining potential outcomes of actions before execution helps agents make\nmore informed decisions, a prospective thinking ability fundamental to human\ncognition. However, mainstream model-free Reinforcement Learning (RL) methods\nlack the ability to proactively envision future scenarios, plan, and guide\nstrategies. These methods typically rely on trial and error to adjust policy\nfunctions, aiming to maximize cumulative rewards or long-term value, even if\nsuch high-reward decisions place the environment in extremely dangerous states.\nTo address this, we propose the Prospective (ProSpec) RL method, which makes\nhigher-value, lower-risk optimal decisions by imagining future n-stream\ntrajectories. Specifically, ProSpec employs a dynamic model to predict future\nstates (termed \"imagined states\") based on the current state and a series of\nsampled actions. Furthermore, we integrate the concept of Model Predictive\nControl and introduce a cycle consistency constraint that allows the agent to\nevaluate and select the optimal actions from these trajectories. Moreover,\nProSpec employs cycle consistency to mitigate two fundamental issues in RL:\naugmenting state reversibility to avoid irreversible events (low risk) and\naugmenting actions to generate numerous virtual trajectories, thereby improving\ndata efficiency. We validated the effectiveness of our method on the DMControl\nbenchmarks, where our approach achieved significant performance improvements.\nCode will be open-sourced upon acceptance.",
    "authors": [
      "Liangliang Liu",
      "Yi Guan",
      "BoRan Wang",
      "Rujia Shen",
      "Yi Lin",
      "Chaoran Kong",
      "Lian Yan",
      "Jingchi Jiang"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21359v1",
    "category": [
      "Reinforcement Learning",
      "Explainable AI"
    ]
  },
  {
    "title": "Deformable 3D Shape Diffusion Model",
    "abstract": "The Gaussian diffusion model, initially designed for image generation, has\nrecently been adapted for 3D point cloud generation. However, these adaptations\nhave not fully considered the intrinsic geometric characteristics of 3D shapes,\nthereby constraining the diffusion model's potential for 3D shape manipulation.\nTo address this limitation, we introduce a novel deformable 3D shape diffusion\nmodel that facilitates comprehensive 3D shape manipulation, including point\ncloud generation, mesh deformation, and facial animation. Our approach\ninnovatively incorporates a differential deformation kernel, which deconstructs\nthe generation of geometric structures into successive non-rigid deformation\nstages. By leveraging a probabilistic diffusion model to simulate this\nstep-by-step process, our method provides a versatile and efficient solution\nfor a wide range of applications, spanning from graphics rendering to facial\nexpression animation. Empirical evidence highlights the effectiveness of our\napproach, demonstrating state-of-the-art performance in point cloud generation\nand competitive results in mesh deformation. Additionally, extensive visual\ndemonstrations reveal the significant potential of our approach for practical\napplications. Our method presents a unique pathway for advancing 3D shape\nmanipulation and unlocking new opportunities in the realm of virtual reality.",
    "authors": [
      "Dengsheng Chen",
      "Jie Hu",
      "Xiaoming Wei",
      "Enhua Wu"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21428v1",
    "category": [
      "Multimodal Learning",
      "Speech Synthesis"
    ]
  },
  {
    "title": "TinyChirp: Bird Song Recognition Using TinyML Models on Low-power Wireless Acoustic Sensors",
    "abstract": "Monitoring biodiversity at scale is challenging. Detecting and identifying\nspecies in fine grained taxonomies requires highly accurate machine learning\n(ML) methods. Training such models requires large high quality data sets. And\ndeploying these models to low power devices requires novel compression\ntechniques and model architectures. While species classification methods have\nprofited from novel data sets and advances in ML methods, in particular neural\nnetworks, deploying these state of the art models to low power devices remains\ndifficult. Here we present a comprehensive empirical comparison of various\ntinyML neural network architectures and compression techniques for species\nclassification. We focus on the example of bird song detection, more concretely\na data set curated for studying the corn bunting bird species. The data set is\nreleased along with all code and experiments of this study. In our experiments\nwe compare predictive performance, memory and time complexity of classical\nspectrogram based methods and recent approaches operating on raw audio signal.\nOur results indicate that individual bird species can be robustly detected with\nrelatively simple architectures that can be readily deployed to low power\ndevices.",
    "authors": [
      "Zhaolan Huang",
      "Adrien Tousnakhoff",
      "Polina Kozyr",
      "Roman Rehausen",
      "Felix Bie\u00dfmann",
      "Robert Lachlan",
      "Cedric Adjih",
      "Emmanuel Baccelli"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21453v1",
    "category": [
      "Datasets",
      "Speech Recognition"
    ]
  },
  {
    "title": "KemenkeuGPT: Leveraging a Large Language Model on Indonesia's Government Financial Data and Regulations to Enhance Decision Making",
    "abstract": "Data is crucial for evidence-based policymaking and enhancing public\nservices, including those at the Ministry of Finance of the Republic of\nIndonesia. However, the complexity and dynamic nature of governmental financial\ndata and regulations can hinder decision-making. This study investigates the\npotential of Large Language Models (LLMs) to address these challenges, focusing\non Indonesia's financial data and regulations. While LLMs are effective in the\nfinancial sector, their use in the public sector in Indonesia is unexplored.\nThis study undertakes an iterative process to develop KemenkeuGPT using the\nLangChain with Retrieval-Augmented Generation (RAG), prompt engineering and\nfine-tuning. The dataset from 2003 to 2023 was collected from the Ministry of\nFinance, Statistics Indonesia and the International Monetary Fund (IMF).\nSurveys and interviews with Ministry officials informed, enhanced and\nfine-tuned the model. We evaluated the model using human feedback, LLM-based\nevaluation and benchmarking. The model's accuracy improved from 35% to 61%,\nwith correctness increasing from 48% to 64%. The Retrieval-Augmented Generation\nAssessment (RAGAS) framework showed that KemenkeuGPT achieved 44% correctness\nwith 73% faithfulness, 40% precision and 60% recall, outperforming several\nother base models. An interview with an expert from the Ministry of Finance\nindicated that KemenkeuGPT has the potential to become an essential tool for\ndecision-making. These results are expected to improve with continuous human\nfeedback.",
    "authors": [
      "Gilang Fajar Febrian",
      "Grazziela Figueredo"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21459v1",
    "category": [
      "Speech Recognition",
      "Speech Synthesis"
    ]
  },
  {
    "title": "eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in RDF-star Knowledge Graphs",
    "abstract": "Over the past few years, we have seen the emergence of large knowledge graphs\ncombining information from multiple sources. Sometimes, this information is\nprovided in the form of assertions about other assertions, defining contexts\nwhere assertions are valid. A recent extension to RDF which admits statements\nover statements, called RDF-star, is in revision to become a W3C standard.\nHowever, there is no proposal for a semantics of these RDF-star statements nor\na built-in facility to operate over them. In this paper, we propose a query\nlanguage for epistemic RDF-star metadata based on a four-valued logic, called\neSPARQL. Our proposed query language extends SPARQL-star, the query language\nfor RDF-star, with a new type of FROM clause to facilitate operating with\nmultiple and sometimes conflicting beliefs. We show that the proposed query\nlanguage can express four use case queries, including the following features:\n(i) querying the belief of an individual, (ii) the aggregating of beliefs,\n(iii) querying who is conflicting with somebody, and (iv) beliefs about beliefs\n(i.e., nesting of beliefs).",
    "authors": [
      "Xiny Pan",
      "Daniel Hern\u00e1ndez",
      "Philipp Seifer",
      "Ralf L\u00e4mmel",
      "Steffen Staab"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21483v2",
    "category": [
      "Datasets",
      "Explainable AI"
    ]
  },
  {
    "title": "Parallel Strategies for Best-First Generalized Planning",
    "abstract": "In recent years, there has been renewed interest in closing the performance\ngap between state-of-the-art planning solvers and generalized planning (GP), a\nresearch area of AI that studies the automated synthesis of algorithmic-like\nsolutions capable of solving multiple classical planning instances. One of the\ncurrent advancements has been the introduction of Best-First Generalized\nPlanning (BFGP), a GP algorithm based on a novel solution space that can be\nexplored with heuristic search, one of the foundations of modern planners. This\npaper evaluates the application of parallel search techniques to BFGP, another\ncritical component in closing the performance gap. We first discuss why BFGP is\nwell suited for parallelization and some of its differentiating characteristics\nfrom classical planners. Then, we propose two simple shared-memory parallel\nstrategies with good scaling with the number of cores.",
    "authors": [
      "Alejandro Fern\u00e1ndez-Alburquerque",
      "Javier Segovia-Aguas"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21485v1",
    "category": [
      "Reinforcement Learning",
      "AI in Healthcare"
    ]
  },
  {
    "title": "Breaking the Hourglass Phenomenon of Residual Quantization: Enhancing the Upper Bound of Generative Retrieval",
    "abstract": "Generative retrieval (GR) has emerged as a transformative paradigm in search\nand recommender systems, leveraging numeric-based identifier representations to\nenhance efficiency and generalization. Notably, methods like TIGER employing\nResidual Quantization-based Semantic Identifiers (RQ-SID), have shown\nsignificant promise in e-commerce scenarios by effectively managing item IDs.\nHowever, a critical issue termed the \"\\textbf{Hourglass}\" phenomenon, occurs in\nRQ-SID, where intermediate codebook tokens become overly concentrated,\nhindering the full utilization of generative retrieval methods. This paper\nanalyses and addresses this problem by identifying data sparsity and\nlong-tailed distribution as the primary causes. Through comprehensive\nexperiments and detailed ablation studies, we analyze the impact of these\nfactors on codebook utilization and data distribution. Our findings reveal that\nthe \"Hourglass\" phenomenon substantially impacts the performance of RQ-SID in\ngenerative retrieval. We propose effective solutions to mitigate this issue,\nthereby significantly enhancing the effectiveness of generative retrieval in\nreal-world E-commerce applications.",
    "authors": [
      "Zhirui Kuai",
      "Zuxu Chen",
      "Huimu Wang",
      "Mingming Li",
      "Dadong Miao",
      "Binbin Wang",
      "Xusong Chen",
      "Li Kuang",
      "Yuxing Han",
      "Jiaxing Wang",
      "Guoyu Tang",
      "Lin Liu",
      "Songlin Wang",
      "Jingwei Zhuo"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21488v1",
    "category": [
      "Datasets",
      "LLMs"
    ]
  },
  {
    "title": "FSSC: Federated Learning of Transformer Neural Networks for Semantic Image Communication",
    "abstract": "In this paper, we address the problem of image semantic communication in a\nmulti-user deployment scenario and propose a federated learning (FL) strategy\nfor a Swin Transformer-based semantic communication system (FSSC). Firstly, we\ndemonstrate that the adoption of a Swin Transformer for joint source-channel\ncoding (JSCC) effectively extracts semantic information in the communication\nsystem. Next, the FL framework is introduced to collaboratively learn a global\nmodel by aggregating local model parameters, rather than directly sharing\nclients' data. This approach enhances user privacy protection and reduces the\nworkload on the server or mobile edge. Simulation evaluations indicate that our\nmethod outperforms the typical JSCC algorithm and traditional separate-based\ncommunication algorithms. Particularly after integrating local semantics, the\nglobal aggregation model has further increased the Peak Signal-to-Noise Ratio\n(PSNR) by more than 2dB, thoroughly proving the effectiveness of our algorithm.",
    "authors": [
      "Yuna Yan",
      "Xin Zhang",
      "Lixin Li",
      "Wensheng Lin",
      "Rui Li",
      "Wenchi Cheng",
      "Zhu Han"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21507v1",
    "category": [
      "Multimodal Learning",
      "Datasets"
    ]
  },
  {
    "title": "Tabular Data Augmentation for Machine Learning: Progress and Prospects of Embracing Generative AI",
    "abstract": "Machine learning (ML) on tabular data is ubiquitous, yet obtaining abundant\nhigh-quality tabular data for model training remains a significant obstacle.\nNumerous works have focused on tabular data augmentation (TDA) to enhance the\noriginal table with additional data, thereby improving downstream ML tasks.\nRecently, there has been a growing interest in leveraging the capabilities of\ngenerative AI for TDA. Therefore, we believe it is time to provide a\ncomprehensive review of the progress and future prospects of TDA, with a\nparticular emphasis on the trending generative AI. Specifically, we present an\narchitectural view of the TDA pipeline, comprising three main procedures:\npre-augmentation, augmentation, and post-augmentation. Pre-augmentation\nencompasses preparation tasks that facilitate subsequent TDA, including error\nhandling, table annotation, table simplification, table representation, table\nindexing, table navigation, schema matching, and entity matching. Augmentation\nsystematically analyzes current TDA methods, categorized into retrieval-based\nmethods, which retrieve external data, and generation-based methods, which\ngenerate synthetic data. We further subdivide these methods based on the\ngranularity of the augmentation process at the row, column, cell, and table\nlevels. Post-augmentation focuses on the datasets, evaluation and optimization\naspects of TDA. We also summarize current trends and future directions for TDA,\nhighlighting promising opportunities in the era of generative AI. In addition,\nthe accompanying papers and related resources are continuously updated and\nmaintained in the GitHub repository at\nhttps://github.com/SuDIS-ZJU/awesome-tabular-data-augmentation to reflect\nongoing advancements in the field.",
    "authors": [
      "Lingxi Cui",
      "Huan Li",
      "Ke Chen",
      "Lidan Shou",
      "Gang Chen"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21523v1",
    "category": [
      "Datasets",
      "Explainable AI"
    ]
  },
  {
    "title": "TRGR: Transmissive RIS-aided Gait Recognition Through Walls",
    "abstract": "Gait recognition with radio frequency (RF) signals enables many potential\napplications requiring accurate identification. However, current systems\nrequire individuals to be within a line-of-sight (LOS) environment and struggle\nwith low signal-to-noise ratio (SNR) when signals traverse concrete and thick\nwalls. To address these challenges, we present TRGR, a novel transmissive\nreconfigurable intelligent surface (RIS)-aided gait recognition system. TRGR\ncan recognize human identities through walls using only the magnitude\nmeasurements of channel state information (CSI) from a pair of transceivers.\nSpecifically, by leveraging transmissive RIS alongside a configuration\nalternating optimization algorithm, TRGR enhances wall penetration and signal\nquality, enabling accurate gait recognition. Furthermore, a residual\nconvolution network (RCNN) is proposed as the backbone network to learn robust\nhuman information. Experimental results confirm the efficacy of transmissive\nRIS, highlighting the significant potential of transmissive RIS in enhancing\nRF-based gait recognition systems. Extensive experiment results show that TRGR\nachieves an average accuracy of 97.88\\% in identifying persons when signals\ntraverse concrete walls, demonstrating the effectiveness and robustness of\nTRGR.",
    "authors": [
      "Yunlong Huang",
      "Junshuo Liu",
      "Jianan Zhang",
      "Tiebin Mi",
      "Xin Shi",
      "Robert Caiming Qiu"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21566v1",
    "category": [
      "Speech Recognition",
      "Multimodal Learning"
    ]
  },
  {
    "title": "A Performance Study of LLM-Generated Code on Leetcode",
    "abstract": "This study evaluates the efficiency of code generation by Large Language\nModels (LLMs) and measures their performance against human-crafted solutions\nusing a dataset from Leetcode. We compare 18 LLMs, considering factors such as\nmodel temperature and success rate, and their impact on code performance. This\nresearch introduces a novel method for measuring and comparing the speed of\nLLM-generated code, revealing that LLMs produce code with comparable\nperformance, irrespective of the adopted LLM. We also find that LLMs are\ncapable of generating code that is, on average, more efficient than the code\nwritten by humans. The paper further discusses the use of Leetcode as a\nbenchmarking dataset, the limitations imposed by potential data contamination,\nand the platform's measurement reliability. We believe that our findings\ncontribute to a better understanding of LLM capabilities in code generation and\nset the stage for future optimizations in the field.",
    "authors": [
      "Tristan Coignion",
      "Cl\u00e9ment Quinton",
      "Romain Rouvoy"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21579v1",
    "category": [
      "LLMs",
      "Benchmarking"
    ]
  },
  {
    "title": "Measuring What Matters: Intrinsic Distance Preservation as a Robust Metric for Embedding Quality",
    "abstract": "Unsupervised embeddings are fundamental to numerous machine learning\napplications, yet their evaluation remains a challenging task. Traditional\nassessment methods often rely on extrinsic variables, such as performance in\ndownstream tasks, which can introduce confounding factors and mask the true\nquality of embeddings. This paper introduces the Intrinsic Distance\nPreservation Evaluation (IDPE) method, a novel approach for assessing embedding\nquality based on the preservation of Mahalanobis distances between data points\nin the original and embedded spaces. We demonstrate the limitations of\nextrinsic evaluation methods through a simple example, highlighting how they\ncan lead to misleading conclusions about embedding quality. IDPE addresses\nthese issues by providing a task-independent measure of how well embeddings\npreserve the intrinsic structure of the original data. Our method leverages\nefficient similarity search techniques to make it applicable to large-scale\ndatasets. We compare IDPE with established intrinsic metrics like\ntrustworthiness and continuity, as well as extrinsic metrics such as Average\nRank and Mean Reciprocal Rank. Our results show that IDPE offers a more\ncomprehensive and reliable assessment of embedding quality across various\nscenarios. We evaluate PCA and t-SNE embeddings using IDPE, revealing insights\ninto their performance that are not captured by traditional metrics. This work\ncontributes to the field by providing a robust, efficient, and interpretable\nmethod for embedding evaluation. IDPE's focus on intrinsic properties offers a\nvaluable tool for researchers and practitioners seeking to develop and assess\nhigh-quality embeddings for diverse machine learning applications.",
    "authors": [
      "Steven N. Hart",
      "Thomas E. Tavolara"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21590v1",
    "category": [
      "Multimodal Learning",
      "Datasets"
    ]
  },
  {
    "title": "Enhancing Partially Spoofed Audio Localization with Boundary-aware Attention Mechanism",
    "abstract": "The task of partially spoofed audio localization aims to accurately determine\naudio authenticity at a frame level. Although some works have achieved\nencouraging results, utilizing boundary information within a single model\nremains an unexplored research topic. In this work, we propose a novel method\ncalled Boundary-aware Attention Mechanism (BAM). Specifically, it consists of\ntwo core modules: Boundary Enhancement and Boundary Frame-wise Attention. The\nformer assembles the intra-frame and inter-frame information to extract\ndiscriminative boundary features that are subsequently used for boundary\nposition detection and authenticity decision, while the latter leverages\nboundary prediction results to explicitly control the feature interaction\nbetween frames, which achieves effective discrimination between real and fake\nframes. Experimental results on PartialSpoof database demonstrate our proposed\nmethod achieves the best performance. The code is available at\nhttps://github.com/media-sec-lab/BAM.",
    "authors": [
      "Jiafeng Zhong",
      "Bin Li",
      "Jiangyan Yi"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21611v1",
    "category": [
      "Speech Recognition",
      "Speech Synthesis"
    ]
  },
  {
    "title": "Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music",
    "abstract": "Generative AI models have recently blossomed, significantly impacting\nartistic and musical traditions. Research investigating how humans interact\nwith and deem these models is therefore crucial. Through a listening and\nreflection study, we explore participants' perspectives on AI- vs\nhuman-generated progressive metal, in symbolic format, using rock music as a\ncontrol group. AI-generated examples were produced by ProgGP, a\nTransformer-based model. We propose a mixed methods approach to assess the\neffects of generation type (human vs. AI), genre (progressive metal vs. rock),\nand curation process (random vs. cherry-picked). This combines quantitative\nfeedback on genre congruence, preference, creativity, consistency, playability,\nhumanness, and repeatability, and qualitative feedback to provide insights into\nlisteners' experiences. A total of 32 progressive metal fans completed the\nstudy. Our findings validate the use of fine-tuning to achieve genre-specific\nspecialization in AI music generation, as listeners could distinguish between\nAI-generated rock and progressive metal. Despite some AI-generated excerpts\nreceiving similar ratings to human music, listeners exhibited a preference for\nhuman compositions. Thematic analysis identified key features for genre and AI\nvs. human distinctions. Finally, we consider the ethical implications of our\nwork in promoting musical data diversity within MIR research by focusing on an\nunder-explored genre.",
    "authors": [
      "Pedro Sarmento",
      "Jackson Loth",
      "Mathieu Barthet"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21615v1",
    "category": [
      "Explainable AI",
      "Speech Synthesis"
    ]
  },
  {
    "title": "Universal Approximation Theory: Foundations for Parallelism in Neural Networks",
    "abstract": "Neural networks are increasingly evolving towards training large models with\nbig data, a method that has demonstrated superior performance across many\ntasks. However, this approach introduces an urgent problem: current deep\nlearning models are predominantly serial, meaning that as the number of network\nlayers increases, so do the training and inference times. This is unacceptable\nif deep learning is to continue advancing. Therefore, this paper proposes a\ndeep learning parallelization strategy based on the Universal Approximation\nTheorem (UAT). From this foundation, we designed a parallel network called\nPara-Former to test our theory. Unlike traditional serial models, the inference\ntime of Para-Former does not increase with the number of layers, significantly\naccelerating the inference speed of multi-layer networks. Experimental results\nvalidate the effectiveness of this network.",
    "authors": [
      "Wei Wang",
      "Qing Li"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21670v1",
    "category": [
      "Reinforcement Learning",
      "LLMs"
    ]
  },
  {
    "title": "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities",
    "abstract": "Task-oriented dialogue (TOD) systems aim to efficiently handle task-oriented\nconversations, including information gathering. How to utilize ToD accurately,\nefficiently and effectively for information gathering has always been a\ncritical and challenging task. Recent studies have demonstrated that Large\nLanguage Models (LLMs) excel in dialogue, instruction generation, and\nreasoning, and can significantly enhance the performance of TOD through\nfine-tuning. However, current datasets primarily cater to user-led systems and\nare limited to predefined specific scenarios and slots, thereby necessitating\nimprovements in the proactiveness, diversity, and capabilities of TOD. In this\nstudy, we present a detailed multi-domain task-oriented data construction\nprocess for conversations, and a Chinese dialogue dataset generated based on\nthis process, \\textbf{TransferTOD}, which authentically simulates human-machine\ndialogues in 30 popular life service scenarios. Leveraging this dataset, we\ntrained a \\textbf{TransferTOD-7B} model using full-parameter fine-tuning,\nshowcasing notable abilities in slot filling and questioning. Our work has\ndemonstrated its strong generalization capabilities in various downstream\nscenarios, significantly enhancing both data utilization efficiency and system\nperformance. The data is released in\nhttps://github.com/KongLongGeFDU/TransferTOD.",
    "authors": [
      "Ming Zhang",
      "Caishuang Huang",
      "Yilong Wu",
      "Shichun Liu",
      "Huiyuan Zheng",
      "Yurui Dong",
      "Yujiong Shen",
      "Shihan Dou",
      "Jun Zhao",
      "Junjie Ye",
      "Qi Zhang",
      "Tao Gui",
      "Xuanjing Huang"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21693v1",
    "category": [
      "Speech Synthesis",
      "Speech Recognition"
    ]
  },
  {
    "title": "CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature",
    "abstract": "Ontologies are formal representations of knowledge in specific domains that\nprovide a structured framework for organizing and understanding complex\ninformation. Creating ontologies, however, is a complex and time-consuming\nendeavor. ChEBI is a well-known ontology in the field of chemistry, which\nprovides a comprehensive resource for defining chemical entities and their\nproperties. However, it covers only a small fraction of the rapidly growing\nknowledge in chemistry and does not provide references to the scientific\nliterature. To address this, we propose a methodology that involves augmenting\nexisting annotated text corpora with knowledge from Chebi and fine-tuning a\nlarge language model (LLM) to recognize chemical entities and their roles in\nscientific text. Our experiments demonstrate the effectiveness of our approach.\nBy combining ontological knowledge and the language understanding capabilities\nof LLMs, we achieve high precision and recall rates in identifying both the\nchemical entities and roles in scientific literature. Furthermore, we extract\nthem from a set of 8,000 ChemRxiv articles, and apply a second LLM to create a\nknowledge graph (KG) of chemical entities and roles (CEAR), which provides\ncomplementary information to ChEBI, and can help to extend it.",
    "authors": [
      "Stefan Langer",
      "Fabian Neuhaus",
      "Andreas N\u00fcrnberger"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21708v1",
    "category": [
      "LLMs",
      "Datasets"
    ]
  },
  {
    "title": "Social Learning through Interactions with Other Agents: A Survey",
    "abstract": "Social learning plays an important role in the development of human\nintelligence. As children, we imitate our parents' speech patterns until we are\nable to produce sounds; we learn from them praising us and scolding us; and as\nadults, we learn by working with others. In this work, we survey the degree to\nwhich this paradigm -- social learning -- has been mirrored in machine\nlearning. In particular, since learning socially requires interacting with\nothers, we are interested in how embodied agents can and have utilised these\ntechniques. This is especially in light of the degree to which recent advances\nin natural language processing (NLP) enable us to perform new forms of social\nlearning. We look at how behavioural cloning and next-token prediction mirror\nhuman imitation, how learning from human feedback mirrors human education, and\nhow we can go further to enable fully communicative agents that learn from each\nother. We find that while individual social learning techniques have been used\nsuccessfully, there has been little unifying work showing how to bring them\ntogether into socially embodied agents.",
    "authors": [
      "Dylan hillier",
      "Cheston Tan",
      "Jing Jiang"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21713v1",
    "category": [
      "Speech Synthesis",
      "Reinforcement Learning"
    ]
  },
  {
    "title": "UMMAN: Unsupervised Multi-graph Merge Adversarial Network for Disease Prediction Based on Intestinal Flora",
    "abstract": "The abundance of intestinal flora is closely related to human diseases, but\ndiseases are not caused by a single gut microbe. Instead, they result from the\ncomplex interplay of numerous microbial entities. This intricate and implicit\nconnection among gut microbes poses a significant challenge for disease\nprediction using abundance information from OTU data. Recently, several methods\nhave shown potential in predicting corresponding diseases. However, these\nmethods fail to learn the inner association among gut microbes from different\nhosts, leading to unsatisfactory performance. In this paper, we present a novel\narchitecture, Unsupervised Multi-graph Merge Adversarial Network (UMMAN). UMMAN\ncan obtain the embeddings of nodes in the Multi-Graph in an unsupervised\nscenario, so that it helps learn the multiplex association. Our method is the\nfirst to combine Graph Neural Network with the task of intestinal flora disease\nprediction. We employ complex relation-types to construct the Original-Graph\nand disrupt the relationships among nodes to generate corresponding\nShuffled-Graph. We introduce the Node Feature Global Integration (NFGI) module\nto represent the global features of the graph. Furthermore, we design a joint\nloss comprising adversarial loss and hybrid attention loss to ensure that the\nreal graph embedding aligns closely with the Original-Graph and diverges from\nthe Shuffled-Graph. Comprehensive experiments on five classical OTU gut\nmicrobiome datasets demonstrate the effectiveness and stability of our method.\n(We will release our code soon.)",
    "authors": [
      "Dingkun Liu",
      "Hongjie Zhou",
      "Yilu Qu",
      "Huimei Zhang",
      "Yongdong Xu"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21714v1",
    "category": [
      "AI in Healthcare",
      "Datasets"
    ]
  },
  {
    "title": "Artificial Intelligence Approaches for Energy Efficiency: A Review",
    "abstract": "United Nations set Sustainable Development Goals and this paper focuses on\n7th (Affordable and Clean Energy), 9th (Industries, Innovation and\nInfrastructure), and 13th (Climate Action) goals. Climate change is a major\nconcern in our society; for this reason, a current global objective is to\nreduce energy waste. This work summarizes all main approaches towards energy\nefficiency using Artificial Intelligence with a particular focus on multi-agent\nsystems to create smart buildings. It mentions the tight relationship between\nAI, especially IoT, and Big Data. It explains the application of AI to anomaly\ndetection in smart buildings and a possible classification of Intelligent\nEnergy Management Systems: Direct and Indirect. Finally, some drawbacks of AI\napproaches and some possible future research focuses are proposed.",
    "authors": [
      "Alberto Pasqualetto",
      "Lorenzo Serafini",
      "Michele Sprocatti"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21726v1",
    "category": [
      "Explainable AI",
      "AI in Healthcare"
    ]
  },
  {
    "title": "ParLS-PBO: A Parallel Local Search Solver for Pseudo Boolean Optimization",
    "abstract": "As a broadly applied technique in numerous optimization problems, recently,\nlocal search has been employed to solve Pseudo-Boolean Optimization (PBO)\nproblem. A representative local search solver for PBO is LSPBO. In this paper,\nfirstly, we improve LSPBO by a dynamic scoring mechanism, which dynamically\nstrikes a balance between score on hard constraints and score on the objective\nfunction.\n  Moreover, on top of this improved LSPBO , we develop the first parallel local\nsearch PBO solver. The main idea is to share good solutions among different\nthreads to guide the search, by maintaining a pool of feasible solutions. For\nevaluating solutions when updating the pool, we propose a function that\nconsiders both the solution quality and the diversity of the pool. Furthermore,\nwe calculate the polarity density in the pool to enhance the scoring function\nof local search. Our empirical experiments show clear benefits of the proposed\nparallel approach, making it competitive with the parallel version of the\nfamous commercial solver Gurobi.",
    "authors": [
      "Zhihan Chen",
      "Peng Lin",
      "Hao Hu",
      "Shaowei Cai"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21729v1",
    "category": [
      "Reinforcement Learning",
      "Benchmarking"
    ]
  },
  {
    "title": "HGOE: Hybrid External and Internal Graph Outlier Exposure for Graph Out-of-Distribution Detection",
    "abstract": "With the progressive advancements in deep graph learning, out-of-distribution\n(OOD) detection for graph data has emerged as a critical challenge. While the\nefficacy of auxiliary datasets in enhancing OOD detection has been extensively\nstudied for image and text data, such approaches have not yet been explored for\ngraph data. Unlike Euclidean data, graph data exhibits greater diversity but\nlower robustness to perturbations, complicating the integration of outliers. To\ntackle these challenges, we propose the introduction of \\textbf{H}ybrid\nExternal and Internal \\textbf{G}raph \\textbf{O}utlier \\textbf{E}xposure (HGOE)\nto improve graph OOD detection performance. Our framework involves using\nrealistic external graph data from various domains and synthesizing internal\noutliers within ID subgroups to address the poor robustness and presence of OOD\nsamples within the ID class. Furthermore, we develop a boundary-aware OE loss\nthat adaptively assigns weights to outliers, maximizing the use of high-quality\nOOD samples while minimizing the impact of low-quality ones. Our proposed HGOE\nframework is model-agnostic and designed to enhance the effectiveness of\nexisting graph OOD detection models. Experimental results demonstrate that our\nHGOE framework can significantly improve the performance of existing OOD\ndetection models across all 8 real datasets.",
    "authors": [
      "Junwei He",
      "Qianqian Xu",
      "Yangbangyan Jiang",
      "Zitai Wang",
      "Yuchen Sun",
      "Qingming Huang"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.21742v1",
    "category": [
      "Datasets",
      "Multimodal Learning"
    ]
  }
]