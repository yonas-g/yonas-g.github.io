[
  {
    "title": "Capturing the security expert knowledge in feature selection for web application attack detection",
    "abstract": "This article puts forward the use of mutual information values to replicate\nthe expertise of security professionals in selecting features for detecting web\nattacks. The goal is to enhance the effectiveness of web application firewalls\n(WAFs). Web applications are frequently vulnerable to various security threats,\nmaking WAFs essential for their protection. WAFs analyze HTTP traffic using\nrule-based approaches to identify known attack patterns and to detect and block\npotential malicious requests. However, a major challenge is the occurrence of\nfalse positives, which can lead to blocking legitimate traffic and impact the\nnormal functioning of the application. The problem is addressed as an approach\nthat combines supervised learning for feature selection with a semi-supervised\nlearning scenario for training a One-Class SVM model. The experimental findings\nshow that the model trained with features selected by the proposed algorithm\noutperformed the expert-based selection approach in terms of performance.\nAdditionally, the results obtained by the traditional rule-based WAF\nModSecurity, configured with a vanilla set of OWASP CRS rules, were also\nimproved.",
    "authors": [
      "Amanda Riverol",
      "Gustavo Betarte",
      "Rodrigo Mart\u00ednez",
      "\u00c1lvaro Pardo"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18445v1",
    "category": [
      "Datasets",
      "AI in Healthcare"
    ]
  },
  {
    "title": "Diffusion-Driven Semantic Communication for Generative Models with Bandwidth Constraints",
    "abstract": "Diffusion models have been extensively utilized in AI-generated content\n(AIGC) in recent years, thanks to the superior generation capabilities.\nCombining with semantic communications, diffusion models are used for tasks\nsuch as denoising, data reconstruction, and content generation. However,\nexisting diffusion-based generative models do not consider the stringent\nbandwidth limitation, which limits its application in wireless communication.\nThis paper introduces a diffusion-driven semantic communication framework with\nadvanced VAE-based compression for bandwidth-constrained generative model. Our\ndesigned architecture utilizes the diffusion model, where the signal\ntransmission process through the wireless channel acts as the forward process\nin diffusion. To reduce bandwidth requirements, we incorporate a downsampling\nmodule and a paired upsampling module based on a variational auto-encoder with\nreparameterization at the receiver to ensure that the recovered features\nconform to the Gaussian distribution. Furthermore, we derive the loss function\nfor our proposed system and evaluate its performance through comprehensive\nexperiments. Our experimental results demonstrate significant improvements in\npixel-level metrics such as peak signal to noise ratio (PSNR) and semantic\nmetrics like learned perceptual image patch similarity (LPIPS). These\nenhancements are more profound regarding the compression rates and SNR compared\nto deep joint source-channel coding (DJSCC).",
    "authors": [
      "Lei Guo",
      "Wei Chen",
      "Yuxuan Sun",
      "Bo Ai",
      "Nikolaos Pappas",
      "Tony Quek"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18468v1",
    "category": [
      "Speech Synthesis",
      "Multimodal Learning"
    ]
  },
  {
    "title": "Non-Overlapping Placement of Macro Cells based on Reinforcement Learning in Chip Design",
    "abstract": "Due to the increasing complexity of chip design, existing placement methods\nstill have many shortcomings in dealing with macro cells coverage and\noptimization efficiency. Aiming at the problems of layout overlap, inferior\nperformance, and low optimization efficiency in existing chip design methods,\nthis paper proposes an end-to-end placement method, SRLPlacer, based on\nreinforcement learning. First, the placement problem is transformed into a\nMarkov decision process by establishing the coupling relationship graph model\nbetween macro cells to learn the strategy for optimizing layouts. Secondly, the\nwhole placement process is optimized after integrating the standard cell\nlayout. By assessing on the public benchmark ISPD2005, the proposed SRLPlacer\ncan effectively solve the overlap problem between macro cells while considering\nrouting congestion and shortening the total wire length to ensure routability.",
    "authors": [
      "Tao Yu",
      "Peng Gao",
      "Fei Wang",
      "Ru-Yue Yuan"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18499v1",
    "category": [
      "Reinforcement Learning",
      "Benchmarking"
    ]
  },
  {
    "title": "SLIM: Style-Linguistics Mismatch Model for Generalized Audio Deepfake Detection",
    "abstract": "Audio deepfake detection (ADD) is crucial to combat the misuse of speech\nsynthesized from generative AI models. Existing ADD models suffer from\ngeneralization issues, with a large performance discrepancy between in-domain\nand out-of-domain data. Moreover, the black-box nature of existing models\nlimits their use in real-world scenarios, where explanations are required for\nmodel decisions. To alleviate these issues, we introduce a new ADD model that\nexplicitly uses the StyleLInguistics Mismatch (SLIM) in fake speech to separate\nthem from real speech. SLIM first employs self-supervised pretraining on only\nreal samples to learn the style-linguistics dependency in the real class. The\nlearned features are then used in complement with standard pretrained acoustic\nfeatures (e.g., Wav2vec) to learn a classifier on the real and fake classes.\nWhen the feature encoders are frozen, SLIM outperforms benchmark methods on\nout-of-domain datasets while achieving competitive results on in-domain data.\nThe features learned by SLIM allow us to quantify the (mis)match between style\nand linguistic content in a sample, hence facilitating an explanation of the\nmodel decision.",
    "authors": [
      "Yi Zhu",
      "Surya Koppisetti",
      "Trang Tran",
      "Gaurav Bharaj"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18517v1",
    "category": [
      "Speech Synthesis",
      "Speech Recognition"
    ]
  },
  {
    "title": "TCGPN: Temporal-Correlation Graph Pre-trained Network for Stock Forecasting",
    "abstract": "Recently, the incorporation of both temporal features and the correlation\nacross time series has become an effective approach in time series prediction.\nSpatio-Temporal Graph Neural Networks (STGNNs) demonstrate good performance on\nmany Temporal-correlation Forecasting Problem. However, when applied to tasks\nlacking periodicity, such as stock data prediction, the effectiveness and\nrobustness of STGNNs are found to be unsatisfactory. And STGNNs are limited by\nmemory savings so that cannot handle problems with a large number of nodes. In\nthis paper, we propose a novel approach called the Temporal-Correlation Graph\nPre-trained Network (TCGPN) to address these limitations. TCGPN utilize\nTemporal-correlation fusion encoder to get a mixed representation and\npre-training method with carefully designed temporal and correlation\npre-training tasks. Entire structure is independent of the number and order of\nnodes, so better results can be obtained through various data enhancements. And\nmemory consumption during training can be significantly reduced through\nmultiple sampling. Experiments are conducted on real stock market data sets\nCSI300 and CSI500 that exhibit minimal periodicity. We fine-tune a simple MLP\nin downstream tasks and achieve state-of-the-art results, validating the\ncapability to capture more robust temporal correlation patterns.",
    "authors": [
      "Wenbo Yan",
      "Ying Tan"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18519v1",
    "category": [
      "Multimodal Learning",
      "Datasets"
    ]
  },
  {
    "title": "Patched MOA: optimizing inference for diverse software development tasks",
    "abstract": "This paper introduces Patched MOA (Mixture of Agents), an inference\noptimization technique that significantly enhances the performance of large\nlanguage models (LLMs) across diverse software development tasks. We evaluate\nthree inference optimization algorithms - Best of N, Mixture of Agents, and\nMonte Carlo Tree Search and demonstrate that Patched MOA can boost the\nperformance of smaller models to surpass that of larger, more expensive models.\nNotably, our approach improves the gpt-4o-mini model's performance on the\nArena-Hard-Auto benchmark by 15.52%, outperforming gpt-4-turbo at a fraction of\nthe cost. We also apply Patched MOA to various software development workflows,\nshowing consistent improvements in task completion rates. Our method is\nmodel-agnostic, transparent to end-users, and can be easily integrated into\nexisting LLM pipelines. This work contributes to the growing field of LLM\noptimization, offering a cost-effective solution for enhancing model\nperformance without the need for fine-tuning or larger models.",
    "authors": [
      "Asankhaya Sharma"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18521v1",
    "category": [
      "LLMs",
      "Speech Recognition"
    ]
  },
  {
    "title": "Towards Improving NAM-to-Speech Synthesis Intelligibility using Self-Supervised Speech Models",
    "abstract": "We propose a novel approach to significantly improve the intelligibility in\nthe Non-Audible Murmur (NAM)-to-speech conversion task, leveraging\nself-supervision and sequence-to-sequence (Seq2Seq) learning techniques. Unlike\nconventional methods that explicitly record ground-truth speech, our\nmethodology relies on self-supervision and speech-to-speech synthesis to\nsimulate ground-truth speech. Despite utilizing simulated speech, our method\nsurpasses the current state-of-the-art (SOTA) by 29.08% improvement in the\nMel-Cepstral Distortion (MCD) metric. Additionally, we present error rates and\ndemonstrate our model's proficiency to synthesize speech in novel voices of\ninterest. Moreover, we present a methodology for augmenting the existing CSTR\nNAM TIMIT Plus corpus, setting a benchmark with a Word Error Rate (WER) of\n42.57% to gauge the intelligibility of the synthesized speech. Speech samples\ncan be found at https://nam2speech.github.io/NAM2Speech/",
    "authors": [
      "Neil Shah",
      "Shirish Karande",
      "Vineet Gandhi"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18541v1",
    "category": [
      "Speech Synthesis",
      "Speech Recognition"
    ]
  },
  {
    "title": "Look Globally and Reason: Two-stage Path Reasoning over Sparse Knowledge Graphs",
    "abstract": "Sparse Knowledge Graphs (KGs), frequently encountered in real-world\napplications, contain fewer facts in the form of (head entity, relation, tail\nentity) compared to more populated KGs. The sparse KG completion task, which\nreasons answers for given queries in the form of (head entity, relation, ?) for\nsparse KGs, is particularly challenging due to the necessity of reasoning\nmissing facts based on limited facts. Path-based models, known for excellent\nexplainability, are often employed for this task. However, existing path-based\nmodels typically rely on external models to fill in missing facts and\nsubsequently perform path reasoning. This approach introduces unexplainable\nfactors or necessitates meticulous rule design. In light of this, this paper\nproposes an alternative approach by looking inward instead of seeking external\nassistance. We introduce a two-stage path reasoning model called LoGRe (Look\nGlobally and Reason) over sparse KGs. LoGRe constructs a relation-path\nreasoning schema by globally analyzing the training data to alleviate the\nsparseness problem. Based on this schema, LoGRe then aggregates paths to reason\nout answers. Experimental results on five benchmark sparse KG datasets\ndemonstrate the effectiveness of the proposed LoGRe model.",
    "authors": [
      "Saiping Guan",
      "Jiyao Wei",
      "Xiaolong Jin",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18556v1",
    "category": [
      "Explainable AI",
      "Datasets"
    ]
  },
  {
    "title": "Speech Bandwidth Expansion Via High Fidelity Generative Adversarial Networks",
    "abstract": "Speech bandwidth expansion is crucial for expanding the frequency range of\nlow-bandwidth speech signals, thereby improving audio quality, clarity and\nperceptibility in digital applications. Its applications span telephony,\ncompression, text-to-speech synthesis, and speech recognition. This paper\npresents a novel approach using a high-fidelity generative adversarial network,\nunlike cascaded systems, our system is trained end-to-end on paired narrowband\nand wideband speech signals. Our method integrates various bandwidth upsampling\nratios into a single unified model specifically designed for speech bandwidth\nexpansion applications. Our approach exhibits robust performance across various\nbandwidth expansion factors, including those not encountered during training,\ndemonstrating zero-shot capability. To the best of our knowledge, this is the\nfirst work to showcase this capability. The experimental results demonstrate\nthat our method outperforms previous end-to-end approaches, as well as\ninterpolation and traditional techniques, showcasing its effectiveness in\npractical speech enhancement applications.",
    "authors": [
      "Mahmoud Salhab",
      "Haidar Harmanani"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18571v1",
    "category": [
      "Speech Synthesis",
      "Speech Recognition"
    ]
  },
  {
    "title": "Climbing the Complexity Ladder with Expressive Attention",
    "abstract": "Attention involves comparing query and key vectors in terms of a scalar\nproduct, $\\mathbf{Q}^T\\mathbf{K}$, together with a subsequent softmax\nnormalization. Classicaly, parallel/orthogonal/antiparallel queries and keys\nlead to large/intermediate/small attention weights. Here we study expressive\nattention (EA), which is based on $(\\mathbf{Q}^T\\mathbf{K})^2$, the squared dot\nproduct. In this case attention is enhanced when query and key are either\nparallel or antiparallel, and suppressed for orthogonal configurations. For a\nseries of autoregressive prediction tasks, we find that EA performs at least as\nwell as the standard mechanism, dot-product attention (DPA). Increasing task\ncomplexity, EA is observed to outperform DPA with increasing margins, which\nalso holds for multi-task settings. For a given model size, EA manages to\nachieve 100\\% performance for a range of complexity levels not accessible to\nDPA.",
    "authors": [
      "Claudius Gros"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18601v1",
    "category": [
      "Explainable AI",
      "Reinforcement Learning"
    ]
  },
  {
    "title": "Using GPT-4 to guide causal machine learning",
    "abstract": "Since its introduction to the public, ChatGPT has had an unprecedented\nimpact. While some experts praised AI advancements and highlighted their\npotential risks, others have been critical about the accuracy and usefulness of\nLarge Language Models (LLMs). In this paper, we are interested in the ability\nof LLMs to identify causal relationships. We focus on the well-established\nGPT-4 (Turbo) and evaluate its performance under the most restrictive\nconditions, by isolating its ability to infer causal relationships based solely\non the variable labels without being given any context, demonstrating the\nminimum level of effectiveness one can expect when it is provided with\nlabel-only information. We show that questionnaire participants judge the GPT-4\ngraphs as the most accurate in the evaluated categories, closely followed by\nknowledge graphs constructed by domain experts, with causal Machine Learning\n(ML) far behind. We use these results to highlight the important limitation of\ncausal ML, which often produces causal graphs that violate common sense,\naffecting trust in them. However, we show that pairing GPT-4 with causal ML\novercomes this limitation, resulting in graphical structures learnt from real\ndata that align more closely with those identified by domain experts, compared\nto structures learnt by causal ML alone. Overall, our findings suggest that\ndespite GPT-4 not being explicitly designed to reason causally, it can still be\na valuable tool for causal representation, as it improves the causal discovery\nprocess of causal ML algorithms that are designed to do just that.",
    "authors": [
      "Anthony C. Constantinou",
      "Neville K. Kitson",
      "Alessio Zanga"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18607v1",
    "category": [
      "Explainable AI",
      "LLMs"
    ]
  },
  {
    "title": "Collaborative Evolving Strategy for Automatic Data-Centric Development",
    "abstract": "Artificial Intelligence (AI) significantly influences many fields, largely\nthanks to the vast amounts of high-quality data for machine learning models.\nThe emphasis is now on a data-centric AI strategy, prioritizing data\ndevelopment over model design progress. Automating this process is crucial. In\nthis paper, we serve as the first work to introduce the automatic data-centric\ndevelopment (AD^2) task and outline its core challenges, which require\ndomain-experts-like task scheduling and implementation capability, largely\nunexplored by previous work.\n  By leveraging the strong complex problem-solving capabilities of large\nlanguage models (LLMs), we propose an LLM-based autonomous agent, equipped with\na strategy named Collaborative Knowledge-STudying-Enhanced Evolution by\nRetrieval (Co-STEER), to simultaneously address all the challenges.\nSpecifically, our proposed Co-STEER agent enriches its domain knowledge through\nour proposed evolving strategy and develops both its scheduling and\nimplementation skills by accumulating and retrieving domain-specific practical\nexperience. With an improved schedule, the capability for implementation\naccelerates. Simultaneously, as implementation feedback becomes more thorough,\nthe scheduling accuracy increases. These two capabilities evolve together\nthrough practical feedback, enabling a collaborative evolution process.\n  Extensive experimental results demonstrate that our Co-STEER agent breaks new\nground in AD^2 research, possesses strong evolvable schedule and implementation\nability, and demonstrates the significant effectiveness of its components. Our\nCo-STEER paves the way for AD^2 advancements.",
    "authors": [
      "Xu Yang",
      "Haotian Chen",
      "Wenjun Feng",
      "Haoxue Wang",
      "Zeqi Ye",
      "Xinjie Shen",
      "Xiao Yang",
      "Shizhao Sun",
      "Weiqing Liu",
      "Jiang Bian"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18690v1",
    "category": [
      "Explainable AI",
      "LLMs"
    ]
  },
  {
    "title": "Neurosymbolic AI for Enhancing Instructability in Generative AI",
    "abstract": "Generative AI, especially via Large Language Models (LLMs), has transformed\ncontent creation across text, images, and music, showcasing capabilities in\nfollowing instructions through prompting, largely facilitated by instruction\ntuning. Instruction tuning is a supervised fine-tuning method where LLMs are\ntrained on datasets formatted with specific tasks and corresponding\ninstructions. This method systematically enhances the model's ability to\ncomprehend and execute the provided directives. Despite these advancements,\nLLMs still face challenges in consistently interpreting complex, multi-step\ninstructions and generalizing them to novel tasks, which are essential for\nbroader applicability in real-world scenarios. This article explores why\nneurosymbolic AI offers a better path to enhance the instructability of LLMs.\nWe explore the use a symbolic task planner to decompose high-level instructions\ninto structured tasks, a neural semantic parser to ground these tasks into\nexecutable actions, and a neuro-symbolic executor to implement these actions\nwhile dynamically maintaining an explicit representation of state. We also seek\nto show that neurosymbolic approach enhances the reliability and\ncontext-awareness of task execution, enabling LLMs to dynamically interpret and\nrespond to a wider range of instructional contexts with greater precision and\nflexibility.",
    "authors": [
      "Amit Sheth",
      "Vishal Pallagani",
      "Kaushik Roy"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18722v1",
    "category": [
      "Speech Synthesis",
      "Speech Recognition"
    ]
  },
  {
    "title": "AutoRDF2GML: Facilitating RDF Integration in Graph Machine Learning",
    "abstract": "In this paper, we introduce AutoRDF2GML, a framework designed to convert RDF\ndata into data representations tailored for graph machine learning tasks.\nAutoRDF2GML enables, for the first time, the creation of both content-based\nfeatures -- i.e., features based on RDF datatype properties -- and\ntopology-based features -- i.e., features based on RDF object properties.\nCharacterized by automated feature extraction, AutoRDF2GML makes it possible\neven for users less familiar with RDF and SPARQL to generate data\nrepresentations ready for graph machine learning tasks, such as link\nprediction, node classification, and graph classification. Furthermore, we\npresent four new benchmark datasets for graph machine learning, created from\nlarge RDF knowledge graphs using our framework. These datasets serve as\nvaluable resources for evaluating graph machine learning approaches, such as\ngraph neural networks. Overall, our framework effectively bridges the gap\nbetween the Graph Machine Learning and Semantic Web communities, paving the way\nfor RDF-based machine learning applications.",
    "authors": [
      "Michael F\u00e4rber",
      "David Lamprecht",
      "Yuni Susanti"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18735v1",
    "category": [
      "Datasets",
      "Multimodal Learning"
    ]
  },
  {
    "title": "Score matching through the roof: linear, nonlinear, and latent variables causal discovery",
    "abstract": "Causal discovery from observational data holds great promise, but existing\nmethods rely on strong assumptions about the underlying causal structure, often\nrequiring full observability of all relevant variables. We tackle these\nchallenges by leveraging the score function $\\nabla \\log p(X)$ of observed\nvariables for causal discovery and propose the following contributions. First,\nwe generalize the existing results of identifiability with the score to\nadditive noise models with minimal requirements on the causal mechanisms.\nSecond, we establish conditions for inferring causal relations from the score\neven in the presence of hidden variables; this result is two-faced: we\ndemonstrate the score's potential as an alternative to conditional independence\ntests to infer the equivalence class of causal graphs with hidden variables,\nand we provide the necessary conditions for identifying direct causes in latent\nvariable models. Building on these insights, we propose a flexible algorithm\nfor causal discovery across linear, nonlinear, and latent variable models,\nwhich we empirically validate.",
    "authors": [
      "Francesco Montagna",
      "Philipp M. Faller",
      "Patrick Bloebaum",
      "Elke Kirschbaum",
      "Francesco Locatello"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18755v1",
    "category": [
      "Explainable AI",
      "Datasets"
    ]
  },
  {
    "title": "Any four real numbers are on all fours with analogy",
    "abstract": "This work presents a formalization of analogy on numbers that relies on\ngeneralized means. It is motivated by recent advances in artificial\nintelligence and applications of machine learning, where the notion of analogy\nis used to infer results, create data and even as an assessment tool of object\nrepresentations, or embeddings, that are basically collections of numbers\n(vectors, matrices, tensors). This extended analogy use asks for mathematical\nfoundations and clear understanding of the notion of analogy between numbers.\nWe propose a unifying view of analogies that relies on generalized means\ndefined in terms of a power parameter. In particular, we show that any four\nincreasing positive real numbers is an analogy in a unique suitable power. In\naddition, we show that any such analogy can be reduced to an equivalent\narithmetic analogy and that any analogical equation has a solution for\nincreasing numbers, which generalizes without restriction to complex numbers.\nThese foundational results provide a better understanding of analogies in areas\nwhere representations are numerical.",
    "authors": [
      "Yves Lepage",
      "Miguel Couceiro"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18770v1",
    "category": [
      "Multimodal Learning",
      "Explainable AI"
    ]
  },
  {
    "title": "Understanding XAI Through the Philosopher's Lens: A Historical Perspective",
    "abstract": "Despite explainable AI (XAI) has recently become a hot topic and several\ndifferent approaches have been developed, there is still a widespread belief\nthat it lacks a convincing unifying foundation. On the other hand, over the\npast centuries, the very concept of explanation has been the subject of\nextensive philosophical analysis in an attempt to address the fundamental\nquestion of \"why\" in the context of scientific law. However, this discussion\nhas rarely been connected with XAI. This paper tries to fill in this gap and\naims to explore the concept of explanation in AI through an epistemological\nlens. By comparing the historical development of both the philosophy of science\nand AI, an intriguing picture emerges. Specifically, we show that a gradual\nprogression has independently occurred in both domains from logical-deductive\nto statistical models of explanation, thereby experiencing in both cases a\nparadigm shift from deterministic to nondeterministic and probabilistic\ncausality. Interestingly, we also notice that similar concepts have\nindependently emerged in both realms such as, for example, the relation between\nexplanation and understanding and the importance of pragmatic factors. Our\nstudy aims to be the first step towards understanding the philosophical\nunderpinnings of the notion of explanation in AI, and we hope that our findings\nwill shed some fresh light on the elusive nature of XAI.",
    "authors": [
      "Martina Mattioli",
      "Antonio Emanuele Cin\u00e0",
      "Marcello Pelillo"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18782v1",
    "category": [
      "Explainable AI",
      "AI in Healthcare"
    ]
  },
  {
    "title": "Robust Learning in Bayesian Parallel Branching Graph Neural Networks: The Narrow Width Limit",
    "abstract": "The infinite width limit of random neural networks is known to result in\nNeural Networks as Gaussian Process (NNGP) (Lee et al. [2018]), characterized\nby task-independent kernels. It is widely accepted that larger network widths\ncontribute to improved generalization (Park et al. [2019]). However, this work\nchallenges this notion by investigating the narrow width limit of the Bayesian\nParallel Branching Graph Neural Network (BPB-GNN), an architecture that\nresembles residual networks. We demonstrate that when the width of a BPB-GNN is\nsignificantly smaller compared to the number of training examples, each branch\nexhibits more robust learning due to a symmetry breaking of branches in kernel\nrenormalization. Surprisingly, the performance of a BPB-GNN in the narrow width\nlimit is generally superior or comparable to that achieved in the wide width\nlimit in bias-limited scenarios. Furthermore, the readout norms of each branch\nin the narrow width limit are mostly independent of the architectural\nhyperparameters but generally reflective of the nature of the data. Our results\ncharacterize a newly defined narrow-width regime for parallel branching\nnetworks in general.",
    "authors": [
      "Zechen Zhang",
      "Haim Sompolinsky"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18807v1",
    "category": [
      "Explainable AI",
      "Reinforcement Learning"
    ]
  },
  {
    "title": "Learning Chaotic Systems and Long-Term Predictions with Neural Jump ODEs",
    "abstract": "The Path-dependent Neural Jump ODE (PD-NJ-ODE) is a model for online\nprediction of generic (possibly non-Markovian) stochastic processes with\nirregular (in time) and potentially incomplete (with respect to coordinates)\nobservations. It is a model for which convergence to the $L^2$-optimal\npredictor, which is given by the conditional expectation, is established\ntheoretically. Thereby, the training of the model is solely based on a dataset\nof realizations of the underlying stochastic process, without the need of\nknowledge of the law of the process. In the case where the underlying process\nis deterministic, the conditional expectation coincides with the process\nitself. Therefore, this framework can equivalently be used to learn the\ndynamics of ODE or PDE systems solely from realizations of the dynamical system\nwith different initial conditions. We showcase the potential of our method by\napplying it to the chaotic system of a double pendulum. When training the\nstandard PD-NJ-ODE method, we see that the prediction starts to diverge from\nthe true path after about half of the evaluation time. In this work we enhance\nthe model with two novel ideas, which independently of each other improve the\nperformance of our modelling setup. The resulting dynamics match the true\ndynamics of the chaotic system very closely. The same enhancements can be used\nto provably enable the PD-NJ-ODE to learn long-term predictions for general\nstochastic datasets, where the standard model fails. This is verified in\nseveral experiments.",
    "authors": [
      "Florian Krach",
      "Josef Teichmann"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18808v1",
    "category": [
      "Reinforcement Learning",
      "Explainable AI"
    ]
  },
  {
    "title": "Online Planning in POMDPs with State-Requests",
    "abstract": "In key real-world problems, full state information is sometimes available but\nonly at a high cost, like activating precise yet energy-intensive sensors or\nconsulting humans, thereby compelling the agent to operate under partial\nobservability. For this scenario, we propose AEMS-SR (Anytime Error\nMinimization Search with State Requests), a principled online planning\nalgorithm tailored for POMDPs with state requests. By representing the search\nspace as a graph instead of a tree, AEMS-SR avoids the exponential growth of\nthe search space originating from state requests. Theoretical analysis\ndemonstrates AEMS-SR's $\\varepsilon$-optimality, ensuring solution quality,\nwhile empirical evaluations illustrate its effectiveness compared with AEMS and\nPOMCP, two SOTA online planning algorithms. AEMS-SR enables efficient planning\nin domains characterized by partial observability and costly state requests\noffering practical benefits across various applications.",
    "authors": [
      "Raphael Avalos",
      "Eugenio Bargiacchi",
      "Ann Now\u00e9",
      "Diederik M. Roijers",
      "Frans A. Oliehoek"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18812v1",
    "category": [
      "Reinforcement Learning",
      "Explainable AI"
    ]
  },
  {
    "title": "Human-artificial intelligence teaming for scientific information extraction from data-driven additive manufacturing research using large language models",
    "abstract": "Data-driven research in Additive Manufacturing (AM) has gained significant\nsuccess in recent years. This has led to a plethora of scientific literature to\nemerge. The knowledge in these works consists of AM and Artificial Intelligence\n(AI) contexts that have not been mined and formalized in an integrated way. It\nrequires substantial effort and time to extract scientific information from\nthese works. AM domain experts have contributed over two dozen review papers to\nsummarize these works. However, information specific to AM and AI contexts\nstill requires manual effort to extract. The recent success of foundation\nmodels such as BERT (Bidirectional Encoder Representations for Transformers) or\nGPT (Generative Pre-trained Transformers) on textual data has opened the\npossibility of expediting scientific information extraction. We propose a\nframework that enables collaboration between AM and AI experts to continuously\nextract scientific information from data-driven AM literature. A demonstration\ntool is implemented based on the proposed framework and a case study is\nconducted to extract information relevant to the datasets, modeling, sensing,\nand AM system categories. We show the ability of LLMs (Large Language Models)\nto expedite the extraction of relevant information from data-driven AM\nliterature. In the future, the framework can be used to extract information\nfrom the broader design and manufacturing literature in the engineering\ndiscipline.",
    "authors": [
      "Mutahar Safdar",
      "Jiarui Xie",
      "Andrei Mircea",
      "Yaoyao Fiona Zhao"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18827v1",
    "category": [
      "Datasets",
      "Explainable AI"
    ]
  },
  {
    "title": "Enhancing material property prediction with ensemble deep graph convolutional networks",
    "abstract": "Machine learning (ML) models have emerged as powerful tools for accelerating\nmaterials discovery and design by enabling accurate predictions of properties\nfrom compositional and structural data. These capabilities are vital for\ndeveloping advanced technologies across fields such as energy, electronics, and\nbiomedicine, potentially reducing the time and resources needed for new\nmaterial exploration and promoting rapid innovation cycles. Recent efforts have\nfocused on employing advanced ML algorithms, including deep learning - based\ngraph neural network, for property prediction. Additionally, ensemble models\nhave proven to enhance the generalizability and robustness of ML and DL.\nHowever, the use of such ensemble strategies in deep graph networks for\nmaterial property prediction remains underexplored. Our research provides an\nin-depth evaluation of ensemble strategies in deep learning - based graph\nneural network, specifically targeting material property prediction tasks. By\ntesting the Crystal Graph Convolutional Neural Network (CGCNN) and its\nmultitask version, MT-CGCNN, we demonstrated that ensemble techniques,\nespecially prediction averaging, substantially improve precision beyond\ntraditional metrics for key properties like formation energy per atom ($\\Delta\nE^{f}$), band gap ($E_{g}$) and density ($\\rho$) in 33,990 stable inorganic\nmaterials. These findings support the broader application of ensemble methods\nto enhance predictive accuracy in the field.",
    "authors": [
      "Chowdhury Mohammad Abid Rahman",
      "Ghadendra Bhandari",
      "Nasser M Nasrabadi",
      "Aldo H. Romero",
      "Prashnna K. Gyawali"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18847v1",
    "category": [
      "Explainable AI",
      "Multimodal Learning"
    ]
  },
  {
    "title": "SOAP-RL: Sequential Option Advantage Propagation for Reinforcement Learning in POMDP Environments",
    "abstract": "This work compares ways of extending Reinforcement Learning algorithms to\nPartially Observed Markov Decision Processes (POMDPs) with options. One view of\noptions is as temporally extended action, which can be realized as a memory\nthat allows the agent to retain historical information beyond the policy's\ncontext window. While option assignment could be handled using heuristics and\nhand-crafted objectives, learning temporally consistent options and associated\nsub-policies without explicit supervision is a challenge. Two algorithms, PPOEM\nand SOAP, are proposed and studied in depth to address this problem. PPOEM\napplies the forward-backward algorithm (for Hidden Markov Models) to optimize\nthe expected returns for an option-augmented policy. However, this learning\napproach is unstable during on-policy rollouts. It is also unsuited for\nlearning causal policies without the knowledge of future trajectories, since\noption assignments are optimized for offline sequences where the entire episode\nis available. As an alternative approach, SOAP evaluates the policy gradient\nfor an optimal option assignment. It extends the concept of the generalized\nadvantage estimation (GAE) to propagate option advantages through time, which\nis an analytical equivalent to performing temporal back-propagation of option\npolicy gradients. This option policy is only conditional on the history of the\nagent, not future actions. Evaluated against competing baselines, SOAP\nexhibited the most robust performance, correctly discovering options for POMDP\ncorridor environments, as well as on standard benchmarks including Atari and\nMuJoCo, outperforming PPOEM, as well as LSTM and Option-Critic baselines. The\nopen-sourced code is available at https://github.com/shuishida/SoapRL.",
    "authors": [
      "Shu Ishida",
      "Jo\u00e3o F. Henriques"
    ],
    "pdf_link": "http://arxiv.org/pdf/2407.18913v1",
    "category": [
      "Reinforcement Learning",
      "Explainable AI"
    ]
  }
]