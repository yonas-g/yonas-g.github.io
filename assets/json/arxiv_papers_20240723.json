[{"title":"Robustness of Speech Separation Models for Similar-pitch Speakers","abstract":"Single-channel speech separation is a crucial task for enhancing speech\nrecognition systems in multi-speaker environments. This paper investigates the\nrobustness of state-of-the-art Neural Network models in scenarios where the\npitch differences between speakers are minimal. Building on earlier findings by\nDitter and Gerkmann, which identified a significant performance drop for the\n2018 Chimera++ under similar-pitch conditions, our study extends the analysis\nto more recent and sophisticated Neural Network models. Our experiments reveal\nthat modern models have substantially reduced the performance gap for matched\ntraining and testing conditions. However, a substantial performance gap\npersists under mismatched conditions, with models performing well for large\npitch differences but showing worse performance if the speakers' pitches are\nsimilar. These findings motivate further research into the generalizability of\nspeech separation models to similar-pitch speakers and unseen data.","authors":["Bunlong Lay","Sebastian Zaczek","Kristina Tesch","Timo Gerkmann"],"pdf_link":"http://arxiv.org/pdf/2407.15749v1","category":["Speech Recognition","Speech Synthesis"]},{"title":"Odyssey: Empowering Agents with Open-World Skills","abstract":"Recent studies have delved into constructing generalist agents for open-world\nembodied environments like Minecraft. Despite the encouraging results, existing\nefforts mainly focus on solving basic programmatic tasks, e.g., material\ncollection and tool-crafting following the Minecraft tech-tree, treating the\nObtainDiamond task as the ultimate goal. This limitation stems from the\nnarrowly defined set of actions available to agents, requiring them to learn\neffective long-horizon strategies from scratch. Consequently, discovering\ndiverse gameplay opportunities in the open world becomes challenging. In this\nwork, we introduce ODYSSEY, a new framework that empowers Large Language Model\n(LLM)-based agents with open-world skills to explore the vast Minecraft world.\nODYSSEY comprises three key parts: (1) An interactive agent with an open-world\nskill library that consists of 40 primitive skills and 183 compositional\nskills. (2) A fine-tuned LLaMA-3 model trained on a large question-answering\ndataset with 390k+ instruction entries derived from the Minecraft Wiki. (3) A\nnew open-world benchmark includes thousands of long-term planning tasks, tens\nof dynamic-immediate planning tasks, and one autonomous exploration task.\nExtensive experiments demonstrate that the proposed ODYSSEY framework can\neffectively evaluate the planning and exploration capabilities of agents. All\ndatasets, model weights, and code are publicly available to motivate future\nresearch on more advanced autonomous agent solutions.","authors":["Shunyu Liu","Yaoru Li","Kongcheng Zhang","Zhenyu Cui","Wenkai Fang","Yuxuan Zheng","Tongya Zheng","Mingli Song"],"pdf_link":"http://arxiv.org/pdf/2407.15325v1","category":["Explainable AI","Reinforcement Learning"]},{"title":"Integrating IP Broadcasting with Audio Tags: Workflow and Challenges","abstract":"The broadcasting industry is increasingly adopting IP techniques,\nrevolutionising both live and pre-recorded content production, from news\ngathering to live music events. IP broadcasting allows for the transport of\naudio and video signals in an easily configurable way, aligning with modern\nnetworking techniques. This shift towards an IP workflow allows for much\ngreater flexibility, not only in routing signals but with the integration of\ntools using standard web development techniques. One possible tool could\ninclude the use of live audio tagging, which has a number of uses in the\nproduction of content. These include from automated closed captioning to\nidentifying unwanted sound events within a scene. In this paper, we describe\nthe process of containerising an audio tagging model into a microservice, a\nsmall segregated code module that can be integrated into a multitude of\ndifferent network setups. The goal is to develop a modular, accessible, and\nflexible tool capable of seamless deployment into broadcasting workflows of all\nsizes, from small productions to large corporations. Challenges surrounding\nlatency of the selected audio tagging model and its effect on the usefulness of\nthe end product are discussed.","authors":["Rhys Burchett-Vass","Arshdeep Singh","Gabriel Bibb\u00f3","Mark D. Plumbley"],"pdf_link":"http://arxiv.org/pdf/2407.15423v2","category":["Speech Recognition","Speech Synthesis"]},{"title":"Decoding BACnet Packets: A Large Language Model Approach for Packet Interpretation","abstract":"The Industrial Control System (ICS) environment encompasses a wide range of\nintricate communication protocols, posing substantial challenges for Security\nOperations Center (SOC) analysts tasked with monitoring, interpreting, and\naddressing network activities and security incidents. Conventional monitoring\ntools and techniques often struggle to provide a clear understanding of the\nnature and intent of ICS-specific communications. To enhance comprehension, we\npropose a software solution powered by a Large Language Model (LLM). This\nsolution currently focused on BACnet protocol, processes a packet file data and\nextracts context by using a mapping database, and contemporary context\nretrieval methods for Retrieval Augmented Generation (RAG). The processed\npacket information, combined with the extracted context, serves as input to the\nLLM, which generates a concise packet file summary for the user. The software\ndelivers a clear, coherent, and easily understandable summary of network\nactivities, enabling SOC analysts to better assess the current state of the\ncontrol system.","authors":["Rashi Sharma","Hiroyuki Okada","Tatsumi Oba","Karthikk Subramanian","Naoto Yanai","Sugiri Pranata"],"pdf_link":"http://arxiv.org/pdf/2407.15428v1","category":["Datasets","Speech Recognition"]},{"title":"Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed Graphs","abstract":"The text-attributed graph (TAG) is one kind of important real-world\ngraph-structured data with each node associated with raw texts. For TAGs,\ntraditional few-shot node classification methods directly conduct training on\nthe pre-processed node features and do not consider the raw texts. The\nperformance is highly dependent on the choice of the feature pre-processing\nmethod. In this paper, we propose P2TAG, a framework designed for few-shot node\nclassification on TAGs with graph pre-training and prompting. P2TAG first\npre-trains the language model (LM) and graph neural network (GNN) on TAGs with\nself-supervised loss. To fully utilize the ability of language models, we adapt\nthe masked language modeling objective for our framework. The pre-trained model\nis then used for the few-shot node classification with a mixed prompt method,\nwhich simultaneously considers both text and graph information. We conduct\nexperiments on six real-world TAGs, including paper citation networks and\nproduct co-purchasing networks. Experimental results demonstrate that our\nproposed framework outperforms existing graph few-shot learning methods on\nthese datasets with +18.98% ~ +35.98% improvements.","authors":["Huanjing Zhao","Beining Yang","Yukuo Cen","Junyu Ren","Chenhui Zhang","Yuxiao Dong","Evgeny Kharlamov","Shu Zhao","Jie Tang"],"pdf_link":"http://arxiv.org/pdf/2407.15431v1","category":["LLMs","Datasets"]},{"title":"Future-Proofing Mobile Networks: A Digital Twin Approach to Multi-Signal Management","abstract":"Digital Twins (DTs) are set to become a key enabling technology in future\nwireless networks, with their use in network management increasing\nsignificantly. We developed a DT framework that leverages the heterogeneity of\nnetwork access technologies as a resource for enhanced network performance and\nmanagement, enabling smart data handling in the physical network. Tested in a\n\\textit{Campus Area Network} environment, our framework integrates diverse data\nsources to provide real-time, holistic insights into network performance and\nenvironmental sensing. We also envision that traditional analytics will evolve\nto rely on emerging AI models, such as Generative AI (GenAI), while leveraging\ncurrent analytics capabilities. This capacity can simplify analytics processes\nthrough advanced ML models, enabling descriptive, diagnostic, predictive, and\nprescriptive analytics in a unified fashion. Finally, we present specific\nresearch opportunities concerning interoperability aspects and envision\naligning advancements in DT technology with evolved AI integration.","authors":["Roberto Morabito","Bivek Pandey","Paulius Daubaris","Yasith R Wanigarathna","Sasu Tarkoma"],"pdf_link":"http://arxiv.org/pdf/2407.15520v1","category":["Datasets","Explainable AI"]},{"title":"Large-scale Time-Varying Portfolio Optimisation using Graph Attention Networks","abstract":"Apart from assessing individual asset performance, investors in financial\nmarkets also need to consider how a set of firms performs collectively as a\nportfolio. Whereas traditional Markowitz-based mean-variance portfolios are\nwidespread, network-based optimisation techniques have built upon these\ndevelopments. However, most studies do not contain firms at risk of default and\nremove any firms that drop off indices over a certain time. This is the first\nstudy to incorporate risky firms and use all the firms in portfolio\noptimisation. We propose and empirically test a novel method that leverages\nGraph Attention networks (GATs), a subclass of Graph Neural Networks (GNNs).\nGNNs, as deep learning-based models, can exploit network data to uncover\nnonlinear relationships. Their ability to handle high-dimensional features and\naccommodate customised layers for specific purposes makes them particularly\nappealing for large-scale problems such as mid- and small-cap portfolio\noptimization. This study utilises 30 years of data on mid-cap firms, creating\ngraphs of firms using distance correlation and the Triangulated Maximally\nFiltered Graph approach. These graphs are the inputs to a GAT model that we\ntrain using custom layers which impose weight and allocation constraints and a\nloss function derived from the Sharpe ratio, thus directly maximising portfolio\nrisk-adjusted returns. This new model is benchmarked against a network\ncharacteristic-based portfolio, a mean variance-based portfolio, and an\nequal-weighted portfolio. The results show that the portfolio produced by the\nGAT-based model outperforms all benchmarks and is consistently superior to\nother strategies over a long period while also being informative of market\ndynamics.","authors":["Kamesh Korangi","Christophe Mues","Cristi\u00e1n Bravo"],"pdf_link":"http://arxiv.org/pdf/2407.15532v1","category":["Reinforcement Learning","Explainable AI"]},{"title":"Discrete Flow Matching","abstract":"Despite Flow Matching and diffusion models having emerged as powerful\ngenerative paradigms for continuous variables such as images and videos, their\napplication to high-dimensional discrete data, such as language, is still\nlimited. In this work, we present Discrete Flow Matching, a novel discrete flow\nparadigm designed specifically for generating discrete data. Discrete Flow\nMatching offers several key contributions: (i) it works with a general family\nof probability paths interpolating between source and target distributions;\n(ii) it allows for a generic formula for sampling from these probability paths\nusing learned posteriors such as the probability denoiser ($x$-prediction) and\nnoise-prediction ($\\epsilon$-prediction); (iii) practically, focusing on\nspecific probability paths defined with different schedulers considerably\nimproves generative perplexity compared to previous discrete diffusion and flow\nmodels; and (iv) by scaling Discrete Flow Matching models up to 1.7B\nparameters, we reach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1\nand 20.6% Pass@10 on 1-shot MBPP coding benchmarks. Our approach is capable of\ngenerating high-quality discrete data in a non-autoregressive fashion,\nsignificantly closing the gap between autoregressive models and discrete flow\nmodels.","authors":["Itai Gat","Tal Remez","Neta Shaul","Felix Kreuk","Ricky T. Q. Chen","Gabriel Synnaeve","Yossi Adi","Yaron Lipman"],"pdf_link":"http://arxiv.org/pdf/2407.15595v1","category":["Speech Synthesis","Speech Recognition"]},{"title":"Sustainable broadcasting in Blockchain Network with Reinforcement Learning","abstract":"Recent estimates put the carbon footprint of Bitcoin and Ethereum at an\naverage of 64 and 26 million tonnes of CO2 per year, respectively. To address\nthis growing problem, several possible approaches have been proposed in the\nliterature: creating alternative blockchain consensus mechanisms, applying\nredundancy reduction techniques, utilizing renewable energy sources, and\nemploying energy-efficient devices, etc. In this paper, we follow the second\navenue and propose an efficient approach based on reinforcement learning that\nimproves the block broadcasting scheme in blockchain networks. The analysis and\nexperimental results confirmed that the proposed improvement of the block\npropagation scheme could cleverly handle network dynamics and achieve better\nresults than the default approach. Additionally, our technical integration of\nthe simulator and developed RL environment can be used as a complete solution\nfor further study of new schemes and protocols that use RL or other ML\ntechniques.","authors":["Danila Valko","Daniel Kudenko"],"pdf_link":"http://arxiv.org/pdf/2407.15616v1","category":["Reinforcement Learning","Explainable AI"]},{"title":"Evaluation of Reinforcement Learning for Autonomous Penetration Testing using A3C, Q-learning and DQN","abstract":"Penetration testing is the process of searching for security weaknesses by\nsimulating an attack. It is usually performed by experienced professionals,\nwhere scanning and attack tools are applied. By automating the execution of\nsuch tools, the need for human interaction and decision-making could be\nreduced. In this work, a Network Attack Simulator (NASim) was used as an\nenvironment to train reinforcement learning agents to solve three predefined\nsecurity scenarios. These scenarios cover techniques of exploitation,\npost-exploitation and wiretapping. A large hyperparameter grid search was\nperformed to find the best hyperparameter combinations. The algorithms\nQ-learning, DQN and A3C were used, whereby A3C was able to solve all scenarios\nand achieve generalization. In addition, A3C could solve these scenarios with\nfewer actions than the baseline automated penetration testing. Although the\ntraining was performed on rather small scenarios and with small state and\naction spaces for the agents, the results show that a penetration test can\nsuccessfully be performed by the RL agent.","authors":["Norman Becker","Daniel Reti","Evridiki V. Ntagiou","Marcus Wallum","Hans D. Schotten"],"pdf_link":"http://arxiv.org/pdf/2407.15656v1","category":["Reinforcement Learning","Benchmarking"]},{"title":"How to Shrink Confidence Sets for Many Equivalent Discrete Distributions?","abstract":"We consider the situation when a learner faces a set of unknown discrete\ndistributions $(p_k)_{k\\in \\mathcal K}$ defined over a common alphabet\n$\\mathcal X$, and can build for each distribution $p_k$ an individual\nhigh-probability confidence set thanks to $n_k$ observations sampled from\n$p_k$. The set $(p_k)_{k\\in \\mathcal K}$ is structured: each distribution $p_k$\nis obtained from the same common, but unknown, distribution q via applying an\nunknown permutation to $\\mathcal X$. We call this\n\\emph{permutation-equivalence}. The goal is to build refined confidence sets\n\\emph{exploiting} this structural property. Like other popular notions of\nstructure (Lipschitz smoothness, Linearity, etc.) permutation-equivalence\nnaturally appears in machine learning problems, and to benefit from its\npotential gain calls for a specific approach. We present a strategy to\neffectively exploit permutation-equivalence, and provide a finite-time\nhigh-probability bound on the size of the refined confidence sets output by the\nstrategy. Since a refinement is not possible for too few observations in\ngeneral, under mild technical assumptions, our finite-time analysis establish\nwhen the number of observations $(n_k)_{k\\in \\mathcal K}$ are large enough so\nthat the output confidence sets improve over initial individual sets. We\ncarefully characterize this event and the corresponding improvement. Further,\nour result implies that the size of confidence sets shrink at asymptotic rates\nof $O(1/\\sqrt{\\sum_{k\\in \\mathcal K} n_k})$ and $O(1/\\max_{k\\in K} n_{k})$,\nrespectively for elements inside and outside the support of q, when the size of\neach individual confidence set shrinks at respective rates of $O(1/\\sqrt{n_k})$\nand $O(1/n_k)$. We illustrate the practical benefit of exploiting permutation\nequivalence on a reinforcement learning task.","authors":["Odalric-Ambrym Maillard","Mohammad Sadegh Talebi"],"pdf_link":"http://arxiv.org/pdf/2407.15662v1","category":["Datasets","Multimodal Learning"]},{"title":"Problems in AI, their roots in philosophy, and implications for science and society","abstract":"Artificial Intelligence (AI) is one of today's most relevant emergent\ntechnologies. In view thereof, this paper proposes that more attention should\nbe paid to the philosophical aspects of AI technology and its use. It is argued\nthat this deficit is generally combined with philosophical misconceptions about\nthe growth of knowledge. To identify these misconceptions, reference is made to\nthe ideas of the philosopher of science Karl Popper and the physicist David\nDeutsch. The works of both thinkers aim against mistaken theories of knowledge,\nsuch as inductivism, empiricism, and instrumentalism. This paper shows that\nthese theories bear similarities to how current AI technology operates. It also\nshows that these theories are very much alive in the (public) discourse on AI,\noften called Bayesianism. In line with Popper and Deutsch, it is proposed that\nall these theories are based on mistaken philosophies of knowledge. This\nincludes an analysis of the implications of these mistaken philosophies for the\nuse of AI in science and society, including some of the likely problem\nsituations that will arise. This paper finally provides a realistic outlook on\nArtificial General Intelligence (AGI) and three propositions on A(G)I and\nphilosophy (i.e., epistemology).","authors":["Max Velthoven","Eric Marcus"],"pdf_link":"http://arxiv.org/pdf/2407.15671v1","category":["Explainable AI","AI in Healthcare"]},{"title":"AI-Driven Fast and Early Detection of IoT Botnet Threats: A Comprehensive Network Traffic Analysis Approach","abstract":"In the rapidly evolving landscape of cyber threats targeting the Internet of\nThings (IoT) ecosystem, and in light of the surge in botnet-driven Distributed\nDenial of Service (DDoS) and brute force attacks, this study focuses on the\nearly detection of IoT bots. It specifically addresses the detection of stealth\nbot communication that precedes and orchestrates attacks. This study proposes a\ncomprehensive methodology for analyzing IoT network traffic, including\nconsiderations for both unidirectional and bidirectional flow, as well as\npacket formats. It explores a wide spectrum of network features critical for\nrepresenting network traffic and characterizing benign IoT traffic patterns\neffectively. Moreover, it delves into the modeling of traffic using various\nsemi-supervised learning techniques. Through extensive experimentation with the\nIoT-23 dataset - a comprehensive collection featuring diverse botnet types and\ntraffic scenarios - we have demonstrated the feasibility of detecting botnet\ntraffic corresponding to different operations and types of bots, specifically\nfocusing on stealth command and control (C2) communications. The results\nobtained have demonstrated the feasibility of identifying C2 communication with\na 100% success rate through packet-based methods and 94% via flow based\napproaches, with a false positive rate of 1.53%.","authors":["Abdelaziz Amara korba","Aleddine Diaf","Yacine Ghamri-Doudane"],"pdf_link":"http://arxiv.org/pdf/2407.15688v1","category":["Datasets","Explainable AI"]},{"title":"TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON","abstract":"TaskGen is an open-sourced agentic framework which uses an Agent to solve an\narbitrary task by breaking them down into subtasks. Each subtask is mapped to\nan Equipped Function or another Agent to execute. In order to reduce verbosity\n(and hence token usage), TaskGen uses StrictJSON that ensures JSON output from\nthe Large Language Model (LLM), along with additional features such as type\nchecking and iterative error correction. Key to the philosophy of TaskGen is\nthe management of information/memory on a need-to-know basis. We empirically\nevaluate TaskGen on various environments such as 40x40 dynamic maze navigation\nwith changing obstacle locations (100% solve rate), TextWorld escape room\nsolving with dense rewards and detailed goals (96% solve rate), web browsing\n(69% of actions successful), solving the MATH dataset (71% solve rate over 100\nLevel-5 problems), Retrieval Augmented Generation on NaturalQuestions dataset\n(F1 score of 47.03%)","authors":["John Chong Min Tan","Prince Saroj","Bharat Runwal","Hardik Maheshwari","Brian Lim Yi Sheng","Richard Cottrill","Alankrit Chona","Ambuj Kumar","Mehul Motani"],"pdf_link":"http://arxiv.org/pdf/2407.15734v1","category":["Explainable AI","Reinforcement Learning"]},{"title":"MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval Augmented Generation","abstract":"In this paper, we introduce MoRSE (Mixture of RAGs Security Experts), the\nfirst specialised AI chatbot for cybersecurity. MoRSE aims to provide\ncomprehensive and complete knowledge about cybersecurity. MoRSE uses two RAG\n(Retrieval Augmented Generation) systems designed to retrieve and organize\ninformation from multidimensional cybersecurity contexts. MoRSE differs from\ntraditional RAGs by using parallel retrievers that work together to retrieve\nsemantically related information in different formats and structures. Unlike\ntraditional Large Language Models (LLMs) that rely on Parametric Knowledge\nBases, MoRSE retrieves relevant documents from Non-Parametric Knowledge Bases\nin response to user queries. Subsequently, MoRSE uses this information to\ngenerate accurate answers. In addition, MoRSE benefits from real-time updates\nto its knowledge bases, enabling continuous knowledge enrichment without\nretraining. We have evaluated the effectiveness of MoRSE against other\nstate-of-the-art LLMs, evaluating the system on 600 cybersecurity specific\nquestions. The experimental evaluation has shown that the improvement in terms\nof relevance and correctness of the answer is more than 10\\% compared to known\nsolutions such as GPT-4 and Mixtral 7x8.","authors":["Marco Simoni","Andrea Saracino","Vinod P.","Mauro Conti"],"pdf_link":"http://arxiv.org/pdf/2407.15748v1","category":["Speech Recognition","Speech Synthesis"]},{"title":"Model editing for distribution shifts in uranium oxide morphological analysis","abstract":"Deep learning still struggles with certain kinds of scientific data. Notably,\npretraining data may not provide coverage of relevant distribution shifts\n(e.g., shifts induced via the use of different measurement instruments). We\nconsider deep learning models trained to classify the synthesis conditions of\nuranium ore concentrates (UOCs) and show that model editing is particularly\neffective for improving generalization to distribution shifts common in this\ndomain. In particular, model editing outperforms finetuning on two curated\ndatasets comprising of micrographs taken of U$_{3}$O$_{8}$ aged in humidity\nchambers and micrographs acquired with different scanning electron microscopes,\nrespectively.","authors":["Davis Brown","Cody Nizinski","Madelyn Shapiro","Corey Fallon","Tianzhixi Yin","Henry Kvinge","Jonathan H. Tu"],"pdf_link":"http://arxiv.org/pdf/2407.15756v1","category":["Datasets","Explainable AI"]},{"title":"Diffusion Model Based Resource Allocation Strategy in Ultra-Reliable Wireless Networked Control Systems","abstract":"Diffusion models are vastly used in generative AI, leveraging their\ncapability to capture complex data distributions. However, their potential\nremains largely unexplored in the field of resource allocation in wireless\nnetworks. This paper introduces a novel diffusion model-based resource\nallocation strategy for Wireless Networked Control Systems (WNCSs) with the\nobjective of minimizing total power consumption through the optimization of the\nsampling period in the control system, and blocklength and packet error\nprobability in the finite blocklength regime of the communication system. The\nproblem is first reduced to the optimization of blocklength only based on the\nderivation of the optimality conditions. Then, the optimization theory solution\ncollects a dataset of channel gains and corresponding optimal blocklengths.\nFinally, the Denoising Diffusion Probabilistic Model (DDPM) uses this collected\ndataset to train the resource allocation algorithm that generates optimal\nblocklength values conditioned on the channel state information (CSI). Via\nextensive simulations, the proposed approach is shown to outperform previously\nproposed Deep Reinforcement Learning (DRL) based approaches with close to\noptimal performance regarding total power consumption. Moreover, an improvement\nof up to eighteen-fold in the reduction of critical constraint violations is\nobserved, further underscoring the accuracy of the solution.","authors":["Amirhassan Babazadeh Darabi","Sinem Coleri"],"pdf_link":"http://arxiv.org/pdf/2407.15784v1","category":["Reinforcement Learning","Explainable AI"]},{"title":"On shallow planning under partial observability","abstract":"Formulating a real-world problem under the Reinforcement Learning framework\ninvolves non-trivial design choices, such as selecting a discount factor for\nthe learning objective (discounted cumulative rewards), which articulates the\nplanning horizon of the agent. This work investigates the impact of the\ndiscount factor on the biasvariance trade-off given structural parameters of\nthe underlying Markov Decision Process. Our results support the idea that a\nshorter planning horizon might be beneficial, especially under partial\nobservability.","authors":["Randy Lefebvre","Audrey Durand"],"pdf_link":"http://arxiv.org/pdf/2407.15820v1","category":["Reinforcement Learning","Explainable AI"]},{"title":"NV-Retriever: Improving text embedding models with effective hard-negative mining","abstract":"Text embedding models have been popular for information retrieval\napplications such as semantic search and Question-Answering systems based on\nRetrieval-Augmented Generation (RAG). Those models are typically Transformer\nmodels that are fine-tuned with contrastive learning objectives. Many papers\nintroduced new embedding model architectures and training approaches, however,\none of the key ingredients, the process of mining negative passages, remains\npoorly explored or described. One of the challenging aspects of fine-tuning\nembedding models is the selection of high quality hard-negative passages for\ncontrastive learning. In this paper we propose a family of positive-aware\nmining methods that leverage the positive relevance score for more effective\nfalse negatives removal. We also provide a comprehensive ablation study on\nhard-negative mining methods over their configurations, exploring different\nteacher and base models. We demonstrate the efficacy of our proposed methods by\nintroducing the NV-Retriever-v1 model, which scores 60.9 on MTEB Retrieval\n(BEIR) benchmark and 0.65 points higher than previous methods. The model placed\n1st when it was published to MTEB Retrieval on July 07, 2024.","authors":["Gabriel de Souza P. Moreira","Radek Osmulski","Mengyao Xu","Ronay Ak","Benedikt Schifferer","Even Oldridge"],"pdf_link":"http://arxiv.org/pdf/2407.15831v1","category":["LLMs","Speech Recognition"]},{"title":"LLMmap: Fingerprinting For Large Language Models","abstract":"We introduce LLMmap, a first-generation fingerprinting attack targeted at\nLLM-integrated applications. LLMmap employs an active fingerprinting approach,\nsending carefully crafted queries to the application and analyzing the\nresponses to identify the specific LLM model in use. With as few as 8\ninteractions, LLMmap can accurately identify LLMs with over 95% accuracy. More\nimportantly, LLMmap is designed to be robust across different application\nlayers, allowing it to identify LLMs operating under various system prompts,\nstochastic sampling hyperparameters, and even complex generation frameworks\nsuch as RAG or Chain-of-Thought.","authors":["Dario Pasquini","Evgenios M. Kornaropoulos","Giuseppe Ateniese"],"pdf_link":"http://arxiv.org/pdf/2407.15847v1","category":["Explainable AI","Benchmarking"]}]