[{"title":"Harnessing the Power of LLMs in Source Code Vulnerability Detection","abstract":"Software vulnerabilities, caused by unintentional flaws in source code, are a\nprimary root cause of cyberattacks. Static analysis of source code has been\nwidely used to detect these unintentional defects introduced by software\ndevelopers. Large Language Models (LLMs) have demonstrated human-like\nconversational abilities due to their capacity to capture complex patterns in\nsequential data, such as natural languages. In this paper, we harness LLMs'\ncapabilities to analyze source code and detect known vulnerabilities. To ensure\nthe proposed vulnerability detection method is universal across multiple\nprogramming languages, we convert source code to LLVM IR and train LLMs on\nthese intermediate representations. We conduct extensive experiments on various\nLLM architectures and compare their accuracy. Our comprehensive experiments on\nreal-world and synthetic codes from NVD and SARD demonstrate high accuracy in\nidentifying source code vulnerabilities.","authors":["Andrew A Mahyari"],"pdf_link":"http://arxiv.org/pdf/2408.03489v1","category":["Speech Recognition","LLMs"]},{"title":"Advanced User Credit Risk Prediction Model using LightGBM, XGBoost and Tabnet with SMOTEENN","abstract":"Bank credit risk is a significant challenge in modern financial transactions,\nand the ability to identify qualified credit card holders among a large number\nof applicants is crucial for the profitability of a bank'sbank's credit card\nbusiness. In the past, screening applicants'applicants' conditions often\nrequired a significant amount of manual labor, which was time-consuming and\nlabor-intensive. Although the accuracy and reliability of previously used ML\nmodels have been continuously improving, the pursuit of more reliable and\npowerful AI intelligent models is undoubtedly the unremitting pursuit by major\nbanks in the financial industry. In this study, we used a dataset of over\n40,000 records provided by a commercial bank as the research object. We\ncompared various dimensionality reduction techniques such as PCA and T-SNE for\npreprocessing high-dimensional datasets and performed in-depth adaptation and\ntuning of distributed models such as LightGBM and XGBoost, as well as deep\nmodels like Tabnet. After a series of research and processing, we obtained\nexcellent research results by combining SMOTEENN with these techniques. The\nexperiments demonstrated that LightGBM combined with PCA and SMOTEENN\ntechniques can assist banks in accurately predicting potential high-quality\ncustomers, showing relatively outstanding performance compared to other models.","authors":["Chang Yu","Yixin Jin","Qianwen Xing","Ye Zhang","Shaobo Guo","Shuchen Meng"],"pdf_link":"http://arxiv.org/pdf/2408.03497v1","category":["Datasets","AI in Healthcare"]},{"title":"RepoMasterEval: Evaluating Code Completion via Real-World Repositories","abstract":"With the growing reliance on automated code completion tools in software\ndevelopment, the need for robust evaluation benchmarks has become critical.\nHowever, existing benchmarks focus more on code generation tasks in function\nand class level and provide rich text description to prompt the model. By\ncontrast, such descriptive prompt is commonly unavailable in real development\nand code completion can occur in wider range of situations such as in the\nmiddle of a function or a code block. These limitations makes the evaluation\npoorly align with the practical scenarios of code completion tools. In this\npaper, we propose RepoMasterEval, a novel benchmark for evaluating code\ncompletion models constructed from real-world Python and TypeScript\nrepositories. Each benchmark datum is generated by masking a code snippet\n(ground truth) from one source code file with existing test suites. To improve\ntest accuracy of model generated code, we employ mutation testing to measure\nthe effectiveness of the test cases and we manually crafted new test cases for\nthose test suites with low mutation score. Our empirical evaluation on 6\nstate-of-the-art models shows that test argumentation is critical in improving\nthe accuracy of the benchmark and RepoMasterEval is able to report difference\nin model performance in real-world scenarios. The deployment of RepoMasterEval\nin a collaborated company for one month also revealed that the benchmark is\nuseful to give accurate feedback during model training and the score is in high\ncorrelation with the model's performance in practice. Based on our findings, we\ncall for the software engineering community to build more LLM benchmarks\ntailored for code generation tools taking the practical and complex development\nenvironment into consideration.","authors":["Qinyun Wu","Chao Peng","Pengfei Gao","Ruida Hu","Haoyu Gan","Bo Jiang","Jinhe Tang","Zhiwen Deng","Zhanming Guan","Cuiyun Gao","Xia Liu","Ping Yang"],"pdf_link":"http://arxiv.org/pdf/2408.03519v1","category":["Benchmarking","LLMs"]},{"title":"Exploring the extent of similarities in software failures across industries using LLMs","abstract":"The rapid evolution of software development necessitates enhanced safety\nmeasures. Extracting information about software failures from companies is\nbecoming increasingly more available through news articles.\n  This research utilizes the Failure Analysis Investigation with LLMs (FAIL)\nmodel to extract industry-specific information. Although the FAIL model's\ndatabase is rich in information, it could benefit from further categorization\nand industry-specific insights to further assist software engineers.\n  In previous work news articles were collected from reputable sources and\ncategorized by incidents inside a database. Prompt engineering and Large\nLanguage Models (LLMs) were then applied to extract relevant information\nregarding the software failure. This research extends these methods by\ncategorizing articles into specific domains and types of software failures. The\nresults are visually represented through graphs.\n  The analysis shows that throughout the database some software failures occur\nsignificantly more often in specific industries. This categorization provides a\nvaluable resource for software engineers and companies to identify and address\ncommon failures.\n  This research highlights the synergy between software engineering and Large\nLanguage Models (LLMs) to automate and enhance the analysis of software\nfailures. By transforming data from the database into an industry specific\nmodel, we provide a valuable resource that can be used to identify common\nvulnerabilities, predict potential risks, and implement proactive measures for\npreventing software failures. Leveraging the power of the current FAIL database\nand data visualization, we aim to provide an avenue for safer and more secure\nsoftware in the future.","authors":["Martin Detloff"],"pdf_link":"http://arxiv.org/pdf/2408.03528v2","category":["Datasets","AI in Healthcare"]},{"title":"Lifelong Personalized Low-Rank Adaptation of Large Language Models for Recommendation","abstract":"We primarily focus on the field of large language models (LLMs) for\nrecommendation, which has been actively explored recently and poses a\nsignificant challenge in effectively enhancing recommender systems with logical\nreasoning abilities and open-world knowledge. Current mainstream efforts mainly\ncenter around injecting personalized information from recommendation models\ninto LLMs by customizing input templates or aligning representations between\nsemantic and recommendation spaces at the prediction layer. However, they face\nthree significant limitations: (1) LoRA is mostly used as a core component in\nexisting works, but personalization is not well established in LoRA parameters\nas the LoRA matrix shared by every user may not cater to different users'\ncharacteristics, leading to suboptimal performance. (2) Although lifelong\npersonalized behavior sequences are ideal for personalization, their use raises\neffectiveness and efficiency issues since LLMs require escalating training and\ninference time to extend text lengths. (3) Existing approaches aren't scalable\nfor large datasets due to training efficiency constraints. Thus, LLMs only see\na small fraction of the datasets (e.g., less than 10%) instead of the whole\ndatasets, limiting their exposure to the full training space. To address these\nproblems, we propose RecLoRA. This model incorporates a Personalized LoRA\nmodule that maintains independent LoRAs for different users and a Long-Short\nModality Retriever that retrieves different history lengths for different\nmodalities, significantly improving performance while adding minimal time cost.\nFurthermore, we design a Few2Many Learning Strategy, using a conventional\nrecommendation model as a lens to magnify small training spaces to full spaces.\nExtensive experiments on public datasets demonstrate the efficacy of our\nRecLoRA compared to existing baseline models.","authors":["Jiachen Zhu","Jianghao Lin","Xinyi Dai","Bo Chen","Rong Shan","Jieming Zhu","Ruiming Tang","Yong Yu","Weinan Zhang"],"pdf_link":"http://arxiv.org/pdf/2408.03533v1","category":["LLMs","Speech Recognition"]},{"title":"2D-OOB: Attributing Data Contribution through Joint Valuation Framework","abstract":"Data valuation has emerged as a powerful framework to quantify the\ncontribution of each datum to the training of a particular machine learning\nmodel. However, it is crucial to recognize that the quality of various cells\nwithin a single data point can vary greatly in practice. For example, even in\nthe case of an abnormal data point, not all cells are necessarily noisy. The\nsingle scalar valuation assigned by existing methods blurs the distinction\nbetween noisy and clean cells of a data point, thereby compromising the\ninterpretability of the valuation. In this paper, we propose 2D-OOB, an\nout-of-bag estimation framework for jointly determining helpful (or\ndetrimental) samples, as well as the particular cells that drive them. Our\ncomprehensive experiments demonstrate that 2D-OOB achieves state-of-the-art\nperformance across multiple use cases, while being exponentially faster. 2D-OOB\nexcels in detecting and rectifying fine-grained outliers at the cell level, as\nwell as localizing backdoor triggers in data poisoning attacks.","authors":["Yifan Sun","Jingyan Shen","Yongchan Kwon"],"pdf_link":"http://arxiv.org/pdf/2408.03572v1","category":["Datasets","Explainable AI"]},{"title":"Hierarchical Neural Constructive Solver for Real-world TSP Scenarios","abstract":"Existing neural constructive solvers for routing problems have predominantly\nemployed transformer architectures, conceptualizing the route construction as a\nset-to-sequence learning task. However, their efficacy has primarily been\ndemonstrated on entirely random problem instances that inadequately capture\nreal-world scenarios. In this paper, we introduce realistic Traveling Salesman\nProblem (TSP) scenarios relevant to industrial settings and derive the\nfollowing insights: (1) The optimal next node (or city) to visit often lies\nwithin proximity to the current node, suggesting the potential benefits of\nbiasing choices based on current locations. (2) Effectively solving the TSP\nrequires robust tracking of unvisited nodes and warrants succinct grouping\nstrategies. Building upon these insights, we propose integrating a learnable\nchoice layer inspired by Hypernetworks to prioritize choices based on the\ncurrent location, and a learnable approximate clustering algorithm inspired by\nthe Expectation-Maximization algorithm to facilitate grouping the unvisited\ncities. Together, these two contributions form a hierarchical approach towards\nsolving the realistic TSP by considering both immediate local neighbourhoods\nand learning an intermediate set of node representations. Our hierarchical\napproach yields superior performance compared to both classical and recent\ntransformer models, showcasing the efficacy of the key designs.","authors":["Yong Liang Goh","Zhiguang Cao","Yining Ma","Yanfei Dong","Mohammed Haroon Dupty","Wee Sun Lee"],"pdf_link":"http://arxiv.org/pdf/2408.03585v1","category":["Reinforcement Learning","Explainable AI"]},{"title":"Facing the Music: Tackling Singing Voice Separation in Cinematic Audio Source Separation","abstract":"Cinematic audio source separation (CASS) is a fairly new subtask of audio\nsource separation. A typical setup of CASS is a three-stem problem, with the\naim of separating the mixture into the dialogue stem (DX), music stem (MX), and\neffects stem (FX). In practice, however, several edge cases exist as some sound\nsources do not fit neatly in either of these three stems, necessitating the use\nof additional auxiliary stems in production. One very common edge case is the\nsinging voice in film audio, which may belong in either the DX or MX, depending\nheavily on the cinematic context. In this work, we demonstrate a very\nstraightforward extension of the dedicated-decoder Bandit and query-based\nsingle-decoder Banquet models to a four-stem problem, treating non-musical\ndialogue, instrumental music, singing voice, and effects as separate stems.\nInterestingly, the query-based Banquet model outperformed the dedicated-decoder\nBandit model. We hypothesized that this is due to a better feature alignment at\nthe bottleneck as enforced by the band-agnostic FiLM layer. Dataset and model\nimplementation will be made available at\nhttps://github.com/kwatcharasupat/source-separation-landing.","authors":["Karn N. Watcharasupat","Chih-Wei Wu","Iroro Orife"],"pdf_link":"http://arxiv.org/pdf/2408.03588v1","category":["Speech Recognition","Speech Synthesis"]},{"title":"Enhancing Output Diversity Improves Conjugate Gradient-based Adversarial Attacks","abstract":"Deep neural networks are vulnerable to adversarial examples, and adversarial\nattacks that generate adversarial examples have been studied in this context.\nExisting studies imply that increasing the diversity of model outputs\ncontributes to improving the attack performance. This study focuses on the Auto\nConjugate Gradient (ACG) attack, which is inspired by the conjugate gradient\nmethod and has a high diversification performance. We hypothesized that\nincreasing the distance between two consecutive search points would enhance the\noutput diversity. To test our hypothesis, we propose Rescaling-ACG (ReACG),\nwhich automatically modifies the two components that significantly affect the\ndistance between two consecutive search points, including the search direction\nand step size. ReACG showed higher attack performance than that of ACG, and is\nparticularly effective for ImageNet models with several classification classes.\nExperimental results show that the distance between two consecutive search\npoints enhances the output diversity and may help develop new potent attacks.\nThe code is available at \\url{https://github.com/yamamura-k/ReACG}","authors":["Keiichiro Yamamura","Issa Oe","Hiroki Ishikura","Katsuki Fujisawa"],"pdf_link":"http://arxiv.org/pdf/2408.03972v1","category":["Explainable AI","Multimodal Learning"]},{"title":"HiQuE: Hierarchical Question Embedding Network for Multimodal Depression Detection","abstract":"The utilization of automated depression detection significantly enhances\nearly intervention for individuals experiencing depression. Despite numerous\nproposals on automated depression detection using recorded clinical interview\nvideos, limited attention has been paid to considering the hierarchical\nstructure of the interview questions. In clinical interviews for diagnosing\ndepression, clinicians use a structured questionnaire that includes routine\nbaseline questions and follow-up questions to assess the interviewee's\ncondition. This paper introduces HiQuE (Hierarchical Question Embedding\nnetwork), a novel depression detection framework that leverages the\nhierarchical relationship between primary and follow-up questions in clinical\ninterviews. HiQuE can effectively capture the importance of each question in\ndiagnosing depression by learning mutual information across multiple\nmodalities. We conduct extensive experiments on the widely-used clinical\ninterview data, DAIC-WOZ, where our model outperforms other state-of-the-art\nmultimodal depression detection models and emotion recognition models,\nshowcasing its clinical utility in depression detection.","authors":["Juho Jung","Chaewon Kang","Jeewoo Yoon","Seungbae Kim","Jinyoung Han"],"pdf_link":"http://arxiv.org/pdf/2408.03648v1","category":["AI in Healthcare","Multimodal Learning"]},{"title":"Generative Design of Periodic Orbits in the Restricted Three-Body Problem","abstract":"The Three-Body Problem has fascinated scientists for centuries and it has\nbeen crucial in the design of modern space missions. Recent developments in\nGenerative Artificial Intelligence hold transformative promise for addressing\nthis longstanding problem. This work investigates the use of Variational\nAutoencoder (VAE) and its internal representation to generate periodic orbits.\nWe utilize a comprehensive dataset of periodic orbits in the Circular\nRestricted Three-Body Problem (CR3BP) to train deep-learning architectures that\ncapture key orbital characteristics, and we set up physical evaluation metrics\nfor the generated trajectories. Through this investigation, we seek to enhance\nthe understanding of how Generative AI can improve space mission planning and\nastrodynamics research, leading to novel, data-driven approaches in the field.","authors":["Alvaro Francisco Gil","Walther Litteri","Victor Rodriguez-Fernandez","David Camacho","Massimiliano Vasile"],"pdf_link":"http://arxiv.org/pdf/2408.03691v1","category":["Explainable AI","Reinforcement Learning"]},{"title":"Online Model-based Anomaly Detection in Multivariate Time Series: Taxonomy, Survey, Research Challenges and Future Directions","abstract":"Time-series anomaly detection plays an important role in engineering\nprocesses, like development, manufacturing and other operations involving\ndynamic systems. These processes can greatly benefit from advances in the\nfield, as state-of-the-art approaches may aid in cases involving, for example,\nhighly dimensional data. To provide the reader with understanding of the\nterminology, this survey introduces a novel taxonomy where a distinction\nbetween online and offline, and training and inference is made. Additionally,\nit presents the most popular data sets and evaluation metrics used in the\nliterature, as well as a detailed analysis. Furthermore, this survey provides\nan extensive overview of the state-of-the-art model-based online semi- and\nunsupervised anomaly detection approaches for multivariate time-series data,\ncategorising them into different model families and other properties. The\nbiggest research challenge revolves around benchmarking, as currently there is\nno reliable way to compare different approaches against one another. This\nproblem is two-fold: on the one hand, public data sets suffers from at least\none fundamental flaw, while on the other hand, there is a lack of intuitive and\nrepresentative evaluation metrics in the field. Moreover, the way most\npublications choose a detection threshold disregards real-world conditions,\nwhich hinders the application in the real world. To allow for tangible advances\nin the field, these issues must be addressed in future work.","authors":["Lucas Correia","Jan-Christoph Goos","Philipp Klein","Thomas B\u00e4ck","Anna V. Kononova"],"pdf_link":"http://arxiv.org/pdf/2408.03747v1","category":["Datasets","Explainable AI"]},{"title":"Relevance meets Diversity: A User-Centric Framework for Knowledge Exploration through Recommendations","abstract":"Providing recommendations that are both relevant and diverse is a key\nconsideration of modern recommender systems. Optimizing both of these measures\npresents a fundamental trade-off, as higher diversity typically comes at the\ncost of relevance, resulting in lower user engagement. Existing recommendation\nalgorithms try to resolve this trade-off by combining the two measures,\nrelevance and diversity, into one aim and then seeking recommendations that\noptimize the combined objective, for a given number of items to recommend.\nTraditional approaches, however, do not consider the user interaction with the\nrecommended items.\n  In this paper, we put the user at the central stage, and build on the\ninterplay between relevance, diversity, and user behavior. In contrast to\napplications where the goal is solely to maximize engagement, we focus on\nscenarios aiming at maximizing the total amount of knowledge encountered by the\nuser. We use diversity as a surrogate of the amount of knowledge obtained by\nthe user while interacting with the system, and we seek to maximize diversity.\nWe propose a probabilistic user-behavior model in which users keep interacting\nwith the recommender system as long as they receive relevant recommendations,\nbut they may stop if the relevance of the recommended items drops. Thus, for a\nrecommender system to achieve a high-diversity measure, it will need to produce\nrecommendations that are both relevant and diverse.\n  Finally, we propose a novel recommendation strategy that combines relevance\nand diversity by a copula function. We conduct an extensive evaluation of the\nproposed methodology over multiple datasets, and we show that our strategy\noutperforms several state-of-the-art competitors. Our implementation is\npublicly available at https://github.com/EricaCoppolillo/EXPLORE.","authors":["Erica Coppolillo","Giuseppe Manco","Aristides Gionis"],"pdf_link":"http://arxiv.org/pdf/2408.03772v1","category":["Reinforcement Learning","Multimodal Learning"]},{"title":"Learning from Noisy Labels for Long-tailed Data via Optimal Transport","abstract":"Noisy labels, which are common in real-world datasets, can significantly\nimpair the training of deep learning models. However, recent adversarial\nnoise-combating methods overlook the long-tailed distribution of real data,\nwhich can significantly harm the effect of denoising strategies. Meanwhile, the\nmismanagement of noisy labels further compromises the model's ability to handle\nlong-tailed data. To tackle this issue, we propose a novel approach to manage\ndata characterized by both long-tailed distributions and noisy labels. First,\nwe introduce a loss-distance cross-selection module, which integrates class\npredictions and feature distributions to filter clean samples, effectively\naddressing uncertainties introduced by noisy labels and long-tailed\ndistributions. Subsequently, we employ optimal transport strategies to generate\npseudo-labels for the noise set in a semi-supervised training manner, enhancing\npseudo-label quality while mitigating the effects of sample scarcity caused by\nthe long-tailed distribution. We conduct experiments on both synthetic and\nreal-world datasets, and the comprehensive experimental results demonstrate\nthat our method surpasses current state-of-the-art methods. Our code will be\navailable in the future.","authors":["Mengting Li","Chuang Zhu"],"pdf_link":"http://arxiv.org/pdf/2408.03977v1","category":["Datasets","Explainable AI"]},{"title":"Frank's triangular norms in Piaget's logical proportions","abstract":"Starting from the Boolean notion of logical proportion in Piaget's sense,\nwhich turns out to be equivalent to analogical proportion, this note proposes a\ndefinition of analogical proportion between numerical values based on\ntriangular norms (and dual co-norms). Frank's family of triangular norms is\nparticularly interesting from this perspective. The article concludes with a\ncomparative discussion with another very recent proposal for defining\nanalogical proportions between numerical values based on the family of\ngeneralized means.","authors":["Henri Prade","Gilles Richard"],"pdf_link":"http://arxiv.org/pdf/2408.03795v1","category":["Multimodal Learning","Benchmarking"]},{"title":"Automated Code Fix Suggestions for Accessibility Issues in Mobile Apps","abstract":"Accessibility is crucial for inclusive app usability, yet developers often\nstruggle to identify and fix app accessibility issues due to a lack of\nawareness, expertise, and inadequate tools. Current accessibility testing tools\ncan identify accessibility issues but may not always provide guidance on how to\naddress them. We introduce FixAlly, an automated tool designed to suggest\nsource code fixes for accessibility issues detected by automated accessibility\nscanners. FixAlly employs a multi-agent LLM architecture to generate fix\nstrategies, localize issues within the source code, and propose code\nmodification suggestions to fix the accessibility issue. Our empirical study\ndemonstrates FixAlly's capability in suggesting fixes that resolve issues found\nby accessibility scanners -- with an effectiveness of 77% in generating\nplausible fix suggestions -- and our survey of 12 iOS developers finds they\nwould be willing to accept 69.4% of evaluated fix suggestions.","authors":["Forough Mehralian","Titus Barik","Jeff Nichols","Amanda Swearngin"],"pdf_link":"http://arxiv.org/pdf/2408.03827v1","category":["Speech Recognition","AI in Healthcare"]},{"title":"MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models","abstract":"The application of large language models to facilitate automated software\noperations and tool generation (SOTG), thus augmenting software productivity,\nmirrors the early stages of human evolution when the ability to create and use\ntools accelerated the progress of civilization. These complex tasks require AI\nto continuously summarize and improve. Current research often overlooks the\nimportance of converting real-time task experiences into system memory and\ndifferentiating the value of existing knowledge for future reference. This\npaper addresses these issues by evolving external memory models into\nMemory-Loop Networks for timely memorization and experience referencing. We\nalso enhance a RAG mechanism with knowledge precision segmentation to utilize\nmemory based on value differentiation, and design the MaxMind model for SOTG\naccordingly.To demonstrate our approach, we developed MaxMind4Sheet, an\nelectronic spreadsheet processing system aligned with the MaxMind philosophy.\nComparative experiments with SheetCopilot have demonstrated that the\naccumulation and recycling of task memories lead to a steady enhancement in\ntask success rate, with an improvement rate of approximately 3%-6% per round in\nthis implementation example. Note that as the memories continue to grow, this\ncumulative improvement may be substantial. The inclusion of memory recycling\ncan also boost the system's task execution efficiency by up to 25%, and it can\naddress the retraining issue faced by LLMs when handling specialized tasks\nthrough memories transfer.These suggest that MaxMind has significant potential\nto enhance the capabilities and productivity of LLM systems in SOTG.","authors":["Yuchen Dong","XiaoXiang Fang","Yuchen Hu","Renshuang Jiang","Zhe Jiang"],"pdf_link":"http://arxiv.org/pdf/2408.03841v1","category":["Speech Recognition","Speech Synthesis"]},{"title":"Inter-Series Transformer: Attending to Products in Time Series Forecasting","abstract":"Time series forecasting is an important task in many fields ranging from\nsupply chain management to weather forecasting. Recently, Transformer neural\nnetwork architectures have shown promising results in forecasting on common\ntime series benchmark datasets. However, application to supply chain demand\nforecasting, which can have challenging characteristics such as sparsity and\ncross-series effects, has been limited.\n  In this work, we explore the application of Transformer-based models to\nsupply chain demand forecasting. In particular, we develop a new\nTransformer-based forecasting approach using a shared, multi-task per-time\nseries network with an initial component applying attention across time series,\nto capture interactions and help address sparsity. We provide a case study\napplying our approach to successfully improve demand prediction for a medical\ndevice manufacturing company. To further validate our approach, we also apply\nit to public demand forecasting datasets as well and demonstrate competitive to\nsuperior performance compared to a variety of baseline and state-of-the-art\nforecast methods across the private and public datasets.","authors":["Rares Cristian","Pavithra Harsha","Clemente Ocejo","Georgia Perakis","Brian Quanz","Ioannis Spantidakis","Hamza Zerhouni"],"pdf_link":"http://arxiv.org/pdf/2408.03872v1","category":["Reinforcement Learning","Datasets"]},{"title":"Knowledge Probing for Graph Representation Learning","abstract":"Graph learning methods have been extensively applied in diverse application\nareas. However, what kind of inherent graph properties e.g. graph proximity,\ngraph structural information has been encoded into graph representation\nlearning for downstream tasks is still under-explored. In this paper, we\npropose a novel graph probing framework (GraphProbe) to investigate and\ninterpret whether the family of graph learning methods has encoded different\nlevels of knowledge in graph representation learning. Based on the intrinsic\nproperties of graphs, we design three probes to systematically investigate the\ngraph representation learning process from different perspectives, respectively\nthe node-wise level, the path-wise level, and the structural level. We\nconstruct a thorough evaluation benchmark with nine representative graph\nlearning methods from random walk based approaches, basic graph neural networks\nand self-supervised graph methods, and probe them on six benchmark datasets for\nnode classification, link prediction and graph classification. The experimental\nevaluation verify that GraphProbe can estimate the capability of graph\nrepresentation learning. Remaking results have been concluded: GCN and\nWeightedGCN methods are relatively versatile methods achieving better results\nwith respect to different tasks.","authors":["Mingyu Zhao","Xingyu Huang","Ziyu Lyu","Yanlin Wang","Lixin Cui","Lu Bai"],"pdf_link":"http://arxiv.org/pdf/2408.03877v1","category":["Datasets","Explainable AI"]},{"title":"Learning Rate-Free Reinforcement Learning: A Case for Model Selection with Non-Stationary Objectives","abstract":"The performance of reinforcement learning (RL) algorithms is sensitive to the\nchoice of hyperparameters, with the learning rate being particularly\ninfluential. RL algorithms fail to reach convergence or demand an extensive\nnumber of samples when the learning rate is not optimally set. In this work, we\nshow that model selection can help to improve the failure modes of RL that are\ndue to suboptimal choices of learning rate. We present a model selection\nframework for Learning Rate-Free Reinforcement Learning that employs model\nselection methods to select the optimal learning rate on the fly. This approach\nof adaptive learning rate tuning neither depends on the underlying RL algorithm\nnor the optimizer and solely uses the reward feedback to select the learning\nrate; hence, the framework can input any RL algorithm and produce a learning\nrate-free version of it. We conduct experiments for policy optimization methods\nand evaluate various model selection strategies within our framework. Our\nresults indicate that data-driven model selection algorithms are better\nalternatives to standard bandit algorithms when the optimal choice of\nhyperparameter is time-dependent and non-stationary.","authors":["Aida Afshar","Aldo Pacchiano"],"pdf_link":"http://arxiv.org/pdf/2408.04046v1","category":["Reinforcement Learning","Explainable AI"]},{"title":"Machine Learning-Based Reward-Driven Tuning of Scanning Probe Microscopy: Towards Fully Automated Microscopy","abstract":"Since the dawn of scanning probe microscopy (SPM), tapping or intermittent\ncontact mode has been one of the most widely used imaging modes. Manual\noptimization of tapping mode not only takes a lot of instrument and operator\ntime, but also often leads to frequent probe and sample damage, poor image\nquality and reproducibility issues for new types of samples or inexperienced\nusers. Despite wide use, optimization of tapping mode imaging is an extremely\nhard problem, ill-suited to either classical control methods or machine\nlearning. Here we introduce a reward-driven workflow to automate the\noptimization of SPM in the tapping mode. The reward function is defined based\non multiple channels with physical and empirical knowledge of good scans\nencoded, representing a sample-agnostic measure of image quality and imitating\nthe decision-making logic employed by human operators. This automated workflow\ngives optimal scanning parameters for different probes and samples and gives\nhigh-quality SPM images consistently in the attractive mode. This study\nbroadens the application and accessibility of SPM and opens the door for fully\nautomated SPM.","authors":["Yu Liu","Roger Proksch","Jason Bemis","Utkarsh Pratiush","Astita Dubey","Mahshid Ahmadi","Reece Emery","Philip D. Rack","Yu-Chen Liu","Jan-Chi Yang","Sergei V. Kalinin"],"pdf_link":"http://arxiv.org/pdf/2408.04055v1","category":["Reinforcement Learning","Explainable AI"]},{"title":"PowerPM: Foundation Model for Power Systems","abstract":"The emergence of abundant electricity time series (ETS) data provides ample\nopportunities for various applications in the power systems, including\ndemand-side management, grid stability, and consumer behavior analysis. Deep\nlearning models have advanced ETS modeling by effectively capturing sequence\ndependence. Nevertheless, learning a generic representation of ETS data for\nvarious applications remains challenging due to the inherently complex\nhierarchical structure of ETS data. Moreover, ETS data exhibits intricate\ntemporal dependencies and is suscepti ble to the influence of exogenous\nvariables. Furthermore, different instances exhibit diverse electricity\nconsumption behavior. In this paper, we propose a foundation model PowerPM to\nmodel ETS data, providing a large-scale, off-the-shelf model for power systems.\nPowerPM consists of a temporal encoder and a hierarchical encoder. The temporal\nencoder captures both temporal dependencies in ETS data, considering exogenous\nvariables. The hierarchical encoder models the correlation between hierarchy.\nFurthermore, PowerPM leverages a novel self-supervised pretraining framework\nconsisting of masked ETS modeling and dual-view contrastive learning, which\nenable PowerPM to capture temporal dependency within ETS windows and aware the\ndiscrepancy across ETS windows, providing two different perspectives to learn\ngeneric representation. Our experiments involve five real world scenario\ndatasets, comprising private and public data. Through pre-training on massive\nETS data, PowerPM achieves SOTA performance on diverse downstream tasks within\nthe private dataset. Impressively, when transferred to the public datasets,\nPowerPM maintains its superiority, showcasing its remarkable generalization\nability across various tasks and domains. Moreover, ablation studies, few-shot\nexperiments provide additional evidence of the effectiveness of our model.","authors":["Shihao Tu","Yupeng Zhang","Jing Zhang","Yang Yang"],"pdf_link":"http://arxiv.org/pdf/2408.04057v1","category":["Datasets","Explainable AI"]},{"title":"Digital Avatars: Framework Development and Their Evaluation","abstract":"We present a novel prompting strategy for artificial intelligence driven\ndigital avatars. To better quantify how our prompting strategy affects\nanthropomorphic features like humor, authenticity, and favorability we present\nCrowd Vote - an adaptation of Crowd Score that allows for judges to elect a\nlarge language model (LLM) candidate over competitors answering the same or\nsimilar prompts. To visualize the responses of our LLM, and the effectiveness\nof our prompting strategy we propose an end-to-end framework for creating\nhigh-fidelity artificial intelligence (AI) driven digital avatars. This\npipeline effectively captures an individual's essence for interaction and our\nstreaming algorithm delivers a high-quality digital avatar with real-time\naudio-video streaming from server to mobile device. Both our visualization\ntool, and our Crowd Vote metrics demonstrate our AI driven digital avatars have\nstate-of-the-art humor, authenticity, and favorability outperforming all\ncompetitors and baselines. In the case of our Donald Trump and Joe Biden\navatars, their authenticity and favorability are rated higher than even their\nreal-world equivalents.","authors":["Timothy Rupprecht","Sung-En Chang","Yushu Wu","Lei Lu","Enfu Nan","Chih-hsiang Li","Caiyue Lai","Zhimin Li","Zhijun Hu","Yumei He","David Kaeli","Yanzhi Wang"],"pdf_link":"http://arxiv.org/pdf/2408.04068v1","category":["Speech Synthesis","Speech Recognition"]},{"title":"The Data Addition Dilemma","abstract":"In many machine learning for healthcare tasks, standard datasets are\nconstructed by amassing data across many, often fundamentally dissimilar,\nsources. But when does adding more data help, and when does it hinder progress\non desired model outcomes in real-world settings? We identify this situation as\nthe \\textit{Data Addition Dilemma}, demonstrating that adding training data in\nthis multi-source scaling context can at times result in reduced overall\naccuracy, uncertain fairness outcomes, and reduced worst-subgroup performance.\nWe find that this possibly arises from an empirically observed trade-off\nbetween model performance improvements due to data scaling and model\ndeterioration from distribution shift. We thus establish baseline strategies\nfor navigating this dilemma, introducing distribution shift heuristics to guide\ndecision-making on which data sources to add in data scaling, in order to yield\nthe expected model performance improvements. We conclude with a discussion of\nthe required considerations for data collection and suggestions for studying\ndata composition and scale in the age of increasingly larger models.","authors":["Judy Hanwen Shen","Inioluwa Deborah Raji","Irene Y. Chen"],"pdf_link":"http://arxiv.org/pdf/2408.04154v1","category":["Datasets","AI in Healthcare"]},{"title":"Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions","abstract":"This paper considers a scenario in city navigation: an AI agent is provided\nwith language descriptions of the goal location with respect to some well-known\nlandmarks; By only observing the scene around, including recognizing landmarks\nand road network connections, the agent has to make decisions to navigate to\nthe goal location without instructions. This problem is very challenging,\nbecause it requires agent to establish self-position and acquire spatial\nrepresentation of complex urban environment, where landmarks are often\ninvisible. In the absence of navigation instructions, such abilities are vital\nfor the agent to make high-quality decisions in long-range city navigation.\nWith the emergent reasoning ability of large language models (LLMs), a tempting\nbaseline is to prompt LLMs to \"react\" on each observation and make decisions\naccordingly. However, this baseline has very poor performance that the agent\noften repeatedly visits same locations and make short-sighted, inconsistent\ndecisions. To address these issues, this paper introduces a novel agentic\nworkflow featured by its abilities to perceive, reflect and plan. Specifically,\nwe find LLaVA-7B can be fine-tuned to perceive the direction and distance of\nlandmarks with sufficient accuracy for city navigation. Moreover, reflection is\nachieved through a memory mechanism, where past experiences are stored and can\nbe retrieved with current perception for effective decision argumentation.\nPlanning uses reflection results to produce long-term plans, which can avoid\nshort-sighted decisions in long-range navigation. We show the designed workflow\nsignificantly improves navigation ability of the LLM agent compared with the\nstate-of-the-art baselines.","authors":["Qingbin Zeng","Qinglong Yang","Shunan Dong","Heming Du","Liang Zheng","Fengli Xu","Yong Li"],"pdf_link":"http://arxiv.org/pdf/2408.04168v1","category":["Explainable AI","Reinforcement Learning"]},{"title":"Listwise Reward Estimation for Offline Preference-based Reinforcement Learning","abstract":"In Reinforcement Learning (RL), designing precise reward functions remains to\nbe a challenge, particularly when aligning with human intent. Preference-based\nRL (PbRL) was introduced to address this problem by learning reward models from\nhuman feedback. However, existing PbRL methods have limitations as they often\noverlook the second-order preference that indicates the relative strength of\npreference. In this paper, we propose Listwise Reward Estimation (LiRE), a\nnovel approach for offline PbRL that leverages second-order preference\ninformation by constructing a Ranked List of Trajectories (RLT), which can be\nefficiently built by using the same ternary feedback type as traditional\nmethods. To validate the effectiveness of LiRE, we propose a new offline PbRL\ndataset that objectively reflects the effect of the estimated rewards. Our\nextensive experiments on the dataset demonstrate the superiority of LiRE, i.e.,\noutperforming state-of-the-art baselines even with modest feedback budgets and\nenjoying robustness with respect to the number of feedbacks and feedback noise.\nOur code is available at https://github.com/chwoong/LiRE","authors":["Heewoong Choi","Sangwon Jung","Hongjoon Ahn","Taesup Moon"],"pdf_link":"http://arxiv.org/pdf/2408.04190v1","category":["Reinforcement Learning","Explainable AI"]},{"title":"Uncertainty-Aware Crime Prediction With Spatial Temporal Multivariate Graph Neural Networks","abstract":"Crime forecasting is a critical component of urban analysis and essential for\nstabilizing society today. Unlike other time series forecasting problems, crime\nincidents are sparse, particularly in small regions and within specific time\nperiods. Traditional spatial-temporal deep learning models often struggle with\nthis sparsity, as they typically cannot effectively handle the non-Gaussian\nnature of crime data, which is characterized by numerous zeros and\nover-dispersed patterns. To address these challenges, we introduce a novel\napproach termed Spatial Temporal Multivariate Zero-Inflated Negative Binomial\nGraph Neural Networks (STMGNN-ZINB). This framework leverages diffusion and\nconvolution networks to analyze spatial, temporal, and multivariate\ncorrelations, enabling the parameterization of probabilistic distributions of\ncrime incidents. By incorporating a Zero-Inflated Negative Binomial model,\nSTMGNN-ZINB effectively manages the sparse nature of crime data, enhancing\nprediction accuracy and the precision of confidence intervals. Our evaluation\non real-world datasets confirms that STMGNN-ZINB outperforms existing models,\nproviding a more reliable tool for predicting and understanding crime dynamics.","authors":["Zepu Wang","Xiaobo Ma","Huajie Yang","Weimin Lvu","Peng Sun","Sharath Chandra Guntuku"],"pdf_link":"http://arxiv.org/pdf/2408.04193v1","category":["Datasets","Explainable AI"]},{"title":"Pairwise Judgment Formulation for Semantic Embedding Model in Web Search","abstract":"Semantic Embedding Model (SEM), a neural network-based Siamese architecture,\nis gaining momentum in information retrieval and natural language processing.\nIn order to train SEM in a supervised fashion for Web search, the search engine\nquery log is typically utilized to automatically formulate pairwise judgments\nas training data. Despite the growing application of semantic embeddings in the\nsearch engine industry, little work has been done on formulating effective\npairwise judgments for training SEM. In this paper, we make the first in-depth\ninvestigation of a wide range of strategies for generating pairwise judgments\nfor SEM. An interesting (perhaps surprising) discovery reveals that the\nconventional pairwise judgment formulation strategy wildly used in the field of\npairwise Learning-to-Rank (LTR) is not necessarily effective for training SEM.\nThrough a large-scale empirical study based on query logs and click-through\nactivities from a major commercial search engine, we demonstrate the effective\nstrategies for SEM and highlight the advantages of a hybrid heuristic (i.e.,\nClicked > Non-Clicked) in comparison to the atomic heuristics (e.g., Clicked >\nSkipped) in LTR. We conclude with best practices for training SEM and offer\npromising insights for future research.","authors":["Mengze Hong","Chen Jason Zhang"],"pdf_link":"http://arxiv.org/pdf/2408.04197v1","category":["LLMs","Multimodal Learning"]},{"title":"MMRole: A Comprehensive Framework for Developing and Evaluating Multimodal Role-Playing Agents","abstract":"Recently, Role-Playing Agents (RPAs) have garnered increasing attention for\ntheir potential to deliver emotional value and facilitate sociological\nresearch. However, existing studies are primarily confined to the textual\nmodality, unable to simulate humans' multimodal perceptual capabilities. To\nbridge this gap, we introduce the concept of Multimodal Role-Playing Agents\n(MRPAs), and propose a comprehensive framework, MMRole, for their development\nand evaluation, which comprises a personalized multimodal dataset and a robust\nevaluation method. Specifically, we construct a large-scale, high-quality\ndataset, MMRole-Data, consisting of 85 characters, 11K images, and 14K single\nor multi-turn dialogues. Additionally, we present a robust evaluation method,\nMMRole-Eval, encompassing eight metrics across three dimensions, where a reward\nmodel is trained to score MRPAs with the constructed ground-truth data for\ncomparison. Moreover, we develop the first specialized MRPA, MMRole-Agent.\nExtensive evaluation results demonstrate the improved performance of\nMMRole-Agent and highlight the primary challenges in developing MRPAs,\nemphasizing the need for enhanced multimodal understanding and role-playing\nconsistency. The data, code, and models will be available at\nhttps://github.com/YanqiDai/MMRole.","authors":["Yanqi Dai","Huanran Hu","Lei Wang","Shengjie Jin","Xu Chen","Zhiwu Lu"],"pdf_link":"http://arxiv.org/pdf/2408.04203v1","category":["Multimodal Learning","Speech Synthesis"]},{"title":"AI-Driven Chatbot for Intrusion Detection in Edge Networks: Enhancing Cybersecurity with Ethical User Consent","abstract":"In today's contemporary digital landscape, chatbots have become indispensable\ntools across various sectors, streamlining customer service, providing personal\nassistance, automating routine tasks, and offering health advice. However,\ntheir potential remains underexplored in the realm of network security,\nparticularly for intrusion detection. To bridge this gap, we propose an\narchitecture chatbot specifically designed to enhance security within edge\nnetworks specifically for intrusion detection. Leveraging advanced machine\nlearning algorithms, this chatbot will monitor network traffic to identify and\nmitigate potential intrusions. By securing the network environment using an\nedge network managed by a Raspberry Pi module and ensuring ethical user consent\npromoting transparency and trust, this innovative solution aims to safeguard\nsensitive data and maintain a secure workplace, thereby addressing the growing\nneed for robust network security measures in the digital age.","authors":["Mugheez Asif","Abdul Manan","Abdul Moiz ur Rehman","Mamoona Naveed Asghar","Muhammad Umair"],"pdf_link":"http://arxiv.org/pdf/2408.04281v1","category":["AI in Healthcare","Explainable AI"]},{"title":"Tackling Noisy Clients in Federated Learning with End-to-end Label Correction","abstract":"Recently, federated learning (FL) has achieved wide successes for diverse\nprivacy-sensitive applications without sacrificing the sensitive private\ninformation of clients. However, the data quality of client datasets can not be\nguaranteed since corresponding annotations of different clients often contain\ncomplex label noise of varying degrees, which inevitably causes the performance\ndegradation. Intuitively, the performance degradation is dominated by clients\nwith higher noise rates since their trained models contain more misinformation\nfrom data, thus it is necessary to devise an effective optimization scheme to\nmitigate the negative impacts of these noisy clients. In this work, we propose\na two-stage framework FedELC to tackle this complicated label noise issue. The\nfirst stage aims to guide the detection of noisy clients with higher label\nnoise, while the second stage aims to correct the labels of noisy clients' data\nvia an end-to-end label correction framework which is achieved by learning\npossible ground-truth labels of noisy clients' datasets via back propagation.\nWe implement sixteen related methods and evaluate five datasets with three\ntypes of complicated label noise scenarios for a comprehensive comparison.\nExtensive experimental results demonstrate our proposed framework achieves\nsuperior performance than its counterparts for different scenarios.\nAdditionally, we effectively improve the data quality of detected noisy\nclients' local datasets with our label correction framework. The code is\navailable at https://github.com/Sprinter1999/FedELC.","authors":["Xuefeng Jiang","Sheng Sun","Jia Li","Jingjing Xue","Runhan Li","Zhiyuan Wu","Gang Xu","Yuwei Wang","Min Liu"],"pdf_link":"http://arxiv.org/pdf/2408.04301v1","category":["Datasets","Explainable AI"]},{"title":"Learning with Digital Agents: An Analysis based on the Activity Theory","abstract":"Digital agents are considered a general-purpose technology. They spread\nquickly in private and organizational contexts, including education. Yet,\nresearch lacks a conceptual framing to describe interaction with such agents in\na holistic manner. While focusing on the interaction with a pedagogical agent,\ni.e., a digital agent capable of natural-language interaction with a learner,\nwe propose a model of learning activity based on activity theory. We use this\nmodel and a review of prior research on digital agents in education to analyze\nhow various characteristics of the activity, including features of a\npedagogical agent or learner, influence learning outcomes. The analysis leads\nto identification of IS research directions and guidance for developers of\npedagogical agents and digital agents in general. We conclude by extending the\nactivity theory-based model beyond the context of education and show how it\nhelps designers and researchers ask the right questions when creating a digital\nagent.","authors":["Mateusz Dolata","Dzmitry Katsiuba","Natalie Wellnhammer","Gerhard Schwabe"],"pdf_link":"http://arxiv.org/pdf/2408.04304v1","category":["Explainable AI","Reinforcement Learning"]},{"title":"KnowPC: Knowledge-Driven Programmatic Reinforcement Learning for Zero-shot Coordination","abstract":"Zero-shot coordination (ZSC) remains a major challenge in the cooperative AI\nfield, which aims to learn an agent to cooperate with an unseen partner in\ntraining environments or even novel environments. In recent years, a popular\nZSC solution paradigm has been deep reinforcement learning (DRL) combined with\nadvanced self-play or population-based methods to enhance the neural policy's\nability to handle unseen partners. Despite some success, these approaches\nusually rely on black-box neural networks as the policy function. However,\nneural networks typically lack interpretability and logic, making the learned\npolicies difficult for partners (e.g., humans) to understand and limiting their\ngeneralization ability. These shortcomings hinder the application of\nreinforcement learning methods in diverse cooperative scenarios.We suggest to\nrepresent the agent's policy with an interpretable program. Unlike neural\nnetworks, programs contain stable logic, but they are non-differentiable and\ndifficult to optimize.To automatically learn such programs, we introduce\nKnowledge-driven Programmatic reinforcement learning for zero-shot Coordination\n(KnowPC). We first define a foundational Domain-Specific Language (DSL),\nincluding program structures, conditional primitives, and action primitives. A\nsignificant challenge is the vast program search space, making it difficult to\nfind high-performing programs efficiently. To address this, KnowPC integrates\nan extractor and an reasoner. The extractor discovers environmental transition\nknowledge from multi-agent interaction trajectories, while the reasoner deduces\nthe preconditions of each action primitive based on the transition knowledge.","authors":["Yin Gu","Qi Liu","Zhi Li","Kai Zhang"],"pdf_link":"http://arxiv.org/pdf/2408.04336v1","category":["Reinforcement Learning","Explainable AI"]},{"title":"Optimal Layout-Aware CNOT Circuit Synthesis with Qubit Permutation","abstract":"CNOT optimization plays a significant role in noise reduction for Quantum\nCircuits. Several heuristic and exact approaches exist for CNOT optimization.\nIn this paper, we investigate more complicated variations of optimal synthesis\nby allowing qubit permutations and handling layout restrictions. We encode such\nproblems into Planning, SAT, and QBF. We provide optimization for both CNOT\ngate count and circuit depth. For experimental evaluation, we consider standard\nT-gate optimized benchmarks and optimize CNOT sub-circuits. We show that\nallowing qubit permutations can further reduce up to 56% in CNOT count and 46%\nin circuit depth. In the case of optimally mapped circuits under layout\nrestrictions, we observe a reduction up to 17% CNOT count and 19% CNOT depth.","authors":["Irfansha Shaik","Jaco van de Pol"],"pdf_link":"http://arxiv.org/pdf/2408.04349v1","category":["Speech Synthesis","Explainable AI"]},{"title":"Anomaly Prediction: A Novel Approach with Explicit Delay and Horizon","abstract":"Detecting anomalies in time series data is a critical challenge across\nvarious domains. Traditional methods typically focus on identifying anomalies\nin immediate subsequent steps, often underestimating the significance of\ntemporal dynamics such as delay time and horizons of anomalies, which generally\nrequire extensive post-analysis. This paper introduces a novel approach for\ntime series anomaly prediction, incorporating temporal information directly\ninto the prediction results. We propose a new dataset specifically designed to\nevaluate this approach and conduct comprehensive experiments using several\nstate-of-the-art methods. results demonstrate the efficacy of our approach in\nproviding timely and accurate anomaly predictions, setting a new benchmark for\nfuture research in this field.","authors":["Jiang You","Arben Cela","Ren\u00e9 Natowicz","Jacob Ouanounou","Patrick Siarry"],"pdf_link":"http://arxiv.org/pdf/2408.04377v1","category":["Datasets","Explainable AI"]},{"title":"Judgment2vec: Apply Graph Analytics to Searching and Recommendation of Similar Judgments","abstract":"In court practice, legal professionals rely on their training to provide\nopinions that resolve cases, one of the most crucial aspects being the ability\nto identify similar judgments from previous courts efficiently. However,\nfinding a similar case is challenging and often depends on experience, legal\ndomain knowledge, and extensive labor hours, making veteran lawyers or judges\nindispensable. This research aims to automate the analysis of judgment text\nsimilarity. We utilized a judgment dataset labeled as the \"golden standard\" by\nexperts, which includes human-verified features that can be converted into an\n\"expert similarity score.\" We then constructed a knowledge graph based on\n\"case-article\" relationships, ranking each case using natural language\nprocessing to derive a \"Node2vec similarity score.\" By evaluating these two\nsimilarity scores, we identified their discrepancies and relationships. The\nresults can significantly reduce the labor hours required for legal searches\nand recommendations, with potential applications extending to various fields of\ninformation retrieval.","authors":["Hsuan-Lei Shao"],"pdf_link":"http://arxiv.org/pdf/2408.04382v1","category":["Datasets","AI in Healthcare"]},{"title":"Non-maximizing policies that fulfill multi-criterion aspirations in expectation","abstract":"In dynamic programming and reinforcement learning, the policy for the\nsequential decision making of an agent in a stochastic environment is usually\ndetermined by expressing the goal as a scalar reward function and seeking a\npolicy that maximizes the expected total reward. However, many goals that\nhumans care about naturally concern multiple aspects of the world, and it may\nnot be obvious how to condense those into a single reward function.\nFurthermore, maximization suffers from specification gaming, where the obtained\npolicy achieves a high expected total reward in an unintended way, often taking\nextreme or nonsensical actions.\n  Here we consider finite acyclic Markov Decision Processes with multiple\ndistinct evaluation metrics, which do not necessarily represent quantities that\nthe user wants to be maximized. We assume the task of the agent is to ensure\nthat the vector of expected totals of the evaluation metrics falls into some\ngiven convex set, called the aspiration set. Our algorithm guarantees that this\ntask is fulfilled by using simplices to approximate feasibility sets and\npropagate aspirations forward while ensuring they remain feasible. It has\ncomplexity linear in the number of possible state-action-successor triples and\npolynomial in the number of evaluation metrics. Moreover, the explicitly\nnon-maximizing nature of the chosen policy and goals yields additional degrees\nof freedom, which can be used to apply heuristic safety criteria to the choice\nof actions. We discuss several such safety criteria that aim to steer the agent\ntowards more conservative behavior.","authors":["Simon Dima","Simon Fischer","Jobst Heitzig","Joss Oliver"],"pdf_link":"http://arxiv.org/pdf/2408.04385v1","category":["Reinforcement Learning","Explainable AI"]},{"title":"MM-Forecast: A Multimodal Approach to Temporal Event Forecasting with Large Language Models","abstract":"We study an emerging and intriguing problem of multimodal temporal event\nforecasting with large language models. Compared to using text or graph\nmodalities, the investigation of utilizing images for temporal event\nforecasting has not been fully explored, especially in the era of large\nlanguage models (LLMs). To bridge this gap, we are particularly interested in\ntwo key questions of: 1) why images will help in temporal event forecasting,\nand 2) how to integrate images into the LLM-based forecasting framework. To\nanswer these research questions, we propose to identify two essential functions\nthat images play in the scenario of temporal event forecasting, i.e.,\nhighlighting and complementary. Then, we develop a novel framework, named\nMM-Forecast. It employs an Image Function Identification module to recognize\nthese functions as verbal descriptions using multimodal large language models\n(MLLMs), and subsequently incorporates these function descriptions into\nLLM-based forecasting models. To evaluate our approach, we construct a new\nmultimodal dataset, MidEast-TE-mm, by extending an existing event dataset\nMidEast-TE-mini with images. Empirical studies demonstrate that our MM-Forecast\ncan correctly identify the image functions, and further more, incorporating\nthese verbal function descriptions significantly improves the forecasting\nperformance. The dataset, code, and prompts are available at\nhttps://github.com/LuminosityX/MM-Forecast.","authors":["Haoxuan Li","Zhengmao Yang","Yunshan Ma","Yi Bin","Yang Yang","Tat-Seng Chua"],"pdf_link":"http://arxiv.org/pdf/2408.04388v1","category":["Multimodal Learning","Speech Recognition"]},{"title":"DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generalization","abstract":"This paper addresses the challenge of out-of-distribution (OOD)\ngeneralization in graph machine learning, a field rapidly advancing yet\ngrappling with the discrepancy between source and target data distributions.\nTraditional graph learning algorithms, based on the assumption of uniform\ndistribution between training and test data, falter in real-world scenarios\nwhere this assumption fails, resulting in suboptimal performance. A principal\nfactor contributing to this suboptimal performance is the inherent simplicity\nbias of neural networks trained through Stochastic Gradient Descent (SGD),\nwhich prefer simpler features over more complex yet equally or more predictive\nones. This bias leads to a reliance on spurious correlations, adversely\naffecting OOD performance in various tasks such as image recognition, natural\nlanguage understanding, and graph classification. Current methodologies,\nincluding subgraph-mixup and information bottleneck approaches, have achieved\npartial success but struggle to overcome simplicity bias, often reinforcing\nspurious correlations. To tackle this, we propose DIVE, training a collection\nof models to focus on all label-predictive subgraphs by encouraging the models\nto foster divergence on the subgraph mask, which circumvents the limitation of\na model solely focusing on the subgraph corresponding to simple structural\npatterns. Specifically, we employs a regularizer to punish overlap in extracted\nsubgraphs across models, thereby encouraging different models to concentrate on\ndistinct structural patterns. Model selection for robust OOD performance is\nachieved through validation accuracy. Tested across four datasets from GOOD\nbenchmark and one dataset from DrugOOD benchmark, our approach demonstrates\nsignificant improvement over existing methods, effectively addressing the\nsimplicity bias and enhancing generalization in graph machine learning.","authors":["Xin Sun","Liang Wang","Qiang Liu","Shu Wu","Zilei Wang","Liang Wang"],"pdf_link":"http://arxiv.org/pdf/2408.04400v1","category":["Datasets","Multimodal Learning"]},{"title":"Probabilistic energy forecasting through quantile regression in reproducing kernel Hilbert spaces","abstract":"Accurate energy demand forecasting is crucial for sustainable and resilient\nenergy development. To meet the Net Zero Representative Concentration Pathways\n(RCP) $4.5$ scenario in the DACH countries, increased renewable energy\nproduction, energy storage, and reduced commercial building consumption are\nneeded. This scenario's success depends on hydroelectric capacity and climatic\nfactors. Informed decisions require quantifying uncertainty in forecasts. This\nstudy explores a non-parametric method based on \\emph{reproducing kernel\nHilbert spaces (RKHS)}, known as kernel quantile regression, for energy\nprediction. Our experiments demonstrate its reliability and sharpness, and we\nbenchmark it against state-of-the-art methods in load and price forecasting for\nthe DACH region. We offer our implementation in conjunction with additional\nscripts to ensure the reproducibility of our research.","authors":["Luca Pernigo","Rohan Sen","Davide Baroli"],"pdf_link":"http://arxiv.org/pdf/2408.04405v1","category":["Reinforcement Learning","Datasets"]},{"title":"FedAD-Bench: A Unified Benchmark for Federated Unsupervised Anomaly Detection in Tabular Data","abstract":"The emergence of federated learning (FL) presents a promising approach to\nleverage decentralized data while preserving privacy. Furthermore, the\ncombination of FL and anomaly detection is particularly compelling because it\nallows for detecting rare and critical anomalies (usually also rare in locally\ngathered data) in sensitive data from multiple sources, such as cybersecurity\nand healthcare. However, benchmarking the performance of anomaly detection\nmethods in FL environments remains an underexplored area. This paper introduces\nFedAD-Bench, a unified benchmark for evaluating unsupervised anomaly detection\nalgorithms within the context of FL. We systematically analyze and compare the\nperformance of recent deep learning anomaly detection models under federated\nsettings, which were typically assessed solely in centralized settings.\nFedAD-Bench encompasses diverse datasets and metrics to provide a holistic\nevaluation. Through extensive experiments, we identify key challenges such as\nmodel aggregation inefficiencies and metric unreliability. We present insights\ninto FL's regularization effects, revealing scenarios in which it outperforms\ncentralized approaches due to its inherent ability to mitigate overfitting. Our\nwork aims to establish a standardized benchmark to guide future research and\ndevelopment in federated anomaly detection, promoting reproducibility and fair\ncomparison across studies.","authors":["Ahmed Anwar","Brian Moser","Dayananda Herurkar","Federico Raue","Vinit Hegiste","Tatjana Legler","Andreas Dengel"],"pdf_link":"http://arxiv.org/pdf/2408.04442v1","category":["Datasets","Explainable AI"]},{"title":"Reasoning about Study Regulations in Answer Set Programming","abstract":"We are interested in automating reasoning with and about study regulations,\ncatering to various stakeholders, ranging from administrators, over faculty, to\nstudents at different stages. Our work builds on an extensive analysis of\nvarious study programs at the University of Potsdam. The conceptualization of\nthe underlying principles provides us with a formal account of study\nregulations. In particular, the formalization reveals the properties of\nadmissible study plans. With these at end, we propose an encoding of study\nregulations in Answer Set Programming that produces corresponding study plans.\nFinally, we show how this approach can be extended to a generic user interface\nfor exploring study plans.","authors":["Susana Hahn","Cedric Martens","Amade Nemes","Henry Otunuya","Javier Romero","Torsten Schaub","Sebastian Schellhorn"],"pdf_link":"http://arxiv.org/pdf/2408.04528v1","category":["Explainable AI","Datasets"]},{"title":"Synchronous Multi-modal Semantic CommunicationSystem with Packet-level Coding","abstract":"Although the semantic communication with joint semantic-channel coding design\nhas shown promising performance in transmitting data of different modalities\nover physical layer channels, the synchronization and packet-level forward\nerror correction of multimodal semantics have not been well studied. Due to the\nindependent design of semantic encoders, synchronizing multimodal features in\nboth the semantic and time domains is a challenging problem. In this paper, we\ntake the facial video and speech transmission as an example and propose a\nSynchronous Multimodal Semantic Communication System (SyncSC) with Packet-Level\nCoding. To achieve semantic and time synchronization, 3D Morphable Mode (3DMM)\ncoefficients and text are transmitted as semantics, and we propose a semantic\ncodec that achieves similar quality of reconstruction and synchronization with\nlower bandwidth, compared to traditional methods. To protect semantic packets\nunder the erasure channel, we propose a packet-Level Forward Error Correction\n(FEC) method, called PacSC, that maintains a certain visual quality performance\neven at high packet loss rates. Particularly, for text packets, a text packet\nloss concealment module, called TextPC, based on Bidirectional Encoder\nRepresentations from Transformers (BERT) is proposed, which significantly\nimproves the performance of traditional FEC methods. The simulation results\nshow that our proposed SyncSC reduce transmission overhead and achieve\nhigh-quality synchronous transmission of video and speech over the packet loss\nnetwork.","authors":["Yun Tian","Jingkai Ying","Zhijin Qin","Ye Jin","Xiaoming Tao"],"pdf_link":"http://arxiv.org/pdf/2408.04535v1","category":["Multimodal Learning","Speech Synthesis"]},{"title":"Inference with the Upper Confidence Bound Algorithm","abstract":"In this paper, we discuss the asymptotic behavior of the Upper Confidence\nBound (UCB) algorithm in the context of multiarmed bandit problems and discuss\nits implication in downstream inferential tasks. While inferential tasks become\nchallenging when data is collected in a sequential manner, we argue that this\nproblem can be alleviated when the sequential algorithm at hand satisfies\ncertain stability property. This notion of stability is motivated from the\nseminal work of Lai and Wei (1982). Our first main result shows that such a\nstability property is always satisfied for the UCB algorithm, and as a result\nthe sample means for each arm are asymptotically normal. Next, we examine the\nstability properties of the UCB algorithm when the number of arms $K$ is\nallowed to grow with the number of arm pulls $T$. We show that in such a case\nthe arms are stable when $\\frac{\\log K}{\\log T} \\rightarrow 0$, and the number\nof near-optimal arms are large.","authors":["Koulik Khamaru","Cun-Hui Zhang"],"pdf_link":"http://arxiv.org/pdf/2408.04595v1","category":["Reinforcement Learning","Explainable AI"]}]