[{"title":"Robustness of Speech Separation Models for Similar-pitch Speakers","abstract":"Single-channel speech separation is a crucial task for enhancing speech\nrecognition systems in multi-speaker environments. This paper investigates the\nrobustness of state-of-the-art Neural Network models in scenarios where the\npitch differences between speakers are minimal. Building on earlier findings by\nDitter and Gerkmann, which identified a significant performance drop for the\n2018 Chimera++ under similar-pitch conditions, our study extends the analysis\nto more recent and sophisticated Neural Network models. Our experiments reveal\nthat modern models have substantially reduced the performance gap for matched\ntraining and testing conditions. However, a substantial performance gap\npersists under mismatched conditions, with models performing well for large\npitch differences but showing worse performance if the speakers' pitches are\nsimilar. These findings motivate further research into the generalizability of\nspeech separation models to similar-pitch speakers and unseen data.","authors":["Bunlong Lay","Sebastian Zaczek","Kristina Tesch","Timo Gerkmann"],"pdf_link":"http://arxiv.org/pdf/2407.15749v1","category":["Speech Recognition","Speech Synthesis"]},{"title":"MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music Generation","abstract":"Existing text-to-music models can produce high-quality audio with great\ndiversity. However, textual prompts alone cannot precisely control temporal\nmusical features such as chords and rhythm of the generated music. To address\nthis challenge, we introduce MusiConGen, a temporally-conditioned\nTransformer-based text-to-music model that builds upon the pretrained MusicGen\nframework. Our innovation lies in an efficient finetuning mechanism, tailored\nfor consumer-grade GPUs, that integrates automatically-extracted rhythm and\nchords as the condition signal. During inference, the condition can either be\nmusical features extracted from a reference audio signal, or be user-defined\nsymbolic chord sequence, BPM, and textual prompts. Our performance evaluation\non two datasets -- one derived from extracted features and the other from\nuser-created inputs -- demonstrates that MusiConGen can generate realistic\nbacking track music that aligns well with the specified conditions. We\nopen-source the code and model checkpoints, and provide audio examples online,\nhttps://musicongen.github.io/musicongen_demo/.","authors":["Yun-Han Lan","Wen-Yi Hsiao","Hao-Chung Cheng","Yi-Hsuan Yang"],"pdf_link":"http://arxiv.org/pdf/2407.15060v1","category":["Speech Synthesis","Speech Recognition"]},{"title":"Learning Physics for Unveiling Hidden Earthquake Ground Motions via Conditional Generative Modeling","abstract":"Predicting high-fidelity ground motions for future earthquakes is crucial for\nseismic hazard assessment and infrastructure resilience. Conventional empirical\nsimulations suffer from sparse sensor distribution and geographically localized\nearthquake locations, while physics-based methods are computationally intensive\nand require accurate representations of Earth structures and earthquake\nsources. We propose a novel artificial intelligence (AI) simulator, Conditional\nGenerative Modeling for Ground Motion (CGM-GM), to synthesize high-frequency\nand spatially continuous earthquake ground motion waveforms. CGM-GM leverages\nearthquake magnitudes and geographic coordinates of earthquakes and sensors as\ninputs, learning complex wave physics and Earth heterogeneities, without\nexplicit physics constraints. This is achieved through a probabilistic\nautoencoder that captures latent distributions in the time-frequency domain and\nvariational sequential models for prior and posterior distributions. We\nevaluate the performance of CGM-GM using small-magnitude earthquake records\nfrom the San Francisco Bay Area, a region with high seismic risks. CGM-GM\ndemonstrates a strong potential for outperforming a state-of-the-art\nnon-ergodic empirical ground motion model and shows great promise in seismology\nand beyond.","authors":["Pu Ren","Rie Nakata","Maxime Lacour","Ilan Naiman","Nori Nakata","Jialin Song","Zhengfa Bi","Osman Asif Malik","Dmitriy Morozov","Omri Azencot","N. Benjamin Erichson","Michael W. Mahoney"],"pdf_link":"http://arxiv.org/pdf/2407.15089v1","category":["Multimodal Learning","Datasets"]},{"title":"Chemical Reaction Extraction for Chemical Knowledge Base","abstract":"The task of searching through patent documents is crucial for chemical patent\nrecommendation and retrieval. This can be enhanced by creating a patent\nknowledge base (ChemPatKB) to aid in prior art searches and to provide a\nplatform for domain experts to explore new innovations in chemical compound\nsynthesis and use-cases. An essential foundational component of this KB is the\nextraction of important reaction snippets from long patents documents which\nfacilitates multiple downstream tasks such as reaction co-reference resolution\nand chemical entity role identification. In this work, we explore the problem\nof extracting reactions spans from chemical patents in order to create a\nreactions resource database. We formulate this task as a paragraph-level\nsequence tagging problem, where the system is required to return a sequence of\nparagraphs that contain a description of a reaction. We propose several\napproaches and modifications of the baseline models and study how different\nmethods generalize across different domains of chemical patents.","authors":["Aishwarya Jadhav","Ritam Dutt"],"pdf_link":"http://arxiv.org/pdf/2407.15124v1","category":["Datasets","LLMs"]},{"title":"Proximal Policy Distillation","abstract":"We introduce Proximal Policy Distillation (PPD), a novel policy distillation\nmethod that integrates student-driven distillation and Proximal Policy\nOptimization (PPO) to increase sample efficiency and to leverage the additional\nrewards that the student policy collects during distillation. To assess the\nefficacy of our method, we compare PPD with two common alternatives,\nstudent-distill and teacher-distill, over a wide range of reinforcement\nlearning environments that include discrete actions and continuous control\n(ATARI, Mujoco, and Procgen). For each environment and method, we perform\ndistillation to a set of target student neural networks that are smaller,\nidentical (self-distillation), or larger than the teacher network. Our findings\nindicate that PPD improves sample efficiency and produces better student\npolicies compared to typical policy distillation approaches. Moreover, PPD\ndemonstrates greater robustness than alternative methods when distilling\npolicies from imperfect demonstrations. The code for the paper is released as\npart of a new Python library built on top of stable-baselines3 to facilitate\npolicy distillation: `sb3-distill'.","authors":["Giacomo Spigler"],"pdf_link":"http://arxiv.org/pdf/2407.15134v1","category":["Reinforcement Learning","Explainable AI"]},{"title":"Mitigating Deep Reinforcement Learning Backdoors in the Neural Activation Space","abstract":"This paper investigates the threat of backdoors in Deep Reinforcement\nLearning (DRL) agent policies and proposes a novel method for their detection\nat runtime. Our study focuses on elusive in-distribution backdoor triggers.\nSuch triggers are designed to induce a deviation in the behaviour of a\nbackdoored agent while blending into the expected data distribution to evade\ndetection. Through experiments conducted in the Atari Breakout environment, we\ndemonstrate the limitations of current sanitisation methods when faced with\nsuch triggers and investigate why they present a challenging defence problem.\nWe then evaluate the hypothesis that backdoor triggers might be easier to\ndetect in the neural activation space of the DRL agent's policy network. Our\nstatistical analysis shows that indeed the activation patterns in the agent's\npolicy network are distinct in the presence of a trigger, regardless of how\nwell the trigger is concealed in the environment. Based on this, we propose a\nnew defence approach that uses a classifier trained on clean environment\nsamples and detects abnormal activations. Our results show that even\nlightweight classifiers can effectively prevent malicious actions with\nconsiderable accuracy, indicating the potential of this research direction even\nagainst sophisticated adversaries.","authors":["Sanyam Vyas","Chris Hicks","Vasilios Mavroudis"],"pdf_link":"http://arxiv.org/pdf/2407.15168v1","category":["Explainable AI","Reinforcement Learning"]},{"title":"TADA: Temporal Adversarial Data Augmentation for Time Series Data","abstract":"Domain generalization involves training machine learning models to perform\nrobustly on unseen samples from out-of-distribution datasets. Adversarial Data\nAugmentation (ADA) is a commonly used approach that enhances model adaptability\nby incorporating synthetic samples, designed to simulate potential unseen\nsamples. While ADA effectively addresses amplitude-related distribution shifts,\nit falls short in managing temporal shifts, which are essential for time series\ndata. To address this limitation, we propose the Temporal Adversarial Data\nAugmentation for time teries Data (TADA), which incorporates a time warping\ntechnique specifically targeting temporal shifts. Recognizing the challenge of\nnon-differentiability in traditional time warping, we make it differentiable by\nleveraging phase shifts in the frequency domain. Our evaluations across diverse\ndomains demonstrate that TADA significantly outperforms existing ADA variants,\nenhancing model performance across time series datasets with varied\ndistributions.","authors":["Byeong Tak Lee","Joon-myoung Kwon","Yong-Yeon Jo"],"pdf_link":"http://arxiv.org/pdf/2407.15174v1","category":["Multimodal Learning","Datasets"]},{"title":"Exploiting Pre-trained Models for Drug Target Affinity Prediction with Nearest Neighbors","abstract":"Drug-Target binding Affinity (DTA) prediction is essential for drug\ndiscovery. Despite the application of deep learning methods to DTA prediction,\nthe achieved accuracy remain suboptimal. In this work, inspired by the recent\nsuccess of retrieval methods, we propose $k$NN-DTA, a non-parametric\nembedding-based retrieval method adopted on a pre-trained DTA prediction model,\nwhich can extend the power of the DTA model with no or negligible cost.\nDifferent from existing methods, we introduce two neighbor aggregation ways\nfrom both embedding space and label space that are integrated into a unified\nframework. Specifically, we propose a \\emph{label aggregation} with\n\\emph{pair-wise retrieval} and a \\emph{representation aggregation} with\n\\emph{point-wise retrieval} of the nearest neighbors. This method executes in\nthe inference phase and can efficiently boost the DTA prediction performance\nwith no training cost. In addition, we propose an extension, Ada-$k$NN-DTA, an\ninstance-wise and adaptive aggregation with lightweight learning. Results on\nfour benchmark datasets show that $k$NN-DTA brings significant improvements,\noutperforming previous state-of-the-art (SOTA) results, e.g, on BindingDB\nIC$_{50}$ and $K_i$ testbeds, $k$NN-DTA obtains new records of RMSE\n$\\bf{0.684}$ and $\\bf{0.750}$. The extended Ada-$k$NN-DTA further improves the\nperformance to be $\\bf{0.675}$ and $\\bf{0.735}$ RMSE. These results strongly\nprove the effectiveness of our method. Results in other settings and\ncomprehensive studies/analyses also show the great potential of our $k$NN-DTA\napproach.","authors":["Qizhi Pei","Lijun Wu","Zhenyu He","Jinhua Zhu","Yingce Xia","Shufang Xie","Rui Yan"],"pdf_link":"http://arxiv.org/pdf/2407.15202v1","category":["Datasets","Multimodal Learning"]},{"title":"Explainability Paths for Sustained Artistic Practice with AI","abstract":"The development of AI-driven generative audio mirrors broader AI trends,\noften prioritizing immediate accessibility at the expense of explainability.\nConsequently, integrating such tools into sustained artistic practice remains a\nsignificant challenge. In this paper, we explore several paths to improve\nexplainability, drawing primarily from our research-creation practice in\ntraining and implementing generative audio models. As practical provisions for\nimproved explainability, we highlight human agency over training materials, the\nviability of small-scale datasets, the facilitation of the iterative creative\nprocess, and the integration of interactive machine learning as a mapping tool.\nImportantly, these steps aim to enhance human agency over generative AI systems\nnot only during model inference, but also when curating and preprocessing\ntraining data as well as during the training phase of models.","authors":["Austin Tecks","Thomas Peschlow","Gabriel Vigliensoni"],"pdf_link":"http://arxiv.org/pdf/2407.15216v1","category":["Explainable AI","Speech Synthesis"]},{"title":"Explaining Decisions of Agents in Mixed-Motive Games","abstract":"In recent years, agents have become capable of communicating seamlessly via\nnatural language and navigating in environments that involve cooperation and\ncompetition, a fact that can introduce social dilemmas. Due to the interleaving\nof cooperation and competition, understanding agents' decision-making in such\nenvironments is challenging, and humans can benefit from obtaining\nexplanations. However, such environments and scenarios have rarely been\nexplored in the context of explainable AI. While some explanation methods for\ncooperative environments can be applied in mixed-motive setups, they do not\naddress inter-agent competition, cheap-talk, or implicit communication by\nactions. In this work, we design explanation methods to address these issues.\nThen, we proceed to demonstrate their effectiveness and usefulness for humans,\nusing a non-trivial mixed-motive game as a test case. Lastly, we establish\ngenerality and demonstrate the applicability of the methods to other games,\nincluding one where we mimic human game actions using large language models.","authors":["Maayan Orner","Oleg Maksimov","Akiva Kleinerman","Charles Ortiz","Sarit Kraus"],"pdf_link":"http://arxiv.org/pdf/2407.15255v1","category":["Explainable AI","Reinforcement Learning"]},{"title":"New Rules for Causal Identification with Background Knowledge","abstract":"Identifying causal relations is crucial for a variety of downstream tasks. In\nadditional to observational data, background knowledge (BK), which could be\nattained from human expertise or experiments, is usually introduced for\nuncovering causal relations. This raises an open problem that in the presence\nof latent variables, what causal relations are identifiable from observational\ndata and BK. In this paper, we propose two novel rules for incorporating BK,\nwhich offer a new perspective to the open problem. In addition, we show that\nthese rules are applicable in some typical causality tasks, such as determining\nthe set of possible causal effects with observational data. Our rule-based\napproach enhances the state-of-the-art method by circumventing a process of\nenumerating block sets that would otherwise take exponential complexity.","authors":["Tian-Zuo Wang","Lue Tao","Zhi-Hua Zhou"],"pdf_link":"http://arxiv.org/pdf/2407.15259v1","category":["Datasets","Explainable AI"]},{"title":"Unifying Invariant and Variant Features for Graph Out-of-Distribution via Probability of Necessity and Sufficiency","abstract":"Graph Out-of-Distribution (OOD), requiring that models trained on biased data\ngeneralize to the unseen test data, has considerable real-world applications.\nOne of the most mainstream methods is to extract the invariant subgraph by\naligning the original and augmented data with the help of environment\naugmentation. However, these solutions might lead to the loss or redundancy of\nsemantic subgraphs and result in suboptimal generalization. To address this\nchallenge, we propose exploiting Probability of Necessity and Sufficiency (PNS)\nto extract sufficient and necessary invariant substructures. Beyond that, we\nfurther leverage the domain variant subgraphs related to the labels to boost\nthe generalization performance in an ensemble manner. Specifically, we first\nconsider the data generation process for graph data. Under mild conditions, we\nshow that the sufficient and necessary invariant subgraph can be extracted by\nminimizing an upper bound, built on the theoretical advance of the probability\nof necessity and sufficiency. To further bridge the theory and algorithm, we\ndevise the model called Sufficiency and Necessity Inspired Graph Learning\n(SNIGL), which ensembles an invariant subgraph classifier on top of latent\nsufficient and necessary invariant subgraphs, and a domain variant subgraph\nclassifier specific to the test domain for generalization enhancement.\nExperimental results demonstrate that our SNIGL model outperforms the\nstate-of-the-art techniques on six public benchmarks, highlighting its\neffectiveness in real-world scenarios.","authors":["Xuexin Chen","Ruichu Cai","Kaitao Zheng","Zhifan Jiang","Zhengting Huang","Zhifeng Hao","Zijian Li"],"pdf_link":"http://arxiv.org/pdf/2407.15273v1","category":["Datasets","Multimodal Learning"]},{"title":"Odyssey: Empowering Agents with Open-World Skills","abstract":"Recent studies have delved into constructing generalist agents for open-world\nembodied environments like Minecraft. Despite the encouraging results, existing\nefforts mainly focus on solving basic programmatic tasks, e.g., material\ncollection and tool-crafting following the Minecraft tech-tree, treating the\nObtainDiamond task as the ultimate goal. This limitation stems from the\nnarrowly defined set of actions available to agents, requiring them to learn\neffective long-horizon strategies from scratch. Consequently, discovering\ndiverse gameplay opportunities in the open world becomes challenging. In this\nwork, we introduce ODYSSEY, a new framework that empowers Large Language Model\n(LLM)-based agents with open-world skills to explore the vast Minecraft world.\nODYSSEY comprises three key parts: (1) An interactive agent with an open-world\nskill library that consists of 40 primitive skills and 183 compositional\nskills. (2) A fine-tuned LLaMA-3 model trained on a large question-answering\ndataset with 390k+ instruction entries derived from the Minecraft Wiki. (3) A\nnew open-world benchmark includes thousands of long-term planning tasks, tens\nof dynamic-immediate planning tasks, and one autonomous exploration task.\nExtensive experiments demonstrate that the proposed ODYSSEY framework can\neffectively evaluate the planning and exploration capabilities of agents. All\ndatasets, model weights, and code are publicly available to motivate future\nresearch on more advanced autonomous agent solutions.","authors":["Shunyu Liu","Yaoru Li","Kongcheng Zhang","Zhenyu Cui","Wenkai Fang","Yuxuan Zheng","Tongya Zheng","Mingli Song"],"pdf_link":"http://arxiv.org/pdf/2407.15325v1","category":["Explainable AI","Reinforcement Learning"]},{"title":"Integrating IP Broadcasting with Audio Tags- Workflow and Challenges","abstract":"The broadcasting industry is increasingly adopting IP techniques,\nrevolutionising both live and pre-recorded content production, from news\ngathering to live music events. IP broadcasting allows for the transport of\naudio and video signals in an easily configurable way, aligning with modern\nnetworking techniques. This shift towards an IP workflow allows for much\ngreater flexibility, not only in routing signals but with the integration of\ntools using standard web development techniques. One possible tool could\ninclude the use of live audio tagging, which has a number of uses in the\nproduction of content. These include from automated closed captioning to\nidentifying unwanted sound events within a scene. In this paper, we describe\nthe process of containerising an audio tagging model into a microservice, a\nsmall segregated code module that can be integrated into a multitude of\ndifferent network setups. The goal is to develop a modular, accessible, and\nflexible tool capable of seamless deployment into broadcasting workflows of all\nsizes, from small productions to large corporations. Challenges surrounding\nlatency of the selected audio tagging model and its effect on the usefulness of\nthe end product are discussed.","authors":["Rhys Burchett-Vass","Arshdeep Singh","Gabriel Bibb\u00f3","Mark D. Plumbley"],"pdf_link":"http://arxiv.org/pdf/2407.15423v1","category":["Speech Recognition","Speech Synthesis"]},{"title":"Decoding BACnet Packets: A Large Language Model Approach for Packet Interpretation","abstract":"The Industrial Control System (ICS) environment encompasses a wide range of\nintricate communication protocols, posing substantial challenges for Security\nOperations Center (SOC) analysts tasked with monitoring, interpreting, and\naddressing network activities and security incidents. Conventional monitoring\ntools and techniques often struggle to provide a clear understanding of the\nnature and intent of ICS-specific communications. To enhance comprehension, we\npropose a software solution powered by a Large Language Model (LLM). This\nsolution currently focused on BACnet protocol, processes a packet file data and\nextracts context by using a mapping database, and contemporary context\nretrieval methods for Retrieval Augmented Generation (RAG). The processed\npacket information, combined with the extracted context, serves as input to the\nLLM, which generates a concise packet file summary for the user. The software\ndelivers a clear, coherent, and easily understandable summary of network\nactivities, enabling SOC analysts to better assess the current state of the\ncontrol system.","authors":["Rashi Sharma","Hiroyuki Okada","Tatsumi Oba","Karthikk Subramanian","Naoto Yanai","Sugiri Pranata"],"pdf_link":"http://arxiv.org/pdf/2407.15428v1","category":["Datasets","Speech Recognition"]},{"title":"Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed Graphs","abstract":"The text-attributed graph (TAG) is one kind of important real-world\ngraph-structured data with each node associated with raw texts. For TAGs,\ntraditional few-shot node classification methods directly conduct training on\nthe pre-processed node features and do not consider the raw texts. The\nperformance is highly dependent on the choice of the feature pre-processing\nmethod. In this paper, we propose P2TAG, a framework designed for few-shot node\nclassification on TAGs with graph pre-training and prompting. P2TAG first\npre-trains the language model (LM) and graph neural network (GNN) on TAGs with\nself-supervised loss. To fully utilize the ability of language models, we adapt\nthe masked language modeling objective for our framework. The pre-trained model\nis then used for the few-shot node classification with a mixed prompt method,\nwhich simultaneously considers both text and graph information. We conduct\nexperiments on six real-world TAGs, including paper citation networks and\nproduct co-purchasing networks. Experimental results demonstrate that our\nproposed framework outperforms existing graph few-shot learning methods on\nthese datasets with +18.98% ~ +35.98% improvements.","authors":["Huanjing Zhao","Beining Yang","Yukuo Cen","Junyu Ren","Chenhui Zhang","Yuxiao Dong","Evgeny Kharlamov","Shu Zhao","Jie Tang"],"pdf_link":"http://arxiv.org/pdf/2407.15431v1","category":["LLMs","Datasets"]},{"title":"Future-Proofing Mobile Networks: A Digital Twin Approach to Multi-Signal Management","abstract":"Digital Twins (DTs) are set to become a key enabling technology in future\nwireless networks, with their use in network management increasing\nsignificantly. We developed a DT framework that leverages the heterogeneity of\nnetwork access technologies as a resource for enhanced network performance and\nmanagement, enabling smart data handling in the physical network. Tested in a\n\\textit{Campus Area Network} environment, our framework integrates diverse data\nsources to provide real-time, holistic insights into network performance and\nenvironmental sensing. We also envision that traditional analytics will evolve\nto rely on emerging AI models, such as Generative AI (GenAI), while leveraging\ncurrent analytics capabilities. This capacity can simplify analytics processes\nthrough advanced ML models, enabling descriptive, diagnostic, predictive, and\nprescriptive analytics in a unified fashion. Finally, we present specific\nresearch opportunities concerning interoperability aspects and envision\naligning advancements in DT technology with evolved AI integration.","authors":["Roberto Morabito","Bivek Pandey","Paulius Daubaris","Yasith R Wanigarathna","Sasu Tarkoma"],"pdf_link":"http://arxiv.org/pdf/2407.15520v1","category":["Datasets","Explainable AI"]},{"title":"Large-scale Time-Varying Portfolio Optimisation using Graph Attention Networks","abstract":"Apart from assessing individual asset performance, investors in financial\nmarkets also need to consider how a set of firms performs collectively as a\nportfolio. Whereas traditional Markowitz-based mean-variance portfolios are\nwidespread, network-based optimisation techniques have built upon these\ndevelopments. However, most studies do not contain firms at risk of default and\nremove any firms that drop off indices over a certain time. This is the first\nstudy to incorporate risky firms and use all the firms in portfolio\noptimisation. We propose and empirically test a novel method that leverages\nGraph Attention networks (GATs), a subclass of Graph Neural Networks (GNNs).\nGNNs, as deep learning-based models, can exploit network data to uncover\nnonlinear relationships. Their ability to handle high-dimensional features and\naccommodate customised layers for specific purposes makes them particularly\nappealing for large-scale problems such as mid- and small-cap portfolio\noptimization. This study utilises 30 years of data on mid-cap firms, creating\ngraphs of firms using distance correlation and the Triangulated Maximally\nFiltered Graph approach. These graphs are the inputs to a GAT model that we\ntrain using custom layers which impose weight and allocation constraints and a\nloss function derived from the Sharpe ratio, thus directly maximising portfolio\nrisk-adjusted returns. This new model is benchmarked against a network\ncharacteristic-based portfolio, a mean variance-based portfolio, and an\nequal-weighted portfolio. The results show that the portfolio produced by the\nGAT-based model outperforms all benchmarks and is consistently superior to\nother strategies over a long period while also being informative of market\ndynamics.","authors":["Kamesh Korangi","Christophe Mues","Cristi\u00e1n Bravo"],"pdf_link":"http://arxiv.org/pdf/2407.15532v1","category":["Reinforcement Learning","Explainable AI"]},{"title":"Discrete Flow Matching","abstract":"Despite Flow Matching and diffusion models having emerged as powerful\ngenerative paradigms for continuous variables such as images and videos, their\napplication to high-dimensional discrete data, such as language, is still\nlimited. In this work, we present Discrete Flow Matching, a novel discrete flow\nparadigm designed specifically for generating discrete data. Discrete Flow\nMatching offers several key contributions: (i) it works with a general family\nof probability paths interpolating between source and target distributions;\n(ii) it allows for a generic formula for sampling from these probability paths\nusing learned posteriors such as the probability denoiser ($x$-prediction) and\nnoise-prediction ($\\epsilon$-prediction); (iii) practically, focusing on\nspecific probability paths defined with different schedulers considerably\nimproves generative perplexity compared to previous discrete diffusion and flow\nmodels; and (iv) by scaling Discrete Flow Matching models up to 1.7B\nparameters, we reach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1\nand 20.6% Pass@10 on 1-shot MBPP coding benchmarks. Our approach is capable of\ngenerating high-quality discrete data in a non-autoregressive fashion,\nsignificantly closing the gap between autoregressive models and discrete flow\nmodels.","authors":["Itai Gat","Tal Remez","Neta Shaul","Felix Kreuk","Ricky T. Q. Chen","Gabriel Synnaeve","Yossi Adi","Yaron Lipman"],"pdf_link":"http://arxiv.org/pdf/2407.15595v1","category":["Speech Synthesis","Speech Recognition"]},{"title":"Sustainable broadcasting in Blockchain Network with Reinforcement Learning","abstract":"Recent estimates put the carbon footprint of Bitcoin and Ethereum at an\naverage of 64 and 26 million tonnes of CO2 per year, respectively. To address\nthis growing problem, several possible approaches have been proposed in the\nliterature: creating alternative blockchain consensus mechanisms, applying\nredundancy reduction techniques, utilizing renewable energy sources, and\nemploying energy-efficient devices, etc. In this paper, we follow the second\navenue and propose an efficient approach based on reinforcement learning that\nimproves the block broadcasting scheme in blockchain networks. The analysis and\nexperimental results confirmed that the proposed improvement of the block\npropagation scheme could cleverly handle network dynamics and achieve better\nresults than the default approach. Additionally, our technical integration of\nthe simulator and developed RL environment can be used as a complete solution\nfor further study of new schemes and protocols that use RL or other ML\ntechniques.","authors":["Danila Valko","Daniel Kudenko"],"pdf_link":"http://arxiv.org/pdf/2407.15616v1","category":["Reinforcement Learning","Explainable AI"]},{"title":"Evaluation of Reinforcement Learning for Autonomous Penetration Testing using A3C, Q-learning and DQN","abstract":"Penetration testing is the process of searching for security weaknesses by\nsimulating an attack. It is usually performed by experienced professionals,\nwhere scanning and attack tools are applied. By automating the execution of\nsuch tools, the need for human interaction and decision-making could be\nreduced. In this work, a Network Attack Simulator (NASim) was used as an\nenvironment to train reinforcement learning agents to solve three predefined\nsecurity scenarios. These scenarios cover techniques of exploitation,\npost-exploitation and wiretapping. A large hyperparameter grid search was\nperformed to find the best hyperparameter combinations. The algorithms\nQ-learning, DQN and A3C were used, whereby A3C was able to solve all scenarios\nand achieve generalization. In addition, A3C could solve these scenarios with\nfewer actions than the baseline automated penetration testing. Although the\ntraining was performed on rather small scenarios and with small state and\naction spaces for the agents, the results show that a penetration test can\nsuccessfully be performed by the RL agent.","authors":["Norman Becker","Daniel Reti","Evridiki V. Ntagiou","Marcus Wallum","Hans D. Schotten"],"pdf_link":"http://arxiv.org/pdf/2407.15656v1","category":["Reinforcement Learning","Benchmarking"]},{"title":"How to Shrink Confidence Sets for Many Equivalent Discrete Distributions?","abstract":"We consider the situation when a learner faces a set of unknown discrete\ndistributions $(p_k)_{k\\in \\mathcal K}$ defined over a common alphabet\n$\\mathcal X$, and can build for each distribution $p_k$ an individual\nhigh-probability confidence set thanks to $n_k$ observations sampled from\n$p_k$. The set $(p_k)_{k\\in \\mathcal K}$ is structured: each distribution $p_k$\nis obtained from the same common, but unknown, distribution q via applying an\nunknown permutation to $\\mathcal X$. We call this\n\\emph{permutation-equivalence}. The goal is to build refined confidence sets\n\\emph{exploiting} this structural property. Like other popular notions of\nstructure (Lipschitz smoothness, Linearity, etc.) permutation-equivalence\nnaturally appears in machine learning problems, and to benefit from its\npotential gain calls for a specific approach. We present a strategy to\neffectively exploit permutation-equivalence, and provide a finite-time\nhigh-probability bound on the size of the refined confidence sets output by the\nstrategy. Since a refinement is not possible for too few observations in\ngeneral, under mild technical assumptions, our finite-time analysis establish\nwhen the number of observations $(n_k)_{k\\in \\mathcal K}$ are large enough so\nthat the output confidence sets improve over initial individual sets. We\ncarefully characterize this event and the corresponding improvement. Further,\nour result implies that the size of confidence sets shrink at asymptotic rates\nof $O(1/\\sqrt{\\sum_{k\\in \\mathcal K} n_k})$ and $O(1/\\max_{k\\in K} n_{k})$,\nrespectively for elements inside and outside the support of q, when the size of\neach individual confidence set shrinks at respective rates of $O(1/\\sqrt{n_k})$\nand $O(1/n_k)$. We illustrate the practical benefit of exploiting permutation\nequivalence on a reinforcement learning task.","authors":["Odalric-Ambrym Maillard","Mohammad Sadegh Talebi"],"pdf_link":"http://arxiv.org/pdf/2407.15662v1","category":["Datasets","Multimodal Learning"]},{"title":"Problems in AI, their roots in philosophy, and implications for science and society","abstract":"Artificial Intelligence (AI) is one of today's most relevant emergent\ntechnologies. In view thereof, this paper proposes that more attention should\nbe paid to the philosophical aspects of AI technology and its use. It is argued\nthat this deficit is generally combined with philosophical misconceptions about\nthe growth of knowledge. To identify these misconceptions, reference is made to\nthe ideas of the philosopher of science Karl Popper and the physicist David\nDeutsch. The works of both thinkers aim against mistaken theories of knowledge,\nsuch as inductivism, empiricism, and instrumentalism. This paper shows that\nthese theories bear similarities to how current AI technology operates. It also\nshows that these theories are very much alive in the (public) discourse on AI,\noften called Bayesianism. In line with Popper and Deutsch, it is proposed that\nall these theories are based on mistaken philosophies of knowledge. This\nincludes an analysis of the implications of these mistaken philosophies for the\nuse of AI in science and society, including some of the likely problem\nsituations that will arise. This paper finally provides a realistic outlook on\nArtificial General Intelligence (AGI) and three propositions on A(G)I and\nphilosophy (i.e., epistemology).","authors":["Max Velthoven","Eric Marcus"],"pdf_link":"http://arxiv.org/pdf/2407.15671v1","category":["Explainable AI","AI in Healthcare"]},{"title":"AI-Driven Fast and Early Detection of IoT Botnet Threats: A Comprehensive Network Traffic Analysis Approach","abstract":"In the rapidly evolving landscape of cyber threats targeting the Internet of\nThings (IoT) ecosystem, and in light of the surge in botnet-driven Distributed\nDenial of Service (DDoS) and brute force attacks, this study focuses on the\nearly detection of IoT bots. It specifically addresses the detection of stealth\nbot communication that precedes and orchestrates attacks. This study proposes a\ncomprehensive methodology for analyzing IoT network traffic, including\nconsiderations for both unidirectional and bidirectional flow, as well as\npacket formats. It explores a wide spectrum of network features critical for\nrepresenting network traffic and characterizing benign IoT traffic patterns\neffectively. Moreover, it delves into the modeling of traffic using various\nsemi-supervised learning techniques. Through extensive experimentation with the\nIoT-23 dataset - a comprehensive collection featuring diverse botnet types and\ntraffic scenarios - we have demonstrated the feasibility of detecting botnet\ntraffic corresponding to different operations and types of bots, specifically\nfocusing on stealth command and control (C2) communications. The results\nobtained have demonstrated the feasibility of identifying C2 communication with\na 100% success rate through packet-based methods and 94% via flow based\napproaches, with a false positive rate of 1.53%.","authors":["Abdelaziz Amara korba","Aleddine Diaf","Yacine Ghamri-Doudane"],"pdf_link":"http://arxiv.org/pdf/2407.15688v1","category":["Datasets","Explainable AI"]},{"title":"TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON","abstract":"TaskGen is an open-sourced agentic framework which uses an Agent to solve an\narbitrary task by breaking them down into subtasks. Each subtask is mapped to\nan Equipped Function or another Agent to execute. In order to reduce verbosity\n(and hence token usage), TaskGen uses StrictJSON that ensures JSON output from\nthe Large Language Model (LLM), along with additional features such as type\nchecking and iterative error correction. Key to the philosophy of TaskGen is\nthe management of information/memory on a need-to-know basis. We empirically\nevaluate TaskGen on various environments such as 40x40 dynamic maze navigation\nwith changing obstacle locations (100% solve rate), TextWorld escape room\nsolving with dense rewards and detailed goals (96% solve rate), web browsing\n(69% of actions successful), solving the MATH dataset (71% solve rate over 100\nLevel-5 problems), Retrieval Augmented Generation on NaturalQuestions dataset\n(F1 score of 47.03%)","authors":["John Chong Min Tan","Prince Saroj","Bharat Runwal","Hardik Maheshwari","Brian Lim Yi Sheng","Richard Cottrill","Alankrit Chona","Ambuj Kumar","Mehul Motani"],"pdf_link":"http://arxiv.org/pdf/2407.15734v1","category":["Explainable AI","Reinforcement Learning"]},{"title":"MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval Augmented Generation","abstract":"In this paper, we introduce MoRSE (Mixture of RAGs Security Experts), the\nfirst specialised AI chatbot for cybersecurity. MoRSE aims to provide\ncomprehensive and complete knowledge about cybersecurity. MoRSE uses two RAG\n(Retrieval Augmented Generation) systems designed to retrieve and organize\ninformation from multidimensional cybersecurity contexts. MoRSE differs from\ntraditional RAGs by using parallel retrievers that work together to retrieve\nsemantically related information in different formats and structures. Unlike\ntraditional Large Language Models (LLMs) that rely on Parametric Knowledge\nBases, MoRSE retrieves relevant documents from Non-Parametric Knowledge Bases\nin response to user queries. Subsequently, MoRSE uses this information to\ngenerate accurate answers. In addition, MoRSE benefits from real-time updates\nto its knowledge bases, enabling continuous knowledge enrichment without\nretraining. We have evaluated the effectiveness of MoRSE against other\nstate-of-the-art LLMs, evaluating the system on 600 cybersecurity specific\nquestions. The experimental evaluation has shown that the improvement in terms\nof relevance and correctness of the answer is more than 10\\% compared to known\nsolutions such as GPT-4 and Mixtral 7x8.","authors":["Marco Simoni","Andrea Saracino","Vinod P.","Mauro Conti"],"pdf_link":"http://arxiv.org/pdf/2407.15748v1","category":["Speech Recognition","Speech Synthesis"]},{"title":"Model editing for distribution shifts in uranium oxide morphological analysis","abstract":"Deep learning still struggles with certain kinds of scientific data. Notably,\npretraining data may not provide coverage of relevant distribution shifts\n(e.g., shifts induced via the use of different measurement instruments). We\nconsider deep learning models trained to classify the synthesis conditions of\nuranium ore concentrates (UOCs) and show that model editing is particularly\neffective for improving generalization to distribution shifts common in this\ndomain. In particular, model editing outperforms finetuning on two curated\ndatasets comprising of micrographs taken of U$_{3}$O$_{8}$ aged in humidity\nchambers and micrographs acquired with different scanning electron microscopes,\nrespectively.","authors":["Davis Brown","Cody Nizinski","Madelyn Shapiro","Corey Fallon","Tianzhixi Yin","Henry Kvinge","Jonathan H. Tu"],"pdf_link":"http://arxiv.org/pdf/2407.15756v1","category":["Datasets","Explainable AI"]},{"title":"Diffusion Model Based Resource Allocation Strategy in Ultra-Reliable Wireless Networked Control Systems","abstract":"Diffusion models are vastly used in generative AI, leveraging their\ncapability to capture complex data distributions. However, their potential\nremains largely unexplored in the field of resource allocation in wireless\nnetworks. This paper introduces a novel diffusion model-based resource\nallocation strategy for Wireless Networked Control Systems (WNCSs) with the\nobjective of minimizing total power consumption through the optimization of the\nsampling period in the control system, and blocklength and packet error\nprobability in the finite blocklength regime of the communication system. The\nproblem is first reduced to the optimization of blocklength only based on the\nderivation of the optimality conditions. Then, the optimization theory solution\ncollects a dataset of channel gains and corresponding optimal blocklengths.\nFinally, the Denoising Diffusion Probabilistic Model (DDPM) uses this collected\ndataset to train the resource allocation algorithm that generates optimal\nblocklength values conditioned on the channel state information (CSI). Via\nextensive simulations, the proposed approach is shown to outperform previously\nproposed Deep Reinforcement Learning (DRL) based approaches with close to\noptimal performance regarding total power consumption. Moreover, an improvement\nof up to eighteen-fold in the reduction of critical constraint violations is\nobserved, further underscoring the accuracy of the solution.","authors":["Amirhassan Babazadeh Darabi","Sinem Coleri"],"pdf_link":"http://arxiv.org/pdf/2407.15784v1","category":["Reinforcement Learning","Explainable AI"]},{"title":"On shallow planning under partial observability","abstract":"Formulating a real-world problem under the Reinforcement Learning framework\ninvolves non-trivial design choices, such as selecting a discount factor for\nthe learning objective (discounted cumulative rewards), which articulates the\nplanning horizon of the agent. This work investigates the impact of the\ndiscount factor on the biasvariance trade-off given structural parameters of\nthe underlying Markov Decision Process. Our results support the idea that a\nshorter planning horizon might be beneficial, especially under partial\nobservability.","authors":["Randy Lefebvre","Audrey Durand"],"pdf_link":"http://arxiv.org/pdf/2407.15820v1","category":["Reinforcement Learning","Explainable AI"]},{"title":"NV-Retriever: Improving text embedding models with effective hard-negative mining","abstract":"Text embedding models have been popular for information retrieval\napplications such as semantic search and Question-Answering systems based on\nRetrieval-Augmented Generation (RAG). Those models are typically Transformer\nmodels that are fine-tuned with contrastive learning objectives. Many papers\nintroduced new embedding model architectures and training approaches, however,\none of the key ingredients, the process of mining negative passages, remains\npoorly explored or described. One of the challenging aspects of fine-tuning\nembedding models is the selection of high quality hard-negative passages for\ncontrastive learning. In this paper we propose a family of positive-aware\nmining methods that leverage the positive relevance score for more effective\nfalse negatives removal. We also provide a comprehensive ablation study on\nhard-negative mining methods over their configurations, exploring different\nteacher and base models. We demonstrate the efficacy of our proposed methods by\nintroducing the NV-Retriever-v1 model, which scores 60.9 on MTEB Retrieval\n(BEIR) benchmark and 0.65 points higher than previous methods. The model placed\n1st when it was published to MTEB Retrieval on July 07, 2024.","authors":["Gabriel de Souza P. Moreira","Radek Osmulski","Mengyao Xu","Ronay Ak","Benedikt Schifferer","Even Oldridge"],"pdf_link":"http://arxiv.org/pdf/2407.15831v1","category":["LLMs","Speech Recognition"]},{"title":"LLMmap: Fingerprinting For Large Language Models","abstract":"We introduce LLMmap, a first-generation fingerprinting attack targeted at\nLLM-integrated applications. LLMmap employs an active fingerprinting approach,\nsending carefully crafted queries to the application and analyzing the\nresponses to identify the specific LLM model in use. With as few as 8\ninteractions, LLMmap can accurately identify LLMs with over 95% accuracy. More\nimportantly, LLMmap is designed to be robust across different application\nlayers, allowing it to identify LLMs operating under various system prompts,\nstochastic sampling hyperparameters, and even complex generation frameworks\nsuch as RAG or Chain-of-Thought.","authors":["Dario Pasquini","Evgenios M. Kornaropoulos","Giuseppe Ateniese"],"pdf_link":"http://arxiv.org/pdf/2407.15847v1","category":["Explainable AI","Benchmarking"]}]